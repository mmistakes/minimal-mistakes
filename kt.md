---
layout: single
title: "Kolmogorov Theory References (with Abstract Summaries)"
permalink: /kt/
---


## 2007¬†‚Äì [Information, complexity, brains, and reality (Kolmogorov Manifesto)](https://arxiv.org/abs/0704.1147) (arXiv)  
Proposes that cognition can be understood as the search for programs approaching the Solomonoff‚ÄìKolmogorov‚ÄìChaitin complexity limit.  Introduces ‚Äúreality‚Äù as a mental model constructed by compressing and interpreting coherent sensory inputs.

## 2009¬†‚Äì [Reality as Simplicity](https://arxiv.org/abs/0903.1193) (arXiv)  
Argues that all human experience is built from algorithmic models seeking simplicity.  Develops a hierarchy of processing levels where the brain selects the simplest model consistent with data, illustrating applications in mismatch negativity and Presence research.

## 2016¬†‚Äì [MODELS, NETWORKS AND ALGORITHMIC COMPLEXITY](https://arxiv.org/abs/1612.05627) (arXiv)  
Demonstrates that models, classification functions, invariances, and datasets are algorithmically equivalent.  Shows how neural networks implement these models and fall into a descriptive power hierarchy aligned with recursive‚Äëfunction theory.

## 2017¬†‚Äì [An algorithmic information theory of consciousness](https://academic.oup.com/nc/article/2017/1/nix019/4470874) (Neuroscience of Consciousness)  
Presents an AIT framework for quantifying structured experience from neurophysiological data.  Hypothesizes that compressive generative models by recurrent networks underlie phenomenal structure and self‚Äëawareness; reviews methods such as ERP paradigms, spontaneous state analysis, stimulation, and behavioral measures. [Annotated version](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/nc/2017/1/10.1093_nc_nix019/3/nix019_supp.pdf) 

## 2021¬†‚Äì [The 2D Ising model, criticality and AIT](https://www.biorxiv.org/content/10.1101/2021.10.21.465265v1) (bioRxiv)  
 Uses the 2D Ising model to link phase‚Äëtransition phenomena (correlation length, susceptibility) with algorithmic complexity metrics (LZW compression, information rate).  Shows how sparse long‚Äërange couplings shift critical temperature and discusses implications for neural criticality.

## 2022¬†‚Äì [AIT Foundations of Structured Experience](https://www.worldscientific.com/doi/10.1142/S2705078522500047) (Journal of Artificial Intelligence and Consciousness)  [preprint](https://osf.io/preprints/psyarxiv/k3q6r_v1) 
 Unifies consciousness under AIT by defining structured experience (*ùíÆ*) as the match between data and succinct generative programs.  Associates model length with qualitative aspects of experience and predicts empirical markers via dimensionality reduction and criticality.

## 2022¬†‚Äì [Algorithmic structure of experience and the unfolding argument](https://osf.io/preprints/psyarxiv/7nbsw) (PsyArXiv)   
 Examines how the ‚Äúunfolding argument‚Äù and falsification criteria challenge AIT‚Äëbased consciousness theories.  Analyzes computational hierarchies and resource limits to propose refinements for causal‚Äëstructure models.

## 2023¬†‚Äì [LSD‚Äëinduced increase of Ising temperature and algorithmic complexity of brain dynamics](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010811) (PLOS Computational Biology)  
 Compares fMRI‚Äëderived Ising models under placebo vs. LSD, showing LSD raises system temperature and algorithmic complexity.  Personalized models reveal homotopic connectivity decreases and complexity metrics (LZW, BDM) correlate with temperature.

## 2023¬†‚Äì [Neural geometrodynamics, complexity, and plasticity: a psychedelics perspective](https://www.mdpi.com/1099-4300/26/1/90) (Entropy)  
 Introduces ‚Äúneural geometrodynamics,‚Äù linking fast neural dynamics and slow (meta)plasticity.  Shows psychedelics flatten the neural landscape, increasing entropy and complexity, and discusses how plasticity processes consolidate new dynamics.


## 2024¬†‚Äì [The Algorithmic Agent Perspective and Computational Neuropsychiatry: From Etiology to Advanced Therapy in Major Depressive Disorder](https://www.mdpi.com/1099-4300/26/11/953) (Entropy) 
 Applies the KT framework to model Major Depressive Disorder as low‚Äëvalence states in algorithmic agents.  Maps cognitive, affective, and executive deficits to brain circuits and biotypes; proposes personalized interventions combining stimulation, psychotherapy, and pharmacology.

## 2024¬†‚Äì [Navigating Complexity: How Resource‚ÄëLimited Agents Derive Probability and Generate Emergence](https://osf.io/3xy5d/)  (PsyArXiv)
 Shows that finite data access and computational resources naturally generate probabilistic reasoning, Bayesian inference, and a formal notion of emergence via coarse‚Äëgraining and algorithmic complexity; connects to Free Energy and Active Inference frameworks.

## 2025¬†‚Äì [Structured Dynamics in the Algorithmic Agent](https://www.mdpi.com/1099-4300/27/1/90) (Entropy)  
 Formalizes generative models via Lie‚Äëgroup symmetry and demonstrates that data‚Äëtracking constrains an agent‚Äôs dynamical repertoire to mirror world symmetries, supporting structured experience and the manifold hypothesis. Examines how compressive models and symmetry constraints shape an agent‚Äôs structural and dynamical repertoire, bridging AIT, group theory, and dynamical conservation laws.

 ## 2025¬†‚Äì [Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents](https://arxiv.org/abs/2510.10586) (arXiv)  
 Proposes that agents make sense of sensory data by exploiting compositional symmetry‚Äîmodeling the world using geometric transformations (Lie pseudogroups). They demonstrate that for a neural system to track the world accurately, it must mirror these external symmetries internally (equivariance). This constraint forces the agent's neural dynamics onto efficient, low-dimensional manifolds, providing a geometric explanation for why deep learning works ("the blessing of compositionality") and suggesting a predictive coding model where layers process unresolved residual transformations.
 
## 2025 ‚Äì [The Algorithmic Regulator](https://arxiv.org/abs/2510.10586) (arXiv)  
 Reinterprets the classic Good Regulator Theorem through the lens of Algorithmic Information Theory, modeling agents and environments as interacting Turing machines. The author proves that a successful regulator acts as a compressor that reduces the Kolmogorov complexity of the world‚Äôs output, thereby minimizing the algorithmic cost of survival. This mathematical framework demonstrates that effective regulation requires high mutual algorithmic information, confirming that agents must structurally mirror the environments they seek to control. Ultimately, the paper bridges control theory and the Free Energy Principle by defining intelligence as the capacity to model and compress environmental data

## 2026¬†‚Äì [The algorithmic agent: mathematical foundations (in prep)](https://www.overleaf.com/project/64e8ec470bc7c8920a8f36bf)  
 Develops rigorous definitions for algorithmic agents grounded in Turing‚Äëmachine pairs and AIT.  Analyzes computational degeneracy and argues that high‚Äëlevel program structure must manifest in observable dynamics; discusses identification of agent modules.
