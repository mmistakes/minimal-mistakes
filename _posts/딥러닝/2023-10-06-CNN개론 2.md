---
title: "CNN개론 2"
escerpt: "CNN 개론 공부"

categories:
  - DeepLearning
tags:
  - [AI, DeepLearning, CNN]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2023-10-06
last_modified_at: 2023-10-06

comments: true
 

---


# 5. CV Task - 이미지분류(image classification)

## 5-1. Image classification task
: classification 수행하는 architecture는 크게 2가지로 나눈다.

- 1) conv layer = backborn 
- 2) classification layer(fc layer)

  - conv layer + (object) detector 가 붙으면 object detect 수행하는 architecture가 된다.
  - backborn model은 feature extractor라고 한다. 수많은 conv layer들을 통해서 이미지 데이터들의 공통적인 특성들을 신경망을 통해서 수많은 가중합을 이용하여 깊은 layer를 쌓아서 미세한 특성들을 추출하는것이다.

- 학습방법은?
  - 파일의 폴더명을 label에 넣기
  - dict{"object":0} 이런식으로 넣기
  - images 폴더안에 넣어서 class 포함하는 파일명을 넣는경우.
    - ex.airplain_001.jpg
  - label.txt를 만들어서 파일명_cls 두는경우.

## 5-2. ResNet Architecture 해석

- 참고자료
  - [Deep Residual Learning for Image Recognition 논문](https://arxiv.org/abs/1512.03385)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ff204747-e424-4c63-99c6-d435d1272fbb)


- vgg19
  - pooling 을 하는게 있고, 마지막에 3개의 fc layer를 통해서 데이터의 특성을 한줄로 펴서 1000개의 class에 대해서 classificatino하는 모델.
  - 그래서 가장 높은 확률값을 가지는 cls에 대해서 출력하면 예측하게 된다.
  - 현재는 19 layer까지 만든것.

- 34-layer plan
  - vgg19를 34 - layer까지 쌓은것.

### 5-2-1. gradient vanishing(기울기 소실)이 발생.

- 기존 역전파특징
  - 역전파를 통해서 모델이 학습을 하게 되는데 층층별로 chain rule을 사용해서 각각 층별로 곱해지고 더해져서 cost에 대한 해당 기울기에 대한 변화율(∂C/∂w) 즉, 순간기울기를 구할수 있게 된다. 그것을 각각 곱해줘서 구해주면 이전층의 기울기도 구할수 있게 된다.
- 기존 역전파의 문제점
  - 이때 문제점은 굉장히 작은수를 계속 곱해진다면 0에 수렴하게 된다. 즉, 층이 깊어질수록 기울기가 소실되는 ..즉, weight가 업데이트가 잘되지 않는경우가 발생한다. w=w'-ㅁ ㅁ부분이 없데이트가 안되서 진행이 안됨.

- solution
  - 이걸 극복하기 위해 residual(잔차) 이란 방법을 사용.

  - 동일하게 34-layer를 구한한다. 다만 어떠한 block을 지나는 새로운 지름길을 추가하는거다. 즉 **skip connection** 시키는것.
  - 이렇게 되면 층이 깊어지더라도 역전파로 전달될때 굉장히 작은수가 역전파되면 0으로 수렴하겠지.  하지만 skip conncection때문에 기울기가 그대로 가서 연결된다. 그래서 더 깊은 층을 만들어도 기울기소실을 일어나지 않게 된다.

- resnet은 backborn으로 많이 사용되고 있다.
  - ex) resnext 도 많이 backborn으로 사용됨. 


# 6. CV Task - 객체검출(object detection)
: 3가지의 task로 구성되어 있으며, 이를 동시에 수행하는 task이다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/d21bfbf4-bb11-40e3-897f-36922df222d8)
  
  - localization : 이미지 들어왔을때 어떤 location에 object가 위치해 있는지를 출력.
  - classification : 해당 object가 과연 어떤 class에 속하는지 분류하는 task 
  - multiple objects : 다중객체를 분류하는 task


## 6-1. YOLO Architecture 해석

- 참고자료
  - [You Only Look Once: Unified, Real-Time Object Detection논문](https://arxiv.org/abs/1506.02640)

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/567788dc-a055-4565-99ad-418375fc40cf)

  - one-stage detection이다.
    - 이전에는 two-stage 로 detection 과정이 2개로 나누어져있었다(localization+ classification) 그러나, yolo는 이 과정을 하나로 합쳐서 빠르게 처리하는 architecture를 제안하였다.

  - fc layer를 사용하여 data를 한줄로 펴줌.
  - 그 이후 feature map을 하나 더 추가함!!
  - 만들어진 feature map이 바로 위의 grid라고 할수 있다.
  - sxs는 결국 7x7 이며, 이것이 two track으로 동시에 위아래로 진행된다.
    - 위 : 수많은 bbox를 생성하면서 어떤위치에 object가 존재하는지 예측하는 과정
    - 아래 : class probability map , 즉, 해당 grid에 어떤 cell에 있어서 이것이 어떤 class에 속하는지 예측하는 확률map이다. 
  - probability map에 높은 class가 존재하고 위에서 해당 class의 confidence가 높게 나온게 있다.  이것들이 합쳐져서 해당위치에 class에 해당 object의 bbox가 생성이 되면서 이부분에 object가 있음을 확인할 수 있다.



# 7. CV Task - 객체추적(object tracking)
: object detection에서 한가지가 더 추가된 task(+object ID)

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/e05d9ad4-9cdc-41ca-860a-b464f412dbf6)

  - object detection workflow
    - 어떤 이미지가 주어졌을때 object가 위치하는 localization 을 이용하여 bbox를 찾는 task가 실행되고 거기에 해당하는 class 를 구분하는 classification이 수행된다. 이것이 여러개 이기 떄문에 multi-object 를 수행한다.
  - object tracking workflow
    - object tracking은 여기에 추가로 object ID까지 예측하게 된다.
    - id_1, id_2 등  각 object마다 class가 같더라도 그것이 다른 object임을 확인할 수 있다.
  - object tracking 예시
    - video라면 영상이 재생이된다. 그러면 다음 frame으로 넘어갔을때 어떤 object가 어떻게 이동했는지 확인할 수 없다. 그래서 object마다 고유한 id를 부여해서 다음 frame에서 해당 id가 어떻게 이동했는지를 우리는 알수 있게 된다.그것을 수행하고자 하는게 object tracking task이다. 

- object tracking을 수행하기 위해서는 어떤 dataset을 만들어야 하나면 모든 object마다 id에 대한 정보가 포함되어야 한다.  

- fps(frame per seconds) : 1초당 몇개사진이 찍히는지.
  - 30fps 면 100s 면 3000장이 된다. 

## 7-1. interpolation(보간법)





## 7-2. object tracking annotation tool : DarkLabel
- [DarkLabel tool github](https://github.com/darkpgmr/DarkLabel)
- [DarkLabel 설명](https://darkpgmr.tistory.com/16)

- Darklabel은 interpolation이 적용된 tool이다.
- 데이터간의 가장 자연스러운걸 찾아서 예측하는 task
- 이게 동영상에 label시 어떻게 적용되냐면  1차 label, 10차 label하고 그사이에 있는거에서 가장 자연스럽게 잇는 interpolation을 구해서 자동으로 채워주는것이다.(이건 내부적으로 object detection이 수행되고 있다.)
- 이러면 semi-auto-labeling이 진행될수 있다. 

## 7-3. DeepSORT(Deep Simple Online and Real-time Tracking)
: SORT의 정확도를 개선한 다중객체 추적기술. 

- Deep : deeplearning을 의미한다. 즉, featrue extraction을 사용해서 다양한 id가 존재할때, 같은 class일 경우에도 다양한 id 부여받을수 있기 때문에.. 이걸 단순히 kalman과 hungarian 알고리즘에 의존해서 id를 부여하는것이 아니라 해당 id에 대해서 학습을 진행하여 feature를 추출하여 어떤 class에 해당하는지 예측하는 구조로 만들수 있다. 



- 참고자료
  - [SORT와 DeepSORT의 혼합을 이용한 실시간 다중객체 추적 논문](http://ki-it.com/xml/30742/30742.pdf)

- YOLO + DeepSORT : tracking을 하는 구조를 만들수있다.
  - yolo : object detector 하는 부분
  - DeepSORT : tracker을 수행하는 부분

  - detector를 통해서 object의 detect가 수행된다. 그 다음에 tracker가 해당 obejct에 대해서 id를 부여한다. 


### 7-3-1. SORT(single online real-time tracking) 알고리즘
: Kalman filter + Hungarian algorithm으로 구성되어 있다

- kalman filter

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/990bc3e4-4b8b-4ece-a314-a479c1b44d3f)

  - 이전필터에 등장했던 object의 다음 frame에서의 위치를 예측하는것.
  그러기 위해서 예측하는 분포가 존재하며(p),실제 분포가 있을것이다(t). 이 두 분포를 계속해서 학습을 통해 사이간격을 좁혀가는 알고리즘. 

- hungarian algorithm
  - object가 있다면 동일한 id 인지 판별하는것. 
  - 이건 matching algorithm을 사용하는것.

### 7-3-2. tracking에서 고질적인 2가지 문제점
- 1) occlusion
: id_1 이 id_2에 의해 가려졌을때 id_1에 대해서 tracking이 멈추게 되고 잃어버리게 된다.  가려짐이 없어지고 다시 id_1이 나타나면 새로운 id를 부여하게 된다.

- 2) id switching
어느순간 id가 뒤 바뀔수 있다. 

- 이런걸을 해결하는게 DeepSORT이다.


# 8. CV Task - 영역분할(segmentation)
: 모든 pixel(pixel by pixel)에 대해서 그 pixel이 어떤 class에 속하는지에 대한 예측확률이 가장높은지를 출력하는 task이다

- 참고자료
  - [Dectection and Segmentation_CS232강의](https://www.youtube.com/watch?v=nDPWywWRIRo)


![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/e86e046b-0fe3-47fd-9445-ab7c18572300)


  - semantic segmentation
  : 이런경우는 object를 localization하는 task가 아니다.모든 pixel에 대해서 어떤 class인지를 출력하는 task이다.

  - instance segmentation
  : multiple object처럼 해당 object의 localization이 되고 그 위치에서 어떤 pixel이 어떤 class에 속하는지를 예측수행하는 task이다.

  - segmentation을 위한 dataset 구축 방법
    - 경계선검출을 해야한다.
    - 해당 object의 (x,y)를 경계선기준으로 뽑아낸다.
    - coco dataset은 일반적으로 ""polygon(다각형)"" 형태로 labeling 후 각 꼭지점의 좌표 포인트들을 저장한다.

## 8-1. U-Net
: segmentation task에서 가장 유명한 architecture

- feature들의 크기가 sampling된다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/e17ffdae-55d3-44eb-ba22-1439901d4aad)

|구분|왼쪽|오른쪽|
|---|---|---|
|기능|contracting path|expanding path|
|기능2|축소하는 root|확장하는 root|
|sampling구분|down-sampling|up-sampling|
|의미|encoding과정(압축)|decoding과정(압축해제-정보압축후 복원)|
|해석|채널이 1-> 64->128 , feature map size감소하면서 channel 증가|feature map size 증가하면서 channel 감소|
|해석2|압축하면서 정보가 손실된다고 보기보다는 정보를 다른 형태로 추출한다는 의미로 해석|복원할때도 늘어났던 채널수를 줄이면서 동시에 feature map size를 증가|

- channel이란 정보를 의미한다. 몇개의 filter를 사용해서 몇개의 새로운 특성을 추출하는가를 의미한다.

- 최종적인 channel = 2: binary classification을 수행한다는 의미이다. 
  - 388x388 size에서 2개의 class에 대해서 classification을 수행한다는 의미이다. 즉 pixel마다 classification을 진행한다는 의미
  - 즉, **각각의 pixel의 확률값이 최종적으로 하나의 2차원의 이미지로 segmentation이 시각화 할수 있다.** 

- skip connection구조도 있다. 
: 어떠한 feature map size가 있다면 channel 방향으로 붙여준다는 의미이다.
  - 이것에 대한 효과는?
    - down-sampling과 up-sampling을 다 거친 값과 그대로 온값을 둘다 활용함으로써 즉, 기존에 갖고있었던 정보를 그대로 활용가능.이것을 단계마다 이루어진다. 





# 9. 이미지 전처리(pre-processing)
: input image가 model에 들어가기 전에 어떠한 변화과정을 거쳐서 통일시켜 주는 작업

- size, color 등이 다를수 있기 떄문에 이런 부분들을 일정한 형식에 맞춰서 통일시켜주는 작업

- inference(추론)할때도 이와같은 처리가 있어야 제대로 된 성능검사가 완료된다.

## 9-1. 전처리가 필요한 이유
- 1) Resize
: 이미지 사이즈를 바꾸는것

  - 다양한 사이즈를 가진 dataset에서 하나의 형식으로 resize를 진행하기 위해서. 왜냐하면 이를 model이 학습할때 resolution을 맞춰져야 한다.
  - 즉, 이미지의 형태, 즉 해당 image의 feature의 수가 같아야 하기때문이다.

- 2) Color
: 모든 이미지가 같은 color 형태로 되어있어야 한다.만약 image마다 RGB, Grayscale, HSV등으로 나뉜다면 모델이 학습하기 어렵다.

- 3) Normalization
: 기존에는 0 ~ 255 사이의 pixel value를 0 ~ 1로 scaling하는 기능이었다.
  - minmax, avg등 다양한 기법들이 있다.

## 9-2. Opencv 예시

```
import cv2
img = cv2.imread(‘image.jpg’, cv2.IMREAD_GRAYSCALE)    ## cv2.IMREAD_COLOR, grayscale로 image를 불러들인다.

img.shape                                               ## image의 형태 확인
img = cv2.resize(img, (200, 200))                      ## 256x256 -> 200x200 으로 resize 해줌. 즉, 이미지 축소가 됨
img = img[:100, :100]                                 ## 세로방향으로 0~99까지의 pixel을 불러옴. 즉 해당 image를 ""crop""하는 효과가 있다.

## kernel연산하는데 여기선 sharpen filter이구나. 조금더 이미지를 선명하게 해주는 효과.
kernel = np.array([[0, -1, 0],    
 [-1, 5, -1],
 [0, -1, 0]])
img = cv2.filter2D(img, -1, kernel)                   ## filter2D 함수를 이용해서 적용. -1은 데이터타입 즉,output datatype을 input datatype과 동일하게 사용하겠다는 의미
img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX) ## normalize한다는 의미.minmax normalize를 사용했다. 
```




---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}