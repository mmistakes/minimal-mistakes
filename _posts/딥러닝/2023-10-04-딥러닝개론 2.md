---
title: "딥러닝개론 2"
escerpt: "딥러닝 개론 공부"

categories:
  - DeepLearning
tags:
  - [AI, DeepLearning]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2023-10-04
last_modified_at: 2023-10-04

comments: true


---

# 6. 신경망 (neural networks)
: 신경망은 수많은 선형회귀구조로 되어있다.

- 선형회귀 구조를 이용하여 n개의 특성을 새롭게 만들수 있으며(설정가능), 얼마든지 많은 layer를 쌓아서 deep layer를 쌓을 수 있으며, node의 수가 많아지면 더 많은 가중치 조합을 이용할 수 있다. 그래서 조금더 디테일하고 데이터의 분포의 특성을 추출할 수 있다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/155a3868-b45b-4a0e-b56b-338fa8c7cbd6)
 
  - 해석) input data 3개가 있고, 이를 5개의 새로운 가중치 조합을 통해서 layer_1인 5개의 new feature를 만들었다.
  layer_2는 layer_1이 input으로 사용되고, 이를 가중치조합을 통해서 또다른 new feature를 생성.(이때 node의 개수는 마음대로 지정 가능.)

    - Q) 5개가지고 5개조합을 만들면 같은 값을까?
    : 다르다! layer_1의 feature는 3개의 data기반하에 만들어진 feature이고, input이 모두 다르기에 output이 다르다!

  - 신경망을 깊히 쌓을수록 데이터에 대한 정보를 많이 학습할수 있으며, 많은 특성들이 추출될수 있으며,이를통해 보다 정확한 예측을 할수 있게 된다.

  - 결과값이 그대로 다음 node로 전달되면 layer를 많이 쌓더라도 **선형변환(tranformation)** 문제가 발생할수 있다.
  이 변환이 **비선형변환**이다. 이전 층에서 가중치를 통해 new feature든다. 이후 이 feature를 비선형변환을 한번 거치고 다음층에 전달된다.  

    - Q)  선형변환이 신경망에서 문제가 되는 이유?
      - 신경망은 여러개의 선형변환이 이루어진 구조이다.
      - 깊은신경망을 쌓는 이유가 수많은 특성들을 추출하고 그 특성들을 이용하여 정확한 예측을 하기 위해서! 
      - 그런데 선형변환처럼 간결화가 되면 깊은신경망의 의미가 사라지게 되기에 선형변환한것을 다음 layer에 보낼때는 한번 비선형변환 거친 후 다음 layer로 전달한다.

## 6-1. 선형변환 vs 비선형변환

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/0c29640b-2f39-4035-8c5f-669e98161b0b)

- 선형변환
: 한점을 한 벡터공간에서 다른 벡터공간으로 이동시키는데 그 이동 규칙.
  
  - 축에 대한 변화가 있다고 생각하면 된다.
  - linear한 변화(각도, scale(크기의 변화)) 가 기존에 존재하는 점들의 관계가 그대로 유지된다.즉 점간의 거리가 그대로 유지된다.  

- 비선형변환
linear하지 않은 변화로 축이 변환한다. 그래서 기존의 관계가 그대로 유지되지 않는다. 즉, 기존의 점간의 길이가 같지 않는다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/c42bf3b0-cad4-4e2b-bd11-718a384dd813)


  - Q) 비선형변환을 넣으면 어떻게 될까?
  : 신경망을 깊은구조로 쌓게 되더라고 하나의 layer로 대체될수 있는 신경조합보다는 깊은 층들을 모두 활용할수 있고 input data에 대해서 수많은 특성들을 추출하고 특성을 통해서 정확한 예측을 하는데 문제가 없게 된다.

## 6-2. sigmoid
:로지스틱 회귀분석 or neural network의 **Binary classification** 마지막 레이어의 활성함수로 주로 사용된다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/f74bc1bc-e91a-496c-9c17-65c406c866ed)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/02e2f05e-bbc3-4177-9960-f9000f5c8eca)

- 비선형변환의 대표 예시
- e:자연상수를 의미한다.
- sigmoid함수 통과하면 0~1 사이값만 출력된다.

### 6-2-1. numpy.exp()
: numpy의 numpy.exp() 함수는 밑이 자연상수 e인 지수함수(e^x)로 변환해준다.

```
import numpy as np
print(np.exp(0))    # e^0과 동일
# 1.0

print(np.exp(1))    # e^1과 동일
# 2.718281828459045
```

## 6-3. activation(활성화함수)
:비선형변환에 의해 변환 값을 activation이라 한다. 

- sigmoid는 0~1의 값으로 **switch** 해주고 
- relu(Rectified Linear Unit)도 주로 쓰인다. 
 
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b072c412-8828-42fb-8e77-6f22fe782105)


### 6-3-1. sigmoid vs relu

- sigmoid는 **gradient vanishing(기울기소실)** 이라는 문제점이 존재. 
  - 예측을 잘하는 모델로 되는과정이 학습이며, 이때 가중치는 w-> w' 로 업데이트가 계속 된다. 
  - 가중치 업데이트할때 기울기정보를 사용한다. 
  - 역전파를 할때 cost를 계산한 후에, cost에 대한 feedback 을 적용할때(모델에 대한 업데이트를 진행할때) 가중치의 기울기 변화 와 activation function의 기울기 변화도 곱해서 사용한다.
  - 그래서 sigmoid는 |x| 의 절대값이 클수록, 즉 그래프의 양쪽끝으로 갈수록 기울기가 줄어들어 0에 가깝게 된다.
  - 기울기가 0에 가까워지면 activation function의 기울기도 0이 된다.
  - 즉,역전파를 통해서 거꾸로 발생할때 기울기가 0이 되어 어느순간 학습이 안되고 가중치가 업데이트 안될때가 발생한다. 즉, 학습이 잘안되는 문제점이 발생.


- 하지만 relu는 기울기값이 0과 1만 존재하기에 gradient vanishing 문제를 해결했다고 볼수 있다.
- relu가 계산이 더 단순하다


# 7. 가중치행렬(weight matrix)

## 7-1. one-hot encoding
: 사람이 이해할수 있는 데이터를 컴퓨터가 이해할수 있는 value로 전달하기 위한 방법.
 
```
>>> import torch
>>> import torch.nn.functional as F
>>> F.one_hot(torch.arrange(0,5) %3, num_classes=5)
tensor([[1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0]])
```
  - num_classes 
  : 정답 클래스의 총 개수, 없는 경우 주어진 텐서의 가장 큰 값보다 1만큼 큰수로 설정.

## 7-2. 가중치행렬 추가의 의미

- 가중치행렬에 행 추가
  - 가중치가 몇개 들어오느냐 , 즉 몇개의 가중치 조합을 통해서 다음층에 몇개의 node를 생성할것이냐에 따라서 우측에 열개수가 정해진다.
  - 즉, 데이터 행렬에 행 추가(데이터 point 수 증가)하면 결과값에는 연산결과에 있어서 행이 추가된다. 


- 가중치행렬에 열 추가

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/11fef892-9e5f-4a5a-b46a-41a381039688)

- 데이터 열이 증가 하면, 데이터 input의 node가 더 생기는것.
- 가중치행렬에 열 증가한다는건 가중치가 3개사용되면 3개의 연산결과가 나오고 그것이 다음 layer의 node수인 3개가 생성된다.
  - 4개의 가중치를 사용한다면 4개의 node수가 생긴다.
  - 즉, 가중치의 수가 늘어나면, 결과값에서도 열이 늘어난다.


# 8. 경사하강법(gradient descent)

## 8-1. 비용함수(cost)
: 어느쪽으로 이동해야 cost가 감소하는지는 기울기를 보고 판단한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/348b221d-cbed-44cd-a901-6655c8e86a01)

  - 기울기
  : x가 변화했을때, y가 변화하는 정도

  - -(minus) :  기울기 반대방향으로 가야되기때문에
    - new weight = weight + (기울기 반대방향)
    - 기울기가 음수면 cost함수는 오른쪽으로 가야하고 기울기가 양수면 cost함수는 왼쪽으로 가야하는 특성이 있기 때문에!

## 8-2. Learning rate(학습률)

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b5fabb79-616e-4a8d-9536-6f02f507a06c)

  - α : 학습률을의미.
    - 학습률이 높다면? 기울기 변화가 엄청 커진다.최저값게 수렴하지 못할수 있다.
    - 학습률이 낮다면? 계산리소스가 많이 든다. 

## 8-3. 미분과 편미분(partial derivative)
: 편미분(∂)은 한변수만 미분을 진행한다. 그외는 상수처리한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/63a780e2-848a-4a94-9ff4-5443d80fa6eb)

  - weight개수에 따라서 차원의 개수가 정해진다.
  - 그래서 편미분을 사용하여 해당 weight(가중치)에 한해서만 편미분을 통해 기울기를 구하고 업데이트를 하는것이다.
  - weight가 3개일때 x,y,z축에 3차원 타원형이 생긴다. 편미분 진행시, (편미분적용 변수를 x라고 가정) z축은 상수처리 후 x,y축에만 해당 타원형이 곡선형태로 투영된다. 그것을 이용하여 기울기를 쉽게 구할수 있다. 

# 9. Optimizer의 종류
: 최적화의 방법을 optimizer라고 한다.

## 9-1. gradient descent
  - t : time (현재가중치에서 다음 가중치로 갈때의 기준인 timestamp)
  - lr(α) : 학습률
  - ∂L/∂W : gradient
  - -(minus) :  기울기 반대방향으로 가야되기때문에
    - new weight = weight + (기울기반대방향) 

## 9-2. SGD(Stochastic Gradient Descent)
: 확률적 경사하강법, 무작위적인 확률에 의해서 경사하강법하는것.

- batch GD : 수많은 데이터를 신경망에 통과시켜서 오차(error)를 다 더해서 **cost**를 구한 후 update진행하게 된다. 이때를 batchGD라고 부른다.
batch는 데이터셋 전체의 개수라고 생각하면 된다. 데이터셋을 한바퀴 돌고나서 weight에 대한 update가 진행된다.

- mini-batch GD : batch를 여러개 그룹으로 쪼개서 오차(error)를 구해서 **loss**를 구한 후 update하는것. 
cost function: 전체 batch에 대한 에러의 총합
loss function : mini batch에 대한 에러의 총합 

- single sample GD : 하나의 샘플에 대해서 error를 계산 후 바로 update하는 방법

 
## 9-3. Momentum(관성)

- SGD + Momentum(관성)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/50fcf77f-2dfe-4657-af17-2deeeaa7e856)

  - model은 local을 global로 착각할수 있다.
    - local : 주위에서의 minimum
    - global : 실제최소값 minimum
  - 현재 기울기가 양수로 나왔다고 하더라더라고 관성의 힘으로 더 넘어가서 학습할수 있다. 
  - 현재 time stamp만 고려하는게 아니라 이전 time까지 고려하여 누적된값들을 함께 optimization에 사용한다.

  - torch.optim.SGD(..., momentum=0.9)
  : SGD라는 optimizer안에 momentum이라는 parameter가 포함되어 쓰일수 있다

## 9-4. AdaGrad
: SGD + adaptive 학습률감소(learning rate decay)

- 학습률은 기울기를 계산해서 이동한다.
- 그런데 학습률 자체가 변한다기 보다는 그앞에 새로운 값(1/Σgradient²을 넣어준다.)을 곱해주는것이다. 그래서 크고작은값을 미세하게 조절해준다.

- 학습이 일반적으로 global minimum 근처에서 학습이 제대로 이루어 지지 않고 왔다갔다 하는 경우가 종종발생한다. 이런 부분을 해결하는데 도움이 된다.

## 9-5. RMSprop
: AdaGrad + gradient decay

- AdaGrad만 사용하게 되면 1/Σgradient² 이 값이 계속 누적되어 분모가 계속 무한대로 가서 전체항이 0에 수렴하게 된다. 
- 그렇게 되면 w' = w - costFunction 인데 costFunction이 0이 된다.
- w와w' 즉, 이전 weight와 새로운 weight값에 변화가 없게 되어 학습이 일어나지 않게 된다.
- 그래서 gradient decay는 값을 감소시켜주는 역할을 적용해서 이러한 문제점을 해결한다.

## 9-6. Adam
: Adagrad(RMSpop) + Momentum 이 합쳐진것

- SGD보다 학습속도가 빠르다.
- AdamW, AdamP 등이 나오는데 이러한 이유는 일반화성능을 향상시키기 위해서이며, 딥러닝의 전반적인 task성능 향상방법을 찾기위해서이다.


  * 딥러닝은 어떤한 데이터냐, 어떤구조를 사용하느냐, 어떠한 task를 수행하느냐에 따라서 optimizer종류를 한번 고민해서 적절한걸 사용해라.
  * 이러한 task에는 이러한 optimizer가 성능이 좋다라는걸 찾아내라.

---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}