---
layout: single
title:  "11.ë‹¨ì–´ ë° ë¶ˆìš©ì–´ 2ì°¨ì •ì œ"
categories: TextMining
tag: [python,Jupyter Notebook,TextMining]
toc: true
toc_sticky: true
author_profile: false #ì˜†ì— ì •ë³´ ë„ê¸°
sidebar: 
    nav: "docs"
typora-root-url: ../
---

<div class="notice--info">
<h4>ğŸ’¡ëª©í‘œ</h4>
<ul>
    <li>ë¶ˆìš©ì–´ ì²˜ë¦¬ë°©ë²• 3ê°€ì§€ í™œìš©í•˜ê¸°.</li>
    1. ì •ê·œì‹ì„ ì´ìš©í•´ ì•ŒíŒŒë²³ë§Œ ë‚¨ê¸°ê¸° <br>
    2. í•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ì œê±°í•˜ê¸° <br>
    3. ë‚´ stopwords ê±°ë¥´ê³  ì¼ë¶€ ë‹¨ì–´ ì •ì œ/í†µí•©
</ul>
</div>
---

#### 1. ë¶ˆìš©ì–´ë€?

í…ìŠ¤íŠ¸ ë¬¸ì¥ì—ì„œ í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ **ë¶ˆìš©ì–´(Stopword)**ë¼ê³  í•©ë‹ˆë‹¤. **ë¶ˆìš©ì–´ ì²˜ë¦¬**ëŠ” í…ìŠ¤íŠ¸ë¶„ì„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ **ì „ì²˜ë¦¬ ê³¼ì •** ì¤‘ í•˜ë‚˜ë¡œ I, you, it, is, am ê³¼ ê°™ì´ í…ìŠ¤íŠ¸ ë¶„ì„ì‹œì— í¬ê²Œ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, íšŒì‚¬ì˜ íšŒì˜ì„ í†µí•´ ìì£¼ ì“°ì´ëŠ” ë‹¨ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì„œì˜ ë¶„ìœ„ê¸°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì§„í–‰í•œë‹¤ê³  í•˜ì.



> 1. íšŒì˜ì£¼ì œ : ì†Œë¹„ì íŠ¸ë Œë“œì— ë§ì¶˜ ë§ˆì¼€íŒ… ë°©ë²•
> 2. ì¼ì‹œ : 2023ë…„ 04ì›” 09ì¼ 14:00~16:00
> 3. ì¥ì†Œ : ì œ2íšŒì˜ì‹¤
> 4. íšŒì˜ë‚´ìš© : ì£¼ë¡œ 20~30ëŒ€ì˜ ì†Œë¹„ìì¸µì„ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ  SNSì„ í™œìš©í•œ ë°©ë²•ì„ ì£¼ë¡œ í™ë³´ë¥¼ ì§„í–‰í•œë‹¤. ì´ë²ˆ ìƒí’ˆë“¤ì˜ ì¬ë£Œê°€ ì €ë²ˆê³¼ ë‹¤ë¥¸ ì¹œí™˜ê²½ì ì¸ ì¬ë£Œë¥¼ ì‚¬ìš©í•œ ì ì„ ë¶€ê°ì‹œì¼œ ì†Œë¹„ìì˜ ê±´ê°•ì—ë„ í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¶€ë¶„ì„ ê°•ì¡°í•œë‹¤.
>

ì´ì™€ ê°™ì€ í…ìŠ¤íŠ¸ë¬¸ì´ ìˆì„ë•Œ ì–´ë– í•œ ë¶€ë¶„ì´ ë¶ˆìš©ì–´ì¼ê¹Œ?

- '2023ë…„ 04ì›” 09ì¼' ìˆ«ì

- ' : ' ê¸°í˜¸

- 'ì€,ëŠ”,ì˜,ë¥¼' ì¡°ì‚¬

- 'ë“¤','ì ì¸' ì ‘ë¯¸ì‚¬


ìœ„ì™€ ê°™ì€ ë¶€ë¶„ì´ ê³„ì† ë°˜ë³µë˜ì–´ í° ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠëŠ” **ë¶ˆìš©ì–´** ì¸ ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë¶ˆìš©ì–´ëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ ì œê±°í•´ì¤˜ì•¼ë§Œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë‹¤.

 

#### 2. ë¶ˆìš©ì–´ì²˜ë¦¬ ë°©ë²•

###### ì‹¤ìŠµì¤€ë¹„(íŒ¨í‚¤ì§€ ì„¤ì¹˜)

```python
# pip install wordcloud    #No module named 'wordcloud' ì˜¤ë¥˜ì‹œì— ì‹¤í–‰ì‹œí‚¤ê¸°

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize     #í† í°í™”
from nltk.corpus import stopwords           #ë¶ˆìš©ì–´
from nltk.stem import PorterStemmer, WordNetLemmatizer     #ì–´ê°„ì¶”ì¶œ
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
```

```python
# wos_ai_.csv íŒŒì¼ì—ì„œ ABSTRACT ì—´ë§Œ dataíŒŒì¼ì— ì €ì¥
data=pd.read_csv('wos_ai_.csv',encoding='euc-kr').ABSTRACT
```

ì‹¤ìŠµíŒŒì¼ì¸ wos_ai_.csv íŒŒì¼ì„ ì‚¬ìš©í•˜ì˜€ê³ _
_wos_ai_.csv íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¶„ì„ì— í•„ìš”í•œ ABSTRACT ì—´ë§Œ dataì— ì €ì¥ í•˜ì˜€ìŠµë‹ˆë‹¤.



ì—¬ê¸°ì„œëŠ” nltk.corpus ì˜ stopwords ë¶ˆìš©ì–´ì™€ wordcloud ì˜ STOPWORDS ë¶ˆìš©ì–´ 2ê°€ì§€ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤. wordcloudì˜ ë¶ˆìš©ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëŒ€ë¬¸ìì„ì„ ìœ ì˜í•´ì£¼ì„¸ìš”.

nltkëŠ” ìì—°ì–´ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€ë¡œ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.

<br>

##### **ë°©ë²•1) ì •ê·œì‹ì„ ì´ìš©í•´ ì•ŒíŒŒë²³ë§Œ ë‚¨ê¸°ê¸°**

###### * ì •ê·œì‹ì´ë€?

- ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ íŠ¹ì •í•œ ê·œì¹™ì„ ê°€ì§„ ë¬¸ìì—´ì˜ ì§‘í•©ì„ í‘œí˜„í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” í˜•ì‹ ì–¸ì–´ì´ë‹¤.

- íŠ¹ì • ê²€ìƒ‰ íŒ¨í„´ì— ëŒ€í•œ í•˜ë‚˜ ì´ìƒì˜ ì¼ì¹˜ í•­ëª©ì„ ê²€ìƒ‰í•´ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ë§¤ìš° ìœ ìš©í•˜ë‹¤.

- ë¶ˆìš©ì–´ ì²˜ë¦¬ì—ì„œ [ íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì, ê¸°í˜¸ ]ë¥¼ ì œê±°í•œ [ ì•ŒíŒŒë²³ ]ë§Œ ì¶”ì¶œí•˜ëŠ” ë“± ìœ ìš©í•˜ê²Œ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤. 



íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì, ê¸°í˜¸ë¥¼ ì•ŒíŒŒë²³ ë¹¼ê³  ì „ë¶€ ì œê±°í•˜ê¸° ìœ„í•´ ì •ê·œì‹(ì •ê·œí‘œí˜„ì‹)ì„ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.
ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‚´ì¥ íŒ¨í‚¤ì§€ì¸ **re** ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.

```python
import re  #ì •ê·œì‹ ë¶ˆëŸ¬ì˜¤ê¸°

doc_set = []   #ì „ì²˜ë¦¬ ì „ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
words = []     #ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸

for doc in data :
    if type(doc) != float :
        doc_set.append(doc.replace("_"," "))
```

ë°˜ë³µë¬¸ì„ í†µí•´ì„œ ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ìë§Œ doc_set ë¦¬ìŠ¤íŠ¸ì— _ë¥¼ ê³µë°±ì²˜ë¦¬í•´ì„œ ë„£ì–´ì£¼ì—ˆê³ ,
type(ë³€ìˆ˜)ëŠ” ë³€ìˆ˜ì˜ ë°ì´í„°íƒ€ì…ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ë¡œ dataì˜ ë‹¨ì–´ê°€ ì‹¤ìˆ˜(float)ì¸ì§€ í™•ì¸í•˜ê³  ì•„ë‹ˆë©´(!=) doc_set.append(doc.replace(""," "))ë¥¼ ì‹œí–‰í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

list.append(ë³€ìˆ˜)ëŠ” ë³€ìˆ˜ë¥¼ listì— ì¶”ê°€ì‹œí‚¤ëŠ” í•¨ìˆ˜ë¡œ doc_set ë¦¬ìŠ¤íŠ¸ì— doc.replace("_"," ")ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

ë¬¸ìì—´.replace(old,new,[count]) í•¨ìˆ˜ëŠ” countë²ˆê¹Œì§€ oldë¥¼ newë¡œ ë°”ê¾¸ëŠ” í•¨ìˆ˜ë¡œ ì—¬ê¸°ì„œ "_" êµ¬ë¶„ìë¥¼ ê³µë°±ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.  (ex: new_things -> new things)

```python
stopWords = set(stopwords.words('english'))
stemmer=PorterStemmer()
```

nltk.corpus ì˜ ë¶ˆìš©ì–´ stopwordsë¥¼ ì˜ì–´ë‹¨ì–´ë¥¼ set() ì§‘í•© í˜•íƒœë¡œ stopWords ë³€ìˆ˜ì— í• ë‹¹í•˜ê³  PorterStemmer ëŠ” ì–´ê°„ì¶”ì¶œì„ ìœ„í•œ ëª…ë ¹ì–´ë¡œ í•´ë‹¹ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•´ stemmerì— í• ë‹¹í•˜ì˜€ìŠµë‹ˆë‹¤.

ì–´ê°„ì¶”ì¶œì€ ê°™ì€ ì˜ë¯¸ì´ì§€ë§Œ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì“°ì¸ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ì–´ê°„ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤‘ë³µëœ ë‹¨ì–´ê°€ ìƒê¸°ì§€ ì•Šë„ë¡ í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì • ì¤‘ í•˜ë‚˜ ì…ë‹ˆë‹¤. (ì–´ê°„ì¶”ì¶œì˜ ì˜ˆ : networking -> network , called -> call )

```python
for doc in doc_set:
    noPunctionWords = re.sub(r"[^a-zA-Z]+"," ",str(doc)) #ì•ŒíŒŒë²³ë§Œ ì¶œë ¥
    tokenizedwords = word_tokenize(noPunctionWords.lower()) #ì†Œë¬¸ìì²˜ë¦¬, í† í°í™”
    stoppedwords=[w for w in tokenizedwords if w not in stopWords] #ë¶ˆìš©ì–´ì²˜ë¦¬
    stemmedwords=[stemmer.stem(w) for w in stoppedwords] #ì–´ê°„ì¶”ì¶œ
    words.append(" ".join(stemmedwords)) #ë‹¨ì–´ë“¤ì„ ê³µë°±ìœ¼ë¡œ í•©ì³ì¤Œ
```

doc_setì— ìˆë˜ í…ìŠ¤íŠ¸ê°€ docì— í• ë‹¹ë˜ì–´  ë°˜ë³µë¬¸ìœ¼ë¡œ ì „ì²˜ë¦¬ í•œ ë¶€ë¶„ì„ í•˜ë‚˜ì”© ì‚´í´ë³´ìë©´

- **ë¶ˆìš©ì–´ì²˜ë¦¬**<br>re íŒ¨í‚¤ì§€ì˜ sub()í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ë¶ˆìš©ì–´ì¸ ê¸°í˜¸ì™€ ìˆ«ìë¥¼ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.<br>re.sub(r"\[^a-zA-Z]+"," ",str(doc)) êµ¬ë¬¸ì„ ì‚´í´ë³´ìë©´ a-zA-ZëŠ” ëª¨ë“  ì•ŒíŒŒë²³ì„ ì˜ë¯¸í•˜ê³ , ì—¬ê¸°ì— ^ë¥¼ ì¶”ê°€í•œ ê±´ notê³¼ ê°™ì´ ì•„ë‹ˆë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.<br>
  ì´ë¥¼ []ë¡œ ë¬¶ê³  +ë¥¼ ë¶™ì—¬ ' ' ì „ì²´ë¡œ ë¬¶ì€ í›„ì— rì„ ë¶™ì—¬ì£¼ëŠ” ì´ìœ ëŠ” rì´ ë¬¸ì ê·¸ëŒ€ë¡œ íŒë‹¨í•˜ë¼ëŠ” ì˜ë¯¸ë¡œ \ ë¥¼ '\\' ë¬¸ìë¡œ ì¸ì‹í•˜ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.<br>
  ì¦‰, docë¥¼ ë¬¸ìì—´(str)ë¡œ ë°”ê¾¼ í›„ ëª¨ë“  ì•ŒíŒŒë²³ì„ ì œì™¸í•˜ê³  "  "(ê³µë°±)ìœ¼ë¡œ ë°”ê¾¸ë¼ëŠ” ì˜ë¯¸ì¸ ê²ƒì…ë‹ˆë‹¤.
  
- **í† í°í™”**<br>ì•ŒíŒŒë²³ë§Œ ì¶”ì¶œëœ ë‹¨ì–´ë¥¼ noPunctionWordsì— í• ë‹¹í•˜ê³  ì´ë¥¼ ì†Œë¬¸ìì²˜ë¦¬(.lower())í•œ í›„ì— í† í°í™” ì‹œí‚¤ëŠ” êµ¬ë¬¸ì´ word_tokenize(noPunctionWords.lower()) ì…ë‹ˆë‹¤.

- **ë¶ˆìš©ì–´ì²˜ë¦¬**<br>í† í°í™”ëœ ë‹¨ì–´ê°€ tokenizedwords ë˜ì–´ ë¶ˆìš©ì–´ì²˜ë¦¬ë¥¼ ìœ„í•´ [w for w in tokenizedwords if w not in stopWords] êµ¬ë¬¸ì„ ê±°ì¹˜ëŠ”ë° ì´ëŠ” ë³€ìˆ˜(tokenizedwords)ì˜ ë‹¨ì–´(w)ê°€ ë¶ˆìš©ì–´(stopWords)ê°€ ì•„ë‹ˆë¼ë©´(not in) stoppedwordsì— ë°˜í™˜í•´ì¤€ë‹¤ëŠ” ì˜ë¯¸ ì…ë‹ˆë‹¤.

- **ì–´ê°„ì¶”ì¶œ**<br>ì´ë ‡ê²Œ ë¶ˆìš©ì–´ë¥¼ ê±°ì¹œ stoppedwordsì˜ ë‹¨ì–´ê°€ í•˜ë‚˜ì”© ì–´ê°„ì¶”ì¶œí™”(stemmer.stem(w)) ëœ í›„ì— stemmedwords ë³€ìˆ˜ì— í• ë‹¹ë˜ì–´ì§€ê³  ì´ë¥¼ ê³µë³µ(" ")ìœ¼ë¡œ í•©ì³ì£¼ëŠ” í•¨ìˆ˜ joinì„ í†µí•´ word ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥(.append)í•´ì£¼ê²Œ ë©ë‹ˆë‹¤.

<br>

í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë¬¸ì„ ê±°ì¹˜ë©´ì„œ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ ì‚´í´ë³´ë©´,

| ë°˜ë³µë¬¸ ë‚´ ë³€ìˆ˜ëª…                  | í˜•íƒœ                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| doc_set<br />(ì „ì²˜ë¦¬ ì „)          | ['The rapid growth of social networking usage has initiated a new business  called C2C s-commerce which has opened a novel opportunity for SNS rs to conduct commercial activities among members.'] |
| noPunctionWords<br />(ì•ŒíŒŒë²³ì¶œë ¥) | 'The rapid growth of social networking usage has initiated a new business model called C C s commerce which has opened a novel opportunity for SNS users to conduct commercial activities among members' |
| tokenizedwords<br />(í† í°í™”)      | ['the',  'rapid',  'growth',  'of',  'social',  'networking',  'usage',  'has',  'initiated',  'a',  'new',  'business',  'model',  'called',  'c',  'c',  's',  'commerce',  'which',  'has',  'opened',  'a',  'novel',  'opportunity',  'for',  'sns',  'users',  'to',  'conduct',  'commercial',  'activities',  'among',  'members'] |
| stoppedwords<br />(ë¶ˆìš©ì–´ì²˜ë¦¬)    | ['rapid',  'growth',  'social',  'networking',  'usage',  'initiated',  'new',  'business',  'model',  'called',  'c',  'c',  'commerce',  'opened',  'novel',  'opportunity',  'sns',  'users',  'conduct',  'commercial',  'activities',  'among',  'members'] |
| stemmedwords<br />(ì–´ê°„ì¶”ì¶œ)      | ['rapid',  'growth',  'social',  'network',  'usag',  'initi',  'new',  'busi',  'model',  'call',  'c',  'c',  'commerc',  'open',  'novel',  'opportun',  'sn',  'user',  'conduct',  'commerci',  'activ',  'among',  'member'] |
| words<br />(ì „ì²˜ë¦¬ ì™„ë£Œ)          | ['rapid growth social network usag initi new busi model call c c commerc open novel opportun sn user conduct commerci activ among member'] |

ì´ëŸ¬í•œ ì²˜ë¦¬ê³¼ì •ì„ ê±°ì³ ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ wordsì˜ í˜•íƒœë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.

<br>

ì „ì²˜ë¦¬ëœ wordsë¥¼ í•œëˆˆì— ì‚´í´ë³´ê¸° ìœ„í•´ wordcloudë¥¼ í†µí•´ ì‹œê°í™” í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
cv=CountVectorizer(max_features=100,stop_words='english').fit(words) #ë‹¤ì‹œí•œë²ˆí† í°í™”
dtm = cv.fit_transform(words)

words = cv.get_feature_names_out()

count_mat=dtm.sum(axis=0)
count_mat

count=np.squeeze(np.asarray(count_mat))
word_count=list(zip(words,count))
word_count

word_count=sorted(word_count,key=lambda x:x[1],reverse=True)

stopwords=set(STOPWORDS)

wc = WordCloud(background_color='black',stopwords=stopwords,width=800,height=600)

cloud=wc.generate_from_frequencies(dict(word_count))

plt.figure(figsize=(12,9))
plt.imshow(cloud)
plt.axis('off')
plt.show()
```

btm ìë£Œí˜•íƒœë¡œ ë§Œë“¤ì–´ ì´ë¥¼ ì‹œê°í™”í•˜ëŠ” ë‚´ìš©ì€ ì €ë²ˆ ì—…ë¡œë“œ ë•Œ ìƒì„¸íˆ ë‹¤ë¤˜ìœ¼ë¯€ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.

[10.ë‹¨ì–´ ë¹ˆë„ë¶„ì„ê³¼ Word Clouding]: https://songeunhwa.github.io/textmining/10Textmining/



##### **ë°©ë²•2) í•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ì œê±°í•˜ê¸°**

```python
data=pd.read_csv('wos_ai_.csv',encoding='euc-kr').ABSTRACT

import re  #ì •ê·œì‹ ë¶ˆëŸ¬ì˜¤ê¸°

doc_set = []   #ì „ì²˜ë¦¬ ì „ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
words = []     #ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸

for doc in data :
    if type(doc) != float :
        doc_set.append(doc.replace("_"," "))

stopWords = set(stopwords.words("english"))
```

stopWordsì„ ë§Œë“œëŠ” ê³¼ì •ê¹Œì§€ëŠ” ìœ„ì™€ ë™ì¼í•˜ê²Œ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.



ì´ë²ˆì—ëŠ” re.sub êµ¬ë¬¸ì„ í™œìš©í•´ì„œ ì›í•˜ëŠ” íŠ¹ìˆ˜ë¬¸ìë§Œ ì—†ì–´ì§€ë„ë¡ ì²˜ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
stemmer=PorterStemmer()
for doc in doc_set:
    noPunctionWords = re.sub(r"[()\"#/@;:<>{}`+=~|.!?,]"," ",str(doc)) #ì•ŒíŒŒë²³ë§Œ ì¶œë ¥
    tokenizedwords = word_tokenize(noPunctionWords.lower()) #ì†Œë¬¸ìì²˜ë¦¬, í† í°í™”
    stoppedwords=[w for w in tokenizedwords if w not in stopWords] #ë¶ˆìš©ì–´ì²˜ë¦¬
    noDigitWords= [w for w in stoppedwords if not w.isdigit()] #ìˆ«ìì²˜ë¦¬
    stemmedwords=[stemmer.stem(w) for w in noDigitWords] #ì–´ê°„ì¶”ì¶œ
    words.append(" ".join(stemmedwords)) #ë‹¨ì–´ë“¤ì„ ê³µë°±ìœ¼ë¡œ í•©ì³ì¤Œ
```

- **ì›í•˜ëŠ” ê¸°í˜¸ ì²˜ë¦¬** : ìœ„ì—ì„œ \[^a-zA-Z]ì„ í†µí•´ ëª¨ë“  ì•ŒíŒŒë²³ë§Œ ì œì™¸ì‹œì¼°ë‹¤ë©´, ì´ë²ˆì—ëŠ” ()[]\/@:;.!? ë“± ì›í•˜ëŠ” ê¸°í˜¸ë¥¼ [] ê´„í˜¸ ì•ˆì— ë„£ì–´ ê³µë°±(" ")ìœ¼ë¡œ ë°”ê¿”ì£¼ì—ˆìŠµë‹ˆë‹¤.<br>ì´ëŸ¬í•œ ë°©ë²•ì„ ì´ìš©í•˜ë©´ ì „ì²˜ë¦¬ê°€ ë˜ì§€ ì•Šì•˜ë˜ ê¸°í˜¸ë¥¼ í™•ì¸í•´ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ìˆ«ì ì²˜ë¦¬** : noDigitWords= [w for w in stoppedwords if not w.isdigit()<br>
  ì•„ê¹Œì™€ ë‹¤ë¥´ê²Œ ì´ë²ˆì—” ìˆ«ìë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì„ re.sub()ìœ¼ë¡œ ì£¼ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— forë¬¸ì„ ë¦¬ìŠ¤íŠ¸í•¨ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì—ˆìŠµë‹ˆë‹¤.<br> stoppedwordsì˜ ë‹¨ì–´(w)ê°€ ìˆ«ìê°€ ì•„ë‹ˆë¼ë©´(if not w.isdigit()) ë°˜í™˜í•˜ë¼ëŠ” ì˜ë¯¸ë¡œ ìˆ«ìì²˜ë¦¬ ë˜ì–´ noDigitWordsì— ë°˜í™˜ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ .isdigitë¥¼ ì‚¬ìš©í•˜ë©´ ìˆ«ìëŠ” trueë¡œ ìˆ«ì ì™¸ì—” falseë¡œ ë°˜í™˜ë˜ê¸° ë•Œë¬¸ì— notì„ ë¶™ì—¬ ë¬¸ìê°€ trueë¡œ ë°˜í™˜ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.



##### ë°©ë²•3) í•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ì œê±°í•˜ê¸°

ë¶ˆìš©ì–´ ì²˜ë¦¬ ì‹œì— ì‚¬ìš©ìê°€ ì›í•˜ëŠ” íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì ë˜ëŠ” ë¬¸ìë„ ìˆëŠ”ë° ì´ë•ŒëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ ì—‘ì…€íŒŒì¼ì„ ë§Œë“¤ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆ ë¶ˆìš©ì–´ì²˜ë¦¬ ë°©ë²•ì—ì„œ ì‚¬ìš©ëœ ì—‘ì…€íŒŒì¼ì€ ì•„ë˜ ë§í¬ì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°€ì ¸ì˜¨ ë¶ˆìš©ì–´ë¥¼ ì—‘ì…€íŒŒì¼(stopwordsEN.csv)ë¡œ ë§Œë“¤ì–´ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

[Stop words list (countwordsfree.com)]: https://countwordsfree.com/stopwords

```python
SET = pd.read_csv('stopwordsEN.csv',encoding='utf-8')
stopWords=set(SET)
```



ì—‘ì…€íŒŒì¼ì„ ì¼œì„œ ì›í•˜ëŠ” ë¶ˆìš©ì–´ë¥¼ ì²˜ë¦¬í•˜ê³  ë˜ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” ì‘ì—…ì„ í•´ë„ ê´œì°®ì§€ë§Œ ë²ˆê±°ë¡­ê¸° ë•Œë¬¸ì— ë°˜ë³µë¬¸ì„ í†µí•´ì„œ ì¼ë¶€ ë‹¨ì–´ë¥¼ ì •ì œí•˜ëŠ” ë°˜ë³µë¬¸ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
data=pd.read_csv('wos_ai_.csv',encoding='euc-kr').ABSTRACT

import re  #ì •ê·œì‹ ë¶ˆëŸ¬ì˜¤ê¸°

doc_set = []   #ì „ì²˜ë¦¬ ì „ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
words = []     #ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸

for doc in data :
    if type(doc) != float :        #ì¼ë¶€ ë‹¨ì–´ ì •ì œ/í†µí•©
        doc = doc.replace("model","")
        doc = doc.replace("data","")
        doc = doc.replace("use","")
        doc = doc.replace("algorithm","")
        doc = doc.replace("3-D","3D")
        doc = doc.replace("threeD","3D")
        doc_set.append(doc.replace("_"," "))
```

- doc.replace("a","b") : a ë¬¸ìë¥¼ bë¡œ ë°”ê¾¼ë‹¤. ex: apple -> bpple<br>ì´ë ‡ê²Œ replace êµ¬ë¬¸ì„ í™œìš©í•˜ë©´ ê°™ì€ ì˜ë¯¸ì˜ ë‹¨ì–´ë„ í•œ ë‹¨ì–´ë¡œ ë³€ê²½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  "3-D", "threeD","three-D" ëª¨ë‘ ê°™ì€ ë‹¨ì–´ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ ë‹¤ë¥¸ í‘œí˜„ë°©ì‹ì¼ ë•Œ replace í•¨ìˆ˜ë¥¼ í™œìš©í•´ ëª¨ë‘ "3D" ê°™ì€ ë‹¨ì–´ë¡œ ë³€ê²½í•˜ì—¬ ì „ì²˜ë¦¬í•˜ë©´ ìœ ìš©í•©ë‹ˆë‹¤.

<img src="/images/image-20230410055054277.png" alt="image-20230410055054277" style="zoom: 80%;" /><img src="/images/image-20230410055625780.png" alt="image-20230410055625780" style="zoom: 50%;" />

* ë¶ˆìš©ì–´ ì²˜ë¦¬ ì „  -->  ë¶ˆìš©ì–´ ì²˜ë¦¬ í›„<br> ì‚¬ì§„ê³¼ ê°™ì´ ì‹œê°í™”ë¥¼ í†µí•´ì„œ ì˜ë¯¸ì—†ì´ ë§ì´ ì“°ì´ëŠ” ë‹¨ì–´ë¥¼ ì œì™¸ì‹œì¼œì„œ í™•ì¸í•´ë³´ë‹ˆ ë‹¤ì–‘í•œ ë‹¨ì–´ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ì„œ í•„ìš”ì—†ëŠ” ë‹¨ì–´ëŠ” ê³„ì† ì •ì œí•´ë‚´ëŠ” ì‘ì—…ì´ í…ìŠ¤íŠ¸ë¶„ì„ ì „ì— í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

------

<br>

[\[ ì‹¤ìŠµíŒŒì¼ ë°”ë¡œê°€ê¸°(.git) \]](https://github.com/SongEunHwa/TextMining/commit/594ba36df6eddfca7e24d6a3adde8ed51a9d6362){: .btn .btn--primary .btn--large}{: .align-center}

[\[ ë‹¤ìŒì±•í„° ë°”ë¡œê°€ê¸°(12.ë‹¨ì–´ ì—°ê´€ë¶„ì„ê³¼ Word Network) \]](https://songeunhwa.github.io/textmining/12Textmining){: .btn .btn--primary .btn--large}{: .align-center}
