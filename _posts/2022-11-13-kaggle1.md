---
layout: single
title : '[Kaggle/CV] Intel ì´ë¯¸ì§€ ë¶„ë¥˜ : ê¸°ë³¸ ëª¨ë¸ë¡œ ë¬¸ì œ í•´ê²° ğŸŒ±' 
toc: true
---

### 1. Intel Image Classification

#### 1.0 ë°ì´í„° ì‚´í´ë³´ê¸°
ìºê¸€ì— ìˆëŠ” ì‹¤ì œ ë°ì´í„°ë“¤ì„ ë‹¤ë¤„ë³´ì. Intel Image Classificationì´ë¼ëŠ” ë°ì´í„°ë¥¼ ë‹¤ë£° ê²ƒì¸ë°, [ì´ ë§í¬](https://www.kaggle.com/datasets/puneet6060/intel-image-classification)ë¡œ ë“¤ì–´ê°€ë©´ ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ì„ ìˆ˜ ìˆë‹¤. ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì„ ì‚´í´ë³´ì. Intel Image Classification ë°ì´í„°ëŠ” 150x150 í¬ê¸°ì˜ 6ê°œì˜ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ëˆ„ì–´ì§„ 2ë§Œ 5ì²œì—¬ê°œì˜ ìì—° ê²½ê´€ì— ëŒ€í•œ ì‚¬ì§„ë“¤ì„ í¬í•¨í•˜ê³  ìˆë‹¤. 6ê°€ì§€ ì¹´í…Œê³ ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
* 'buildings' -> 0
* 'forest' -> 1
* 'glacier' -> 2
* 'mountain' -> 3
* 'sea' -> 4
* 'street' -> 5

ë˜í•œ í›ˆë ¨ ë°ì´í„°ëŠ” ì•½ 1ë§Œ 4ì²œê°œ, ê²€ì¦ ë°ì´í„°ëŠ” ì•½ 3ì²œê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œëŠ” ì•½ 7ì²œê°œì˜ ë°ì´í„°ê°€ ê°ê° zip íŒŒì¼ì— ë“¤ì–´ìˆë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆë‹¤. 

### 1.1 ê¸°ë³¸ CNN ëª¨ë¸ë¡œ ë¬¸ì œ í’€ê¸°

#### 1.1.0 í•„ìš”í•œ libraries import í•˜ê¸°
1.1ì—ì„œëŠ” Sequentialì„ ì´ìš©í•œ ê¸°ë³¸ì ì¸ CNN ëª¨ë¸ì„ êµ¬ì¶•í•œ í›„, ì‚¬ì§„ ë¶„ë¥˜ë¥¼ ì§„í–‰í•  ê²ƒì´ë‹¤. ê¸°ë³¸ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë‚®ì„ ìˆ˜ ìˆê² ì§€ë§Œ, ê¸°ë³¸ì ì¸ ëª¨ë¸ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. 1.1ì˜ ì½”ë“œëŠ” [ì´ ìºê¸€ëŸ¬](https://www.kaggle.com/code/ahmadjaved097/multiclass-image-classification-using-cnn)ì˜ ì½”ë“œë¥¼ ì°¸ê³ í–ˆë‹¤.
ë¨¼ì € í•„ìš”í•œ libraryë“¤ì„ import í•œë‹¤.


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

import cv2
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.models import Sequential
from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
```

#### 1.1.1 ë°ì´í„° ë¡œë“œí•˜ê¸°
ë¨¼ì € ë°ì´í„°ì˜ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ê³ , ë°ì´í„° ì´ë¯¸ì§€ë“¤ì˜ í¬ê¸°ê°€ 150x150ì´ê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ì§€ì •í•´ì£¼ê³ , ë°°ì¹˜ í¬ê¸°ë„ 32ë¡œ ì§€ì •í•´ì¤€ë‹¤.


```python
train_dataset_path = '/kaggle/input/intel-image-classification/seg_train/seg_train/'
validation_dataset_path = '/kaggle/input/intel-image-classification/seg_test/seg_test/'

IMG_WIDTH = 150
IMG_HEIGHT = 150
BATCH_SIZE = 32
```

[**ì´ì „ ê¸€**](https://hamin-chang.github.io/small/)ì—ì„œ ë‹¤ë¤˜ë“¯ì´, í›ˆë ¨ì˜ ê³¼ëŒ€ì í•©ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ **ì´ë¯¸ì§€ ì¦ì‹**ì„ ì´ìš©í•´ì„œ í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê² ë‹¤.


```python
train_datagen = ImageDataGenerator(rescale = 1.0/255,
                                  zoom_range = 0.2,
                                  width_shift_range = 0.2,
                                  height_shift_range = 0.2,
                                  fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(train_dataset_path,
                                                   target_size= (IMG_HEIGHT,IMG_HEIGHT),
                                                   batch_size= BATCH_SIZE,
                                                   class_mode= 'categorical',
                                                   shuffle = True)

validation_datagen = ImageDataGenerator(rescale= 1.0/255)
validation_generator= validation_datagen.flow_from_directory(validation_dataset_path,
                                                           target_size=(IMG_WIDTH,IMG_HEIGHT),
                                                           batch_size= BATCH_SIZE,
                                                           class_mode = 'categorical',
                                                           shuffle = True)

```

    Found 14034 images belonging to 6 classes.
    Found 3000 images belonging to 6 classes.


ìœ„ì˜ ì½”ë“œì—ì„œ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì´ 6ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ 14034ê°œì˜ ì´ë¯¸ì§€ê°€, ê²€ì¦ ë°ì´í„°ì—ëŠ” ì´ 6ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ 3000ê°œì˜ ì´ë¯¸ì§€ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ê·¸ ë‹¤ìŒ ì–´ë–¤ í´ë˜ìŠ¤ë“¤ì´ ìˆëŠ”ì§€ class_indicesë¥¼ ì´ìš©í•´ì„œ í´ë˜ìŠ¤ì˜ ì´ë¦„ë“¤ì„ ì¶œë ¥í•´ë³´ì.


```python
labels = {value : key for key, value in train_generator.class_indices.items()}
print("Label Mappings for classes present in the training and validation datasets\n")
for key , value in labels.items():
    print(f'{key} : {value}')
```

    Label Mappings for classes present in the training and validation datasets
    
    0 : buildings
    1 : forest
    2 : glacier
    3 : mountain
    4 : sea
    5 : street


#### 1.1.2 í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”í•˜ê¸°
ë‹¤ìŒ ì½”ë“œë¡œ í›ˆë ¨ ë°ì´í„° ì¤‘ ëª‡ê°€ì§€ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ì‹œê°í™” í•´ë³´ì.


```python
fig, ax = plt.subplots(nrows=2,ncols=5,figsize=(15,12))
idx = 0

for i in range(2):
    for j in range(5):
        label = labels[np.argmax(train_generator[0][1][idx])]
        ax[i, j].set_title(f'{label}')
        ax[i, j].imshow(train_generator[0][0][idx][:,:,:])
        ax[i, j].axis('off')
        idx += 1
        
plt.tight_layout()
plt.suptitle('Sample Training Images', fontsize=21)
plt.show()
```


    
![kaggle1_1](https://user-images.githubusercontent.com/77332628/201599062-d4bf63df-a572-4730-a5e3-4dc0c20e64fe.png)
    


#### 1.1.3 CNN ëª¨ë¸ í›ˆë ¨í•˜ê¸°
ì´ì œ í›ˆë ¨ ë°ì´í„°ê°€ ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë‹ˆ ê°„ë‹¨í•œ CNN ëª¨ë¸ì„ êµ¬ì¶•í•œ í›„ í›ˆë ¨ ì´ë¯¸ì§€ ë°ì´í„°ë“¤ë¡œ í›ˆë ¨í•´ë³´ì.


```python
def creat_model():
    model = Sequential([
        Conv2D(filters=128, kernel_size=(5, 5), padding='valid',input_shape=(IMG_WIDTH,IMG_HEIGHT,3)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2,2)),
        BatchNormalization(),
        
        Conv2D(filters=64, kernel_size=(3, 3), padding = 'valid', kernel_regularizer=l2(0.00005)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2, 2)),
        BatchNormalization(),
        
        Conv2D(filters=32, kernel_size=(3, 3), padding='valid', kernel_regularizer=l2(0.00005)),
        Activation('relu'),
        MaxPooling2D(pool_size=(2, 2)),
        BatchNormalization(),
        
        Flatten(),
        
        Dense(units=256, activation='relu'),
        Dropout(0.5),
        Dense(units=6, activation='softmax')
    ])
    
    return model
```


```python
cnn_model = creat_model()
print(cnn_model.summary())
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 146, 146, 128)     9728      
    _________________________________________________________________
    activation (Activation)      (None, 146, 146, 128)     0         
    _________________________________________________________________
    max_pooling2d (MaxPooling2D) (None, 73, 73, 128)       0         
    _________________________________________________________________
    batch_normalization (BatchNo (None, 73, 73, 128)       512       
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 71, 71, 64)        73792     
    _________________________________________________________________
    activation_1 (Activation)    (None, 71, 71, 64)        0         
    _________________________________________________________________
    max_pooling2d_1 (MaxPooling2 (None, 35, 35, 64)        0         
    _________________________________________________________________
    batch_normalization_1 (Batch (None, 35, 35, 64)        256       
    _________________________________________________________________
    conv2d_2 (Conv2D)            (None, 33, 33, 32)        18464     
    _________________________________________________________________
    activation_2 (Activation)    (None, 33, 33, 32)        0         
    _________________________________________________________________
    max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         
    _________________________________________________________________
    batch_normalization_2 (Batch (None, 16, 16, 32)        128       
    _________________________________________________________________
    flatten (Flatten)            (None, 8192)              0         
    _________________________________________________________________
    dense (Dense)                (None, 256)               2097408   
    _________________________________________________________________
    dropout (Dropout)            (None, 256)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 6)                 1542      
    =================================================================
    Total params: 2,201,830
    Trainable params: 2,201,382
    Non-trainable params: 448
    _________________________________________________________________
    None


ReduceLROnPlateauë¼ëŠ” ì½œë°±í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì¤€ë‹¤. ReduceLROnPlateau ì½œë°±í•¨ìˆ˜ëŠ” ì§€ì •í•œ ì§€í‘œê°€ ìµœì ì˜ ë°©í–¥ìœ¼ë¡œ ë” ì´ìƒ ê°€ì§€ ì•Šì„ ë•Œ learning_rate(í•™ìŠµë¥ )ì„ ë‚®ì¶”ëŠ” ì—­í• ì„ í•œë‹¤.


```python
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = np.sqrt(0.1), patience= 5)
```

ì˜µí‹°ë§ˆì´ì €ëŠ” Adamìœ¼ë¡œ ì •ì˜í•´ì¤€ í›„, ëª¨ë¸ì„ ì»´íŒŒì¼ í•˜ê³  í›ˆë ¨ì„ ì‹œì‘í•œë‹¤.


```python
optimizer = Adam(learning_rate=0.001)
cnn_model.compile(optimizer=optimizer,loss=CategoricalCrossentropy(),metrics=['accuracy'])

history = cnn_model.fit(train_generator, epochs=50, validation_data=validation_generator,
                       verbose = 2, callbacks=[reduce_lr])
```

    Epoch 1/50
    439/439 - 164s - loss: 1.9695 - accuracy: 0.5043 - val_loss: 0.9947 - val_accuracy: 0.6487
    Epoch 2/50
    439/439 - 97s - loss: 1.0539 - accuracy: 0.6252 - val_loss: 0.8171 - val_accuracy: 0.6927
    Epoch 3/50
    439/439 - 96s - loss: 0.9064 - accuracy: 0.6704 - val_loss: 1.0334 - val_accuracy: 0.6870
    Epoch 4/50
    439/439 - 96s - loss: 0.8291 - accuracy: 0.6971 - val_loss: 0.9408 - val_accuracy: 0.6500
    Epoch 5/50
    439/439 - 97s - loss: 0.7846 - accuracy: 0.7198 - val_loss: 0.7990 - val_accuracy: 0.6777
    Epoch 6/50
    439/439 - 95s - loss: 0.7495 - accuracy: 0.7316 - val_loss: 0.9228 - val_accuracy: 0.6257
    Epoch 7/50
    439/439 - 96s - loss: 0.7199 - accuracy: 0.7449 - val_loss: 1.3041 - val_accuracy: 0.6343
    Epoch 8/50
    439/439 - 96s - loss: 0.6787 - accuracy: 0.7597 - val_loss: 0.8333 - val_accuracy: 0.7040
    Epoch 9/50
    439/439 - 96s - loss: 0.6629 - accuracy: 0.7637 - val_loss: 0.9845 - val_accuracy: 0.6673
    Epoch 10/50
    439/439 - 96s - loss: 0.6458 - accuracy: 0.7752 - val_loss: 1.0875 - val_accuracy: 0.6523
    Epoch 11/50
    439/439 - 96s - loss: 0.5519 - accuracy: 0.8065 - val_loss: 0.5719 - val_accuracy: 0.8080
    Epoch 12/50
    439/439 - 98s - loss: 0.5079 - accuracy: 0.8264 - val_loss: 0.5859 - val_accuracy: 0.8073
    Epoch 13/50
    439/439 - 98s - loss: 0.4976 - accuracy: 0.8283 - val_loss: 0.5601 - val_accuracy: 0.8227
    Epoch 14/50
    439/439 - 98s - loss: 0.4858 - accuracy: 0.8348 - val_loss: 0.4887 - val_accuracy: 0.8400
    Epoch 15/50
    439/439 - 98s - loss: 0.4773 - accuracy: 0.8358 - val_loss: 0.4390 - val_accuracy: 0.8583
    Epoch 16/50
    439/439 - 98s - loss: 0.4643 - accuracy: 0.8360 - val_loss: 0.4203 - val_accuracy: 0.8653
    Epoch 17/50
    439/439 - 97s - loss: 0.4681 - accuracy: 0.8380 - val_loss: 0.4737 - val_accuracy: 0.8463
    Epoch 18/50
    439/439 - 97s - loss: 0.4585 - accuracy: 0.8445 - val_loss: 0.3976 - val_accuracy: 0.8750
    Epoch 19/50
    439/439 - 98s - loss: 0.4523 - accuracy: 0.8420 - val_loss: 0.4146 - val_accuracy: 0.8680
    Epoch 20/50
    439/439 - 97s - loss: 0.4456 - accuracy: 0.8477 - val_loss: 0.6212 - val_accuracy: 0.8157
    Epoch 21/50
    439/439 - 98s - loss: 0.4305 - accuracy: 0.8489 - val_loss: 0.4339 - val_accuracy: 0.8580
    Epoch 22/50
    439/439 - 97s - loss: 0.4262 - accuracy: 0.8536 - val_loss: 0.4181 - val_accuracy: 0.8643
    Epoch 23/50
    439/439 - 98s - loss: 0.4251 - accuracy: 0.8521 - val_loss: 0.4194 - val_accuracy: 0.8727
    Epoch 24/50
    439/439 - 98s - loss: 0.3876 - accuracy: 0.8677 - val_loss: 0.3593 - val_accuracy: 0.8897
    Epoch 25/50
    439/439 - 99s - loss: 0.3781 - accuracy: 0.8703 - val_loss: 0.5618 - val_accuracy: 0.8010
    Epoch 26/50
    439/439 - 98s - loss: 0.3765 - accuracy: 0.8711 - val_loss: 0.3758 - val_accuracy: 0.8817
    Epoch 27/50
    439/439 - 97s - loss: 0.3706 - accuracy: 0.8701 - val_loss: 0.3647 - val_accuracy: 0.8830
    Epoch 28/50
    439/439 - 98s - loss: 0.3682 - accuracy: 0.8749 - val_loss: 0.4075 - val_accuracy: 0.8707
    Epoch 29/50
    439/439 - 98s - loss: 0.3747 - accuracy: 0.8690 - val_loss: 0.3665 - val_accuracy: 0.8827
    Epoch 30/50
    439/439 - 98s - loss: 0.3598 - accuracy: 0.8752 - val_loss: 0.3667 - val_accuracy: 0.8767
    Epoch 31/50
    439/439 - 97s - loss: 0.3518 - accuracy: 0.8798 - val_loss: 0.3539 - val_accuracy: 0.8830
    Epoch 32/50
    439/439 - 96s - loss: 0.3456 - accuracy: 0.8807 - val_loss: 0.3468 - val_accuracy: 0.8867
    Epoch 33/50
    439/439 - 96s - loss: 0.3607 - accuracy: 0.8756 - val_loss: 0.3449 - val_accuracy: 0.8890
    Epoch 34/50
    439/439 - 96s - loss: 0.3436 - accuracy: 0.8821 - val_loss: 0.3526 - val_accuracy: 0.8847
    Epoch 35/50
    439/439 - 97s - loss: 0.3429 - accuracy: 0.8804 - val_loss: 0.3536 - val_accuracy: 0.8870
    Epoch 36/50
    439/439 - 96s - loss: 0.3451 - accuracy: 0.8789 - val_loss: 0.3575 - val_accuracy: 0.8863
    Epoch 37/50
    439/439 - 97s - loss: 0.3422 - accuracy: 0.8811 - val_loss: 0.3483 - val_accuracy: 0.8897
    Epoch 38/50
    439/439 - 97s - loss: 0.3455 - accuracy: 0.8809 - val_loss: 0.3429 - val_accuracy: 0.8937
    Epoch 39/50
    439/439 - 96s - loss: 0.3404 - accuracy: 0.8823 - val_loss: 0.3679 - val_accuracy: 0.8833
    Epoch 40/50
    439/439 - 96s - loss: 0.3396 - accuracy: 0.8838 - val_loss: 0.3618 - val_accuracy: 0.8860
    Epoch 41/50
    439/439 - 97s - loss: 0.3411 - accuracy: 0.8835 - val_loss: 0.3478 - val_accuracy: 0.8877
    Epoch 42/50
    439/439 - 96s - loss: 0.3420 - accuracy: 0.8816 - val_loss: 0.3507 - val_accuracy: 0.8900
    Epoch 43/50
    439/439 - 97s - loss: 0.3418 - accuracy: 0.8781 - val_loss: 0.3511 - val_accuracy: 0.8890
    Epoch 44/50
    439/439 - 97s - loss: 0.3360 - accuracy: 0.8834 - val_loss: 0.3472 - val_accuracy: 0.8870
    Epoch 45/50
    439/439 - 97s - loss: 0.3277 - accuracy: 0.8868 - val_loss: 0.3477 - val_accuracy: 0.8903
    Epoch 46/50
    439/439 - 96s - loss: 0.3324 - accuracy: 0.8865 - val_loss: 0.3458 - val_accuracy: 0.8897
    Epoch 47/50
    439/439 - 97s - loss: 0.3317 - accuracy: 0.8862 - val_loss: 0.3466 - val_accuracy: 0.8897
    Epoch 48/50
    439/439 - 96s - loss: 0.3342 - accuracy: 0.8836 - val_loss: 0.3515 - val_accuracy: 0.8883
    Epoch 49/50
    439/439 - 97s - loss: 0.3379 - accuracy: 0.8847 - val_loss: 0.3472 - val_accuracy: 0.8887
    Epoch 50/50
    439/439 - 96s - loss: 0.3312 - accuracy: 0.8830 - val_loss: 0.3514 - val_accuracy: 0.8887


#### 1.1.4 í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”í•˜ê¸°
í›ˆë ¨ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•œ ê²°ê³¼ë“¤ ì¦‰ train_accuracy, val_accuracy, train_loss, val_loss ë“±ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ëŠ” ê³¼ì •ì´ë‹¤.


```python
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

train_loss = history.history['loss']
val_loss = history.history['val_loss']

learning_rate = history.history['lr']

fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12,10))

ax[0].set_title('Training Accuracy vs. Epochs')
ax[0].plot(train_accuracy, 'o-', label='Train Accuracy')
ax[0].plot(val_accuracy, 'o-', label='Validation Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].legend(loc='best')

ax[1].set_title('Training/Validation Loss vs. Epochs')
ax[1].plot(train_loss, 'o-', label='Train Loss')
ax[1].plot(val_loss, 'o-', label='Validation Loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend(loc='best')

ax[2].set_title('Learning Rate vs. Epochs')
ax[2].plot(learning_rate, 'o-', label='Learning Rate')
ax[2].set_xlabel('Epochs')
ax[2].set_ylabel('Loss')
ax[2].legend(loc='best')

plt.tight_layout()
plt.show()
```


![kaggle1_2](https://user-images.githubusercontent.com/77332628/201599068-9e0ea075-af87-4fba-ae24-636429f60614.png)


#### 1.1.5 í…ŒìŠ¤íŠ¸ ë°ì´í„°ì„¸íŠ¸ì—ì„œ ì„±ëŠ¥ í‰ê°€í•˜ê¸°
ë§ˆì§€ë§‰ìœ¼ë¡œ í›ˆë ¨í•œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€í•œë‹¤. ë¨¼ì € í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°ì´í„° ì¦ì‹ ê¸°ë²•ì¸ ImageDataGeneratorë¡œ ìƒì„±í•œë‹¤.


```python
test_dataset = '/kaggle/input/intel-image-classification/seg_test/seg_test/'

test_datagen = ImageDataGenerator(rescale=1.0/255)
test_generator = test_datagen.flow_from_directory(test_dataset,
                                                 shuffle=False,
                                                 batch_size=BATCH_SIZE,
                                                 target_size=(IMG_WIDTH,IMG_HEIGHT),
                                                 class_mode='categorical')
```

    Found 3000 images belonging to 6 classes.


ìœ„ì˜ ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ 6ê°œì˜ í´ë˜ìŠ¤ì™€ ë§¤ì¹­ì´ ë˜ì–´ìˆì§€ ì•Šì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì´ë¯¸ì§€ë“¤ì´ 6ê°œì˜ í´ë˜ìŠ¤ ì¤‘ ì–´ë””ì— í•´ë‹¹í•˜ëŠ”ì§€ ë§¤ì¹­ì„ í•´ë³´ë„ë¡ í•œë‹¤.


```python
predictions = cnn_model.predict(test_generator)

fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12,10))
idx = 0

for i in range(2):
    for j in range(5):
        predicted_label = labels[np.argmax(predictions[idx])]
        ax[i, j].set_title(f'{predicted_label}')
        ax[i, j].imshow(test_generator[0][0][idx])
        ax[i, j].axis('off')
        idx += 1
        
plt.tight_layout()
plt.suptitle('Test Dataset Predictions',fontsize=20)
plt.show()
```


    
![kaggle1_3](https://user-images.githubusercontent.com/77332628/201599071-098accab-0a4e-44a4-8300-181796ab0e1a.png)
    


í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì§„í–‰í•œ í›„, test ì†ì‹¤ê³¼ test ì •í™•ë„ë¥¼ êµ¬í•œë‹¤.


```python
test_loss, test_accuracy = cnn_model.evaluate(test_generator, batch_size=BATCH_SIZE)
print(f'í…ŒìŠ¤íŠ¸ ì†ì‹¤ : {test_loss}')
print(f'í…ŒìŠ¤íŠ¸ ì •í™•ë„ : {test_accuracy}')
```

    94/94 [==============================] - 5s 57ms/step - loss: 0.3514 - accuracy: 0.8887
    í…ŒìŠ¤íŠ¸ ì†ì‹¤ : 0.3514269292354584
    í…ŒìŠ¤íŠ¸ ì •í™•ë„ : 0.8886666893959045


#### 1.1.6 Confusion Matrix ì‚¬ìš©í•´ì„œ ì •í™•ë„ ì‹œê°í™”í•˜ê¸°
Confusion Maxtrixë¥¼ ì´ìš©í•´ì„œ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ ì‚´í´ë³´ì.


```python
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

cf_mtx = confusion_matrix(y_true, y_pred)

group_counts = ["{0:0.0f}".format(value) for value in cf_mtx.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cf_mtx.flatten()/np.sum(cf_mtx)]
box_labels = [f"{v1}\n({v2})" for v1, v2 in zip(group_counts, group_percentages)]
box_labels = np.asarray(box_labels).reshape(6, 6)

plt.figure(figsize = (12, 10))
sns.heatmap(cf_mtx, xticklabels=labels.values(), yticklabels=labels.values(),
           cmap="YlGnBu", fmt="", annot=box_labels)
plt.xlabel('Predicted Classes')
plt.ylabel('True Classes')
plt.show()

print(classification_report(y_true, y_pred, target_names=labels.values()))
```


    
![kaggle1_4](https://user-images.githubusercontent.com/77332628/201599074-60a49b0e-bf7f-4052-bf2b-0ea5657b6bd6.png)
    


                  precision    recall  f1-score   support
    
       buildings       0.83      0.90      0.86       437
          forest       0.96      0.98      0.97       474
         glacier       0.85      0.84      0.84       553
        mountain       0.87      0.81      0.84       525
             sea       0.91      0.93      0.92       510
          street       0.92      0.89      0.90       501
    
        accuracy                           0.89      3000
       macro avg       0.89      0.89      0.89      3000
    weighted avg       0.89      0.89      0.89      3000
    


#### 1.1.7 ì˜ëª»ëœ ì˜ˆì¸¡ê°’ ì‚´í´ë³´ê¸°
ì´ì œ ê° í´ë˜ìŠ¤ë§ˆë‹¤ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì˜ˆì¸¡í–ˆê³  ì–¼ë§ˆë‚˜ ì˜ëª» ì˜ˆì¸¡í–ˆëŠ”ì§€ ì•Œì•„ë´¤ìœ¼ë‹ˆ, ëª¨ë¸ì´ ì–´ëŠ ë¶€ë¶„ì—ì„œ ì˜ëª»ëœ ì˜ˆì¸¡ì„ í–ˆëŠ”ì§€ ì•Œì•„ë³´ê³ , ì˜ëª» ì˜ˆì¸¡í•œ ì´ë¯¸ì§€ì˜ ì •ë‹µì€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ì.


```python
errors = (y_true - y_pred != 0)
y_true_errors = y_true[errors]
y_pred_errors = y_pred[errors]

test_images = test_generator.filenames
test_img = np.asarray(test_images)[errors]

fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(12, 10))
idx = 0

for i in range(2):
    for j in range(5):
        idx = np.random.randint(0, len(test_img))
        true_index = y_true_errors[idx]
        true_label = labels[true_index]
        predicted_index = y_pred_errors[idx]
        predicted_label = labels[predicted_index]
        ax[i, j].set_title(f"True Label: {true_label} \n Predicted Label: {predicted_label}")
        img_path = os.path.join(test_dataset, test_img[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        ax[i, j].imshow(img)
        ax[i, j].axis("off")

plt.tight_layout()
plt.suptitle('Wrong Predictions made on test set', fontsize=20)
plt.show()
```


    

![kaggle1_5](https://user-images.githubusercontent.com/77332628/201599077-8e75cf76-76b2-441c-9f84-91c877ff2abc.png)
    

