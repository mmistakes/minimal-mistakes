

# [AI_8ê¸°] 6ì¡° ë¨¸ì‹ ëŸ¬ë‹ & ë”¥ëŸ¬ë‹ íŒ€ê³¼ì œ

| **íŒ€ì›** | âœ­ë°•ì„±ê·œ                                                                                            | ê¹€ë¯¼ì²                                                                                               | ì´ì‹œí—Œ                                                                                            | ë°•ìœ¤ì§€                                                                                             |
|:------:|:-----------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|
|        | ![á„‡á…¡á†¨á„‰á…¥á†¼á„€á…²á„‚á…µá†·](https://github.com/user-attachments/assets/40f97c52-c562-44b0-bef6-12289e149d27) | ![á„€á…µá†·á„†á…µá†«á„á…¥á†¯á„‚á…µá†·](https://github.com/user-attachments/assets/28b83bd5-13c2-4249-beab-64f7567e1816) | ![á„‹á…µá„‰á…µá„’á…¥á†«á„‚á…µá†·](https://github.com/user-attachments/assets/7b91b2aa-c113-44ed-8f41-e8df1ef7d06d) | ![á„‡á…¡á†¨á„‹á…²á†«á„Œá…µá„‚á…µá†·](https://github.com/user-attachments/assets/8d5be377-1a58-4f88-9ee2-176d1e1d162e) |
| **ì—­í• ** | ì˜¤ë¥˜ ì œì–´ ë° REPO ê´€ë¦¬ <br> ì´ëª¨í‹°ì½˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€                                                                | ëª¨ë¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ë° <br> ê¸°ëŠ¥ ê°œì„                                                                                 | ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë°ì´í„° í†µê³„ <br> ë§ˆí¬ë‹¤ìš´ & README.md ì‘ì„±                                                          | GIT ì¶©ëŒ ê´€ë¦¬ ë° íŒ€ì› ì½”ë“œ ë¦¬ë·° <br> ëª¨ë¸ ë³„ ì‹œê°í™”                                                                 |

## ê°œë°œ í™˜ê²½

![://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1566791609/noticon/nen1y11gazeqhejw7nmhttps1.png](https://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1566791609/noticon/nen1y11gazeqhejw7nm1.png) ![https://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1626170585/noticon/uqui2rrxtt26ngudnhdu.png](https://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1626170585/noticon/uqui2rrxtt26ngudnhdu.png)![https://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1632975248/noticon/sph4ujixspcnhzpw8zky.png](https://noticon-static.tammolo.com/dgggcrkxq/image/upload/v1632975248/noticon/sph4ujixspcnhzpw8zky.png)

# ëª©ì°¨

- ## [íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡](#%EF%B8%8F-íƒ€ì´íƒ€ë‹‰-ìƒì¡´ì-ì˜ˆì¸¡)
  
  - ### [ëª©í‘œ](#ëª©í‘œ-2)
    #### 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    #### 2. feature ë¶„ì„
    #### 3. feature engineering
    #### 4. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° <br> (Logistic Regression, Decision Tree, XGBoost)

  - ### [ì¶”ê°€ ëª©í‘œ](#ì¶”ê°€-ëª©í‘œ-2)
    #### 5. ëª¨ë¸ë³„ ì‹œê°í™” ìë£Œ (ì¶”ê°€)
    #### 6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ì¶”ê°€)

- ## [ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„](#-ì˜í™”-ë¦¬ë·°-ê°ì„±-ë¶„ì„)

  - ### [ëª©í‘œ](#ëª©í‘œ-3)
    #### 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    #### 2. ë°ì´í„° ì „ì²˜ë¦¬
    #### 3. feature ë¶„ì„ (EDA)
    #### 4. ë¦¬ë·° ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° (LSTM)

  - ### [ì¶”ê°€ ëª©í‘œ](#ì¶”ê°€-ëª©í‘œ-3) 
    - [x] NLP ì´ìš©
    - [x] ê¸ì • / ë¶€ì • ë¦¬ë·°ì˜ ì›Œë“œ í´ë¼ìš°ë“œ ê·¸ë ¤ë³´ê¸°

  - ### [ì˜ˆì¸¡ ëª¨ë¸ ê¸°ëŠ¥ ê°œì„  (ì¶”ê°€)](#ì˜ˆì¸¡-ëª¨ë¸-ê¸°ëŠ¥-ê°œì„ -ì¶”ê°€-1)
   - #### ì´ëª¨í‹°ì½˜ ì „ì²˜ë¦¬
   - #### ëª¨ë¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ë° ê¸°ëŠ¥ ê°œì„ 

# â›´ï¸ íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡

> íƒ€ì´íƒ€ë‹‰ íƒ‘ìŠ¹ê° ë°ì´í„°ì…‹ì„ í™œìš©í•´ ìƒì¡´ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” í”„ë¡œì íŠ¸

## ëª©í‘œ

<details>
<summary> 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°</summary>

```python
import seaborn as sns

titanic = sns.load_dataset('titanic')
```

> titanic Dataset

<!-- dataset df -->

<div> 
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>class</th>
      <th>who</th>
      <th>adult_male</th>
      <th>deck</th>
      <th>embark_town</th>
      <th>alive</th>
      <th>alone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>0</td>
      <td>2</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>13.0000</td>
      <td>S</td>
      <td>Second</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
    </tr>
    <tr>
      <th>887</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>B</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
    </tr>
    <tr>
      <th>888</th>
      <td>0</td>
      <td>3</td>
      <td>female</td>
      <td>NaN</td>
      <td>1</td>
      <td>2</td>
      <td>23.4500</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
    </tr>
    <tr>
      <th>889</th>
      <td>1</td>
      <td>1</td>
      <td>male</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>C</td>
      <td>First</td>
      <td>man</td>
      <td>True</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>True</td>
    </tr>
    <tr>
      <th>890</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
      <td>Q</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Queenstown</td>
      <td>no</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
<p>891 rows Ã— 15 columns</p>
</div>
</details>
<br>

<details>
<summary>2. feature ë¶„ì„</summary>

> ë°ì´í„° í”„ë ˆì„ ì²« 5í–‰

```python
titanic.head()
```

<!-- head df -->

<div>
<table border="1" class="dataframe">
   <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>class</th>
      <th>who</th>
      <th>adult_male</th>
      <th>deck</th>
      <th>embark_town</th>
      <th>alive</th>
      <th>alone</th>
    </tr>
   </thead>
   <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
    </tr>
   </tbody>
   </table>
   </div>

> í†µê³„ í™•ì¸ 

```python
titanic.describe()
```

`íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ì£¼ìš” í•­ëª© (í–‰ row)`

| <span style="color:blue">**í•­ëª©**</span>       | <span style="color:blue">**ì„¤ëª…**</span> |
| -------------------------------------------- | -------------------------------------- |
| <span style="color:blue">**survived**</span> | ìŠ¹ê° ìƒì¡´ ì—¬ë¶€ (0 = ì‚¬ë§, 1 = ìƒì¡´)              |
| <span style="color:green">**pclass**</span>  | ê°ì‹¤ ë“±ê¸‰ (1 = 1ë“±ì„, 2 = 2ë“±ì„, 3 = 3ë“±ì„)      |
| <span style="color:purple">**age**</span>    | ìŠ¹ê° ë‚˜ì´                                  |
| <span style="color:orange">**sibsp**</span>  | ë™ë°˜í•œ í˜•ì œìë§¤ ë° ë°°ìš°ì ìˆ˜                       |
| <span style="color:orange">**parch**</span>  | ë™ë°˜í•œ ë¶€ëª¨ ë° ìë…€ ìˆ˜                          |
| <span style="color:teal">**fare**</span>     | ìŠ¹ê°ì´ ì§€ë¶ˆí•œ ìš´ì„ ê¸ˆì•¡                          |

`íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ì£¼ìš” í†µê³„ (ì—´ Column)`

| <span style="color:blue">**ì§€í‘œ**</span>    | <span style="color:blue">**ì„¤ëª…**</span>  |
| ----------------------------------------- | --------------------------------------- |
| <span style="color:blue">**count**</span> | ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” í•­ëª©ì˜ ê°œìˆ˜ (ê²°ì¸¡ì¹˜ë¥¼ ì œì™¸í•œ ê°’ì˜ ê°œìˆ˜)       |
| <span style="color:green">**mean**</span> | ê°’ë“¤ì˜ í‰ê·                                   |
| <span style="color:purple">**std**</span> | í‘œì¤€í¸ì°¨ (ë°ì´í„°ê°€ í‰ê· ìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ í¼ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„)      |
| <span style="color:orange">**min**</span> | ë°ì´í„°ì˜ ìµœì†Œê°’                                |
| <span style="color:teal">**25%**</span>   | í•˜ìœ„ 25%ì— í•´ë‹¹í•˜ëŠ” ê°’. ë°ì´í„°ì˜ 25%ê°€ ì´ ê°’ë³´ë‹¤ ì‘ìŒ      |
| <span style="color:orange">**50%**</span> | ì¤‘ìœ„ê°’ (ë°ì´í„°ì˜ ì¤‘ê°„ ê°’). ë°ì´í„°ì˜ 50%ê°€ ì´ ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŒ |
| <span style="color:teal">**75%**</span>   | ìƒìœ„ 25%ì— í•´ë‹¹í•˜ëŠ” ê°’. ë°ì´í„°ì˜ 75%ê°€ ì´ ê°’ë³´ë‹¤ ì‘ìŒ      |
| <span style="color:red">**max**</span>    | ë°ì´í„°ì˜ ìµœëŒ€ê°’                                |

> ë°ì´í„°ì…‹ í†µê³„

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>
<br>

</details>

<br>

<details>
<summary>
3. feature engineering
</summary>

> ê²°ì¸¡ì¹˜ ì²˜ë¦¬

`ê²°ì¸¡ì¹˜ ê°¯ìˆ˜ í™•ì¸`

```python
titanic.isnull().sum()
```

| <span style="color:blue">**í•­ëª©**</span>            | <span style="color:blue">**ê²°ì¸¡ì¹˜ ìˆ˜**</span> |
| ------------------------------------------------- | ----------------------------------------- |
| <span style="color:blue">**survived**</span>      | 0                                         |
| <span style="color:green">**pclass**</span>       | 0                                         |
| <span style="color:purple">**sex**</span>         | 0                                         |
| <span style="color:orange">**age**</span>         | 177                                       |
| <span style="color:teal">**sibsp**</span>         | 0                                         |
| <span style="color:blue">**parch**</span>         | 0                                         |
| <span style="color:green">**fare**</span>         | 0                                         |
| <span style="color:purple">**embarked**</span>    | 2                                         |
| <span style="color:orange">**class**</span>       | 0                                         |
| <span style="color:teal">**who**</span>           | 0                                         |
| <span style="color:blue">**adult_male**</span>    | 0                                         |
| <span style="color:green">**deck**</span>         | 688                                       |
| <span style="color:purple">**embark_town**</span> | 2                                         |
| <span style="color:orange">**alive**</span>       | 0                                         |
| <span style="color:teal">**alone**</span>         | 0                                         |

`ê²°ì¸¡ì¹˜ ê°’ ëŒ€ì²´`

```python
#Age(ë‚˜ì´)ì˜ ê²°ì¸¡ì¹˜ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ, Embarked(ìŠ¹ì„  í•­êµ¬)ì˜ ê²°ì¸¡ì¹˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´. 
titanic['age'].fillna(titanic['age'].median(), inplace=True)
titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)

# ëŒ€ì²´í•œ í›„ì—, ëŒ€ì²´ ê²°ê³¼ë¥¼ isnull() í•¨ìˆ˜ì™€ sum()  í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ í™•ì¸
print(titanic['age'].isnull().sum())
print(titanic['embarked'].isnull().sum())
```

`ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ì¸ì½”ë”©`

```python
# Sex(ì„±ë³„)ë¥¼ ë‚¨ìëŠ” 0, ì—¬ìëŠ” 1ë¡œ ë³€í™˜. 
# alive(ìƒì¡´ì—¬ë¶€)ë¥¼ TrueëŠ” 1, FalseëŠ” 0ìœ¼ë¡œ ë³€í™˜. 
# Embarked(ìŠ¹ì„  í•­êµ¬)ëŠ” â€˜Câ€™ëŠ” 0ìœ¼ë¡œ, QëŠ” 1ìœ¼ë¡œ, â€˜Sâ€™ëŠ” 2ë¡œ ë³€í™˜. 
# ëª¨ë‘ ë³€í™˜í•œ í›„ì—, ë³€í™˜ ê²°ê³¼ë¥¼ head í•¨ìˆ˜ë¥¼ ì´ìš©í•´ í™•ì¸. 


titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['alive'] = titanic['alive'].map({'no': 1, 'yes': 0})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2,})

print(titanic['sex'].head())
print(titanic['alive'].head())
print(titanic['embarked'].head())
```

`ìƒˆë¡œìš´ feature ìƒì„±`

```python
#Sibsp , Parch ë¥¼ í†µí•´ family_size ìƒì„±
#ìƒˆë¡œìš´ Featureë¥¼ headí•¨ìˆ˜ë¥¼ ì´ìš©í•´ í™•ì¸

titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1

print(titanic['family_size'].head())
```

> ê°€ì¡±êµ¬ì„±ì› í•­ëª© ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>class</th>
      <th>who</th>
      <th>adult_male</th>
      <th>deck</th>
      <th>embark_town</th>
      <th>alive</th>
      <th>alone</th>
      <th>family_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>1</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>0</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>2</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>0</td>
      <td>True</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Southampton</td>
      <td>0</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>2</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>1</td>
      <td>True</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>13.0000</td>
      <td>2</td>
      <td>Second</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>1</td>
      <td>True</td>
      <td>1</td>
    </tr>
    <tr>
      <th>887</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>2</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>B</td>
      <td>Southampton</td>
      <td>0</td>
      <td>True</td>
      <td>1</td>
    </tr>
    <tr>
      <th>888</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>28.0</td>
      <td>1</td>
      <td>2</td>
      <td>23.4500</td>
      <td>2</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>1</td>
      <td>False</td>
      <td>4</td>
    </tr>
    <tr>
      <th>889</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>0</td>
      <td>First</td>
      <td>man</td>
      <td>True</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>0</td>
      <td>True</td>
      <td>1</td>
    </tr>
    <tr>
      <th>890</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
      <td>1</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Queenstown</td>
      <td>1</td>
      <td>True</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>891 rows Ã— 16 columns</p>
</div>

</details>
<br>

<details>
<summary>
4. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° (Logistic Regression, Decision Tree, XGBoost)  
</summary>

`ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì§„í–‰`

```py
#featureì™€ target ë¶„ë¦¬

titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']]
X = titanic.drop('survived', axis=1) # feature
y = titanic['survived'] # target

# xëŠ” ìŠ¹ê°ì˜ ìƒì¡´ ì—¬ë¶€ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ì—´ì„ í•™ìŠµì— ì‚¬ìš©í•  íŠ¹ì§•
# yëŠ” ìŠ¹ê°ì´ ìƒì¡´í–ˆëŠ”ì§€ì˜ ì—¬ë¶€
# xë¡œ yë¥¼ ì˜ˆì¸¡
```

> Logistic Regression

```py
# Logistic Regression

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = model.predict(X_test)

# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:\n{classification_report(y_test, y_pred)}")
```

> ğŸ” Logistic Regression ê²°ê³¼ ìš”ì•½

| **ì§€í‘œ**                | **í¬ìƒì (0)**                      | **ìƒì¡´ì (1)**                      |
| --------------------- | -------------------------------- | -------------------------------- |
| **ì •ë°€ë„ (Precision)**   | 0.82 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'í¬ìƒì' ì¤‘ ì‹¤ì œ í¬ìƒì ë¹„ìœ¨) | 0.78 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'ìƒì¡´ì' ì¤‘ ì‹¤ì œ ìƒì¡´ì ë¹„ìœ¨) |
| **ì¬í˜„ìœ¨ (Recall)**      | 0.86 (ì‹¤ì œ í¬ìƒì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       | 0.73 (ì‹¤ì œ ìƒì¡´ì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       |
| **F1-ìŠ¤ì½”ì–´ (F1-Score)** | 0.84 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            | 0.76 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            |
| **ì§€ì› (Support)**      | 105                              | 74                               |

| **í‰ê·  ì§€í‘œ**          | **ê°’**                                         |
| ------------------ | --------------------------------------------- |
| **ì •í™•ë„ (Accuracy)** | 0.80 (ì „ì²´ ë°ì´í„°ì—ì„œ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)                    |
| **Macro í‰ê· **       | Precision: 0.80, Recall: 0.79, F1-Score: 0.80 |
| **Weighted í‰ê· **    | Precision: 0.80, Recall: 0.80, F1-Score: 0.80 |

**ìš”ì•½**: Logistic Regression ëª¨ë¸ì€ ì•½ 80%ì˜ ì •í™•ë„ë¥¼ ë³´ì´ë©°, í¬ìƒìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ìˆì–´ì„œ ì¬í˜„ìœ¨ì´ ë†’ì•„(0.86) í¬ìƒìë¥¼ ì˜ ì˜ˆì¸¡. ìƒì¡´ìì— ëŒ€í•œ ì¬í˜„ìœ¨ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„(0.73) ìƒì¡´ìë¥¼ ë†“ì¹˜ëŠ” ê²½í–¥ì´ ì•½ê°„ ìˆìŒ.

> Decision Tree

```py
#Decision Tree

from sklearn.tree import DecisionTreeClassifier  # Decision Tree ë¶„ë¥˜ê¸°
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred = model.predict(X_test)

# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:\n{classification_report(y_test, y_pred)}")
```

> ğŸ” Decision Tree ëª¨ë¸ ê²°ê³¼ ìš”ì•½

| **ì§€í‘œ**                | **í¬ìƒì (0)**                      | **ìƒì¡´ì (1)**                      |
| --------------------- | -------------------------------- | -------------------------------- |
| **ì •ë°€ë„ (Precision)**   | 0.83 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'í¬ìƒì' ì¤‘ ì‹¤ì œ í¬ìƒì ë¹„ìœ¨) | 0.70 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'ìƒì¡´ì' ì¤‘ ì‹¤ì œ ìƒì¡´ì ë¹„ìœ¨) |
| **ì¬í˜„ìœ¨ (Recall)**      | 0.76 (ì‹¤ì œ í¬ìƒì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       | 0.78 (ì‹¤ì œ ìƒì¡´ì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       |
| **F1-ìŠ¤ì½”ì–´ (F1-Score)** | 0.80 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            | 0.74 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            |
| **ì§€ì› (Support)**      | 105                              | 74                               |

| **í‰ê·  ì§€í‘œ**          | **ê°’**                                         |
| ------------------ | --------------------------------------------- |
| **ì •í™•ë„ (Accuracy)** | 0.77 (ì „ì²´ ë°ì´í„°ì—ì„œ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)                    |
| **Macro í‰ê· **       | Precision: 0.77, Recall: 0.77, F1-Score: 0.77 |
| **Weighted í‰ê· **    | Precision: 0.78, Recall: 0.77, F1-Score: 0.77 |

**ìš”ì•½**: Decision Tree ëª¨ë¸ì€ ì•½ 77%ì˜ ì •í™•ë„ë¥¼ ë³´ì´ë©°, í¬ìƒì ì˜ˆì¸¡ì—ì„œ ì •ë°€ë„ê°€ ë†’ì•„(0.83) í¬ìƒìë¥¼ ì˜ ë¶„ë¥˜í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ. ìƒì¡´ìì˜ ì¬í˜„ìœ¨ì´ ë‹¤ì†Œ ë†’ì•„(0.78) ìƒì¡´ìë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°ëŠ” ì ìœ¼ë‚˜, ì •ë°€ë„ê°€ í¬ìƒìì— ë¹„í•´ ë‚®ì•„(0.70) ìƒì¡´ì ì˜ˆì¸¡ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.

> XGBoost

```py
import xgboost as xgb
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# XGBoost ëª¨ë¸ ìƒì„±
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# ëª¨ë¸ í•™ìŠµ
xgb_model.fit(X_train_scaled, y_train)

# ì˜ˆì¸¡
y_pred_xgb = xgb_model.predict(X_test_scaled)

# í‰ê°€
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost ëª¨ë¸ì˜ MSE: {mse_xgb}')
```

> ğŸ” XGBoost ëª¨ë¸ ê²°ê³¼ ìš”ì•½

| **ì§€í‘œ**                | **í¬ìƒì (0)**                      | **ìƒì¡´ì (1)**                      |
| --------------------- | -------------------------------- | -------------------------------- |
| **ì •ë°€ë„ (Precision)**   | 0.82 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'í¬ìƒì' ì¤‘ ì‹¤ì œ í¬ìƒì ë¹„ìœ¨) | 0.78 (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 'ìƒì¡´ì' ì¤‘ ì‹¤ì œ ìƒì¡´ì ë¹„ìœ¨) |
| **ì¬í˜„ìœ¨ (Recall)**      | 0.86 (ì‹¤ì œ í¬ìƒì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       | 0.73 (ì‹¤ì œ ìƒì¡´ì ì¤‘ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)       |
| **F1-ìŠ¤ì½”ì–´ (F1-Score)** | 0.84 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            | 0.76 (ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· )            |
| **ì§€ì› (Support)**      | 105                              | 74                               |

| **í‰ê·  ì§€í‘œ**          | **ê°’**                                         |
| ------------------ | --------------------------------------------- |
| **ì •í™•ë„ (Accuracy)** | 0.80 (ì „ì²´ ë°ì´í„°ì—ì„œ ì •í™•íˆ ì˜ˆì¸¡í•œ ë¹„ìœ¨)                    |
| **Macro í‰ê· **       | Precision: 0.80, Recall: 0.79, F1-Score: 0.80 |
| **Weighted í‰ê· **    | Precision: 0.80, Recall: 0.80, F1-Score: 0.80 |

**ìš”ì•½**: XGBoost ëª¨ë¸ì€ ì•½ 80%ì˜ ì •í™•ë„ë¥¼ ë³´ì´ë©°, í¬ìƒì ì˜ˆì¸¡ì—ì„œ ë†’ì€ ì¬í˜„ìœ¨(0.86)ë¡œ ì‹¤ì œ í¬ìƒìë¥¼ ì˜ ì‹ë³„í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ. ìƒì¡´ì ì˜ˆì¸¡ì—ì„œëŠ” ì •ë°€ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì•„(0.78) ìƒì¡´ìë¥¼ ë” ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ë©°, ìƒì¡´ì ì¬í˜„ìœ¨ì€ 0.73ìœ¼ë¡œ ë‹¤ì†Œ ë‚®ìŒ. ì „ë°˜ì ìœ¼ë¡œ, XGBoost ëª¨ë¸ì€ í¬ìƒì ì‹ë³„ì— ê°•ì ì„ ë³´ì„.

</details>

## ì¶”ê°€ ëª©í‘œ

<details>
<summary>
5. ëª¨ë¸ë³„ ì‹œê°í™” ìë£Œ (ì¶”ê°€)
</summary>

> í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (Confusion Matrix)

```py
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc

# Confusion Matrix ì‹œê°í™”
conf_matrix = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
```

![ConfusionMatrix](https://github.com/user-attachments/assets/70734599-86ee-4d8a-ae0a-c4fe5a317771)

> íŠ¹ì„± ì¤‘ìš”ë„ (íšŒê·€ ê³„ìˆ˜) ì‹œê°í™”

```py
feature_importance = model.coef_[0]  # ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ ê³„ìˆ˜
features = X.columns

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.barh(features, feature_importance, color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.title('Feature Importance in Logistic Regression')
plt.show()
```

![LogisticRegression](https://github.com/user-attachments/assets/e439404f-f671-45ad-a209-e46255b45fb8)

> ê²°ì • íŠ¸ë¦¬ ì‹œê°í™” (Decision Tree)

```py
from sklearn.tree import plot_tree

# min_samples_split, min_samples_leafë¡œ ëª¨ë¸ ì œì•½í•˜ê¸°
model = DecisionTreeClassifier(random_state=42, min_samples_split=20, min_samples_leaf=10)
model.fit(X_train, y_train)

plt.figure(figsize=(20,10))
plot_tree(model, filled=True, feature_names=X.columns, class_names=['Not Survived', 'Survived'], max_depth=4)
plt.title('Simplified Decision Tree')
plt.show()
```

![DecisionTree](https://github.com/user-attachments/assets/9664fe93-9318-4619-8edc-c440b41dc8d0)

> XGBoost íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”

```py
# feature_importances_: XGBoost ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ë¥¼ ê° íŠ¹ì„±ì—ì„œ ì–»ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
# íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
feature_importance = xgb_model.feature_importances_
features = X.columns


# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.barh(features, feature_importance, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance in XGBoost')
plt.show()
```

![XgBoost1](https://github.com/user-attachments/assets/322fe761-d333-4853-80f0-da1a5080b558)

</details>
<br>
<details>
<summary>
6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ì¶”ê°€)
</summary>

> ğŸ³ íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ê²°ê³¼ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

| **ëª¨ë¸**                  | **Accuracy** | <span style="color:red">**Precision (í¬ìƒì)**</span> | <span style="color:blue">**Precision (ìƒì¡´ì)**</span> | <span style="color:red">**Recall (í¬ìƒì)**</span> | <span style="color:blue">**Recall (ìƒì¡´ì)**</span> | <span style="color:red">**F1-Score (í¬ìƒì)**</span> | <span style="color:blue">**F1-Score (ìƒì¡´ì)**</span> |
| ----------------------- | ------------ | -------------------------------------------------- | --------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------ | ------------------------------------------------- | -------------------------------------------------- |
| **Logistic Regression** | 0.8045       | <span style="color:red">0.82</span>                | <span style="color:blue">0.78</span>                | <span style="color:red">0.86</span>             | <span style="color:blue">0.73</span>             | <span style="color:red">0.84</span>               | <span style="color:blue">0.76</span>               |
| **Decision Tree**       | 0.7709       | <span style="color:red">0.83</span>                | <span style="color:blue">0.70</span>                | <span style="color:red">0.76</span>             | <span style="color:blue">0.78</span>             | <span style="color:red">0.80</span>               | <span style="color:blue">0.74</span>               |
| **XGBoost**             | 0.8045       | <span style="color:red">0.82</span>                | <span style="color:blue">0.78</span>                | <span style="color:red">0.86</span>             | <span style="color:blue">0.73</span>             | <span style="color:red">0.84</span>               | <span style="color:blue">0.76</span>               |

##### ìš”ì•½

- **Logistic Regression**ì™€ **XGBoost** ëª¨ë¸ì€ ë™ì¼í•œ ì •í™•ë„(80.45%)ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„.
- **Decision Tree**ëŠ” ì •í™•ë„ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì§€ë§Œ, ìƒì¡´ì í´ë˜ìŠ¤(1)ì˜ Recallì´ ë†’ì•„ ìƒì¡´ìë¥¼ ì˜ ì˜ˆì¸¡.
- **Logistic Regression**ì™€ **XGBoost** ëª¨ë¸ì´ Decision Treeë³´ë‹¤ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„.

</details>

# ğŸ¬ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„

> ì˜í™” ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸ì •ì /ë¶€ì •ì  ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” í”„ë¡œì íŠ¸Â 

## ëª©í‘œ

1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

2. ë°ì´í„° ì „ì²˜ë¦¬

3. feature ë¶„ì„ (EDA)

4. ë¦¬ë·° ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸° (LSTM)

## ì¶”ê°€ ëª©í‘œ

- [x] NLP ì´ìš©

- [x] ê¸ì • / ë¶€ì • ë¦¬ë·°ì˜ ì›Œë“œ í´ë¼ìš°ë“œ ê·¸ë ¤ë³´ê¸° 

## ì˜ˆì¸¡ ëª¨ë¸ ê¸°ëŠ¥ ê°œì„  (ì¶”ê°€)

<details>
    <summary>ì´ëª¨í‹°ì½˜ ì „ì²˜ë¦¬</summary>

### ğŸ˜€

```py
  # ì „ì²˜ë¦¬ í•¨ìˆ˜
  import re
  import emoji


  # ì´ëª¨í‹°ì½˜ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì¤‘ë³µ ì œê±°)
  def remove_duplicate_emojis(text):
      # ìœ ë‹ˆì½”ë“œ ì´ëª¨í‹°ì½˜ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì´ëª¨í‹°ì½˜ì„ ì°¾ìŒ
      emoji_pattern = re.compile("[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F]", flags=re.UNICODE)

      # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„¸íŠ¸ (set) ì‚¬ìš©
      emojis = set(emoji_pattern.findall(text))

      # í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ì´ëª¨í‹°ì½˜ì„ ì œê±°í•˜ê³ , í•˜ë‚˜ì˜ ì´ëª¨í‹°ì½˜ë§Œ ë‚¨ê¹€
      for em in emojis:
          text = re.sub(em + '+', em, text)  # ì¤‘ë³µëœ ì´ëª¨í‹°ì½˜ì„ í•˜ë‚˜ë¡œ ì¤„ì„

      return text

  # ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì´ëª¨í‹°ì½˜ ì¤‘ë³µ ì œê±° í›„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
  def preprocess_text(text):
      if isinstance(text, float):
          return ""

      # ì´ëª¨í‹°ì½˜ ì¤‘ë³µ ì œê±°
      text = remove_duplicate_emojis(text)

      # ì´ëª¨í‹°ì½˜ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
      text = emoji.demojize(text, delimiters=(" ", " "))

      # ì†Œë¬¸ìë¡œ ë³€í™˜
      text = text.lower()

      # ìˆ«ì ë° êµ¬ë‘ì  ì œê±°
      text = re.sub(r'\d+', '', text)
      text = re.sub(r'[^\w\s]', '', text)

      # ì•ë’¤ ê³µë°± ì œê±°
      text = text.strip()

      return text

      df['content'] = df['content'].apply(preprocess_text)
```

> ì™œ? ë¦¬ë·°ì—ì„œ ì´ëª¨í‹°ì½˜ì€ í‰ì ê³¼ ê´€ë ¨í•´ ì¤‘ìš”í•œ ë°ì´í„°ë¼ ìƒê°í–ˆê³ , ì´ë¥¼ ì§€ìš°ê¸° ë³´ë‹¨ í™œìš©í•˜ëŠ” ë°©ì•ˆì„ ìƒê°í–ˆë‹¤. ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì¶”ê°€í•´ë´¤ë‹¤. 

```py
print(df['content'])
print('ë°ì´í„° íƒ€ì… : ', type(df['content'])) # ë°ì´í„° íƒ€ì…ì€ pandas ì‹œë¦¬ì¦ˆì¸ê±¸ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.
print('ë°ì´í„° íƒ€ì… : ', type(df['score']))
```

`ì‹¤í–‰ ê²°ê³¼`
| Index   | Review Content                                                                                   |
|---------|--------------------------------------------------------------------------------------------------|
| 0       | great app on the move i can watch my movies and shows anywhere i want                            |
| 1       | good                                                                                             |
| 2       | need to improve and to update some error during streaming                                        |
| 3       | netflix is a nice app but not all the movies are available                                       |
| 4       | not much availability considering options on world cinema                                        |
| ...     | ...                                                                                              |
| 117129  | i really like it there are so many movies and series to choose from                              |
| 117130  | i love netflix i always enjoy my time using it                                                   |
| 117131  | sound quality is very slow of movies                                                             |
| 117132  | rate is very expensive because we see netflix sundry places for free                             |
| 117133  | this app is awesome for english movies series and it brings a wide range of variety              |

**Total Reviews:** 117,134

**Data Type:** `pandas.core.series.Series`

</details>

<br>

<details>
<summary>ëª¨ë¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ë° ê¸°ëŠ¥ ê°œì„ </summary>

<br>

### Keras ë°©ì‹ìœ¼ë¡œ í•™ìŠµ í…ŒìŠ¤íŠ¸

> KerasëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‰½ê²Œ êµ¬ì¶•í•˜ê³  í›ˆë ¨í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê³ ìˆ˜ì¤€ì˜ APIë¡œ, í…ì„œí”Œë¡œìš°ì™€ ê°™ì€ ë°±ì—”ë“œ ìœ„ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤.

```py
# ------------------------------------- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ìƒëµ ------------------------------------

# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("netflix_reviews.csv")  

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    if isinstance(text, float):
        return ""
    text = text.lower()  # ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
    text = re.sub(r'[^\w\s]', '', text)  # êµ¬ë‘ì  ì œê±°
    text = re.sub(r'\d+', '', text)  # ìˆ«ì ì œê±°
    text = text.strip()  # ì–‘ìª½ ê³µë°± ì œê±°
    return text

# ì ìˆ˜ ì¹´ìš´íŠ¸ ê³„ì‚°
score_counts = df['score'].value_counts().reset_index()
score_counts.columns = ['Score', 'Count']

# í…ìŠ¤íŠ¸ í† í°í™”
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['content'])
X = tokenizer.texts_to_sequences(df['content'])
X = pad_sequences(X)

# ë ˆì´ë¸” ì„¤ì •
y = df['score'].values

# í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(Dense(64, activation="relu", input_shape=(X_train.shape[1],)))
model.add(Dropout(0.1))
model.add(Dense(32, activation="relu"))
model.add(Dense(1, activation="linear"))  # íšŒê·€ë¥¼ ìœ„í•´ 'linear' í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©

model.compile(loss="mean_squared_error", optimizer="adam", metrics=["mae"])

# ëª¨ë¸ í›ˆë ¨
model.fit(X_train, y_train, epochs=10, batch_size=4, verbose=1)

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------

Epoch 9/10
23427/23427 [==============================] - 45s 2ms/step - loss: 2.9106 - mae: 1.5832
Epoch 10/10
23427/23427 [==============================] - 47s 2ms/step - loss: 2.9105 - mae: 1.5832

<keras.src.callbacks.History at 0x28b7e82c760>

733/733 [==============================] - 1s 2ms/step
Accuracy: 10.547658684423956%
```

<span style="color:red"> í•™ìŠµë¥  10% </span>

#### ìš”ì•½

- ê°„ê²°í•œ ì½”ë“œ êµ¬ì¡° ë•ë¶„ì— ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ ì‹œë„í–ˆìœ¼ë‚˜, ë‚®ì€ í•™ìŠµë¥ ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- ì´ëŠ” Kerasì— ëŒ€í•œ ì´í•´ ë¶€ì¡±ì´ ì›ì¸ì¼ ìˆ˜ ìˆìœ¼ë‚˜, PyTorchì— ë¹„í•´ í›¨ì”¬ ê°„ë‹¨í•˜ì—¬ ë”¥ëŸ¬ë‹ ìˆ˜ì¤€ì˜ ì½”ë“œê°€ ì˜¤ë¥˜ ì—†ì´ ì‘ë™í•˜ëŠ” ì ì€ ê¸ì •ì ì…ë‹ˆë‹¤.
- ë”°ë¼ì„œ Kerasì— ëŒ€í•œ ì‹¬ì¸µì ì¸ í•™ìŠµì„ ìœ„í•´ ë³„ë„ì˜ ì‹œê°„ì„ í• ì• í•  í•„ìš”ê°€ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.

### 1ì°¨ ê¸°ë³¸ í•™ìŠµ

```py
# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------

# ë°ì´í„° ë¡œë” ì •ì˜
BATCH_SIZE = 16

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)  # í•™ìŠµë¥  ì„¤ì •

# LSTM ëª¨ë¸ ì •ì˜
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout_rate=0.5):
        super(LSTMModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)  
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´

    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, cell) = self.lstm(embedded.unsqueeze(1))  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        hidden = self.dropout(hidden[-1])  # ë“œë¡­ì•„ì›ƒ ì ìš©
        return self.fc(hidden)

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------

Epoch 6, Loss: 1.4112727279465251
Epoch 7, Loss: 1.4057372415535927
Epoch 8, Loss: 1.3953191742201765
Epoch 9, Loss: 1.3764440944643788
Epoch 10, Loss: 1.352955166198948
Accuracy: 47.15499210312887%
```

<span style="color:red"> í•™ìŠµë¥  47%  </span>

#### ìš”ì•½

- ê¸°ë³¸ì ìœ¼ë¡œ ì œì‹œëœ ê³¼ì œ ì¡°ê±´ì— ì¶©ì‹¤í•˜ì—¬ ê¸°ë³¸ ì½”ë“œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.
- ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ë ¤ê³  í–ˆìŠµë‹ˆë‹¤.
- ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ë ¤ê³  ë…¸ë ¥í–ˆìŠµë‹ˆë‹¤.

### 2ì°¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë° ì—í­ ìˆ˜ ì¦ê°€

```py
BATCH_SIZE = 64  # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ 64ë¡œ ì„¤ì •

num_epochs = 100  # í•™ìŠµí•  ì—í­ ìˆ˜ ì¡°ì • ê°€ëŠ¥

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------

Epoch 97, Loss: 1.1228876022348633
Epoch 98, Loss: 1.1235658764025458
Epoch 99, Loss: 1.1214616659965124
Epoch 100, Loss: 1.1203884350561852
Accuracy: 54.420113544201136%
```

<span style="color:red"> í•™ìŠµë¥  54% </span>

#### ìš”ì•½

- ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŒ€í­ ëŠ˜ë ¤ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ê³ , íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ì˜ ë³€ë™ì„±ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
- ì—í¬í¬ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ì í•©í•˜ë„ë¡ í•˜ì—¬ í•™ìŠµ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3ì°¨ 2ë ˆì´ì–´ ì¶”ê°€

```py
#ë ˆì´ì–´ ì¶”ê°€

# LSTM ëª¨ë¸ ì •ì˜
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
        self.lstm1 = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) 
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim) 
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        lstm_out, (hidden, cell) = self.lstm1(embedded.unsqueeze(1))
        lstm_out, (hidden, cell) = self.lstm2(lstm_out)

        # ì–‘ë°©í–¥ì˜ hidden stateë¥¼ ê²°í•©
        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)

        return self.fc2(self.fc1(hidden_cat))

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------
Epoch 90, Loss: 1.1628416655413527
Epoch 91, Loss: 1.1616355441942963
Epoch 92, Loss: 1.1602509196300963
Epoch 93, Loss: 1.1576813772676748
Epoch 94, Loss: 1.1575369658730543
Epoch 95, Loss: 1.153783379601944
Epoch 96, Loss: 1.1520104497365984
Epoch 97, Loss: 1.150690621483448
Epoch 98, Loss: 1.1507031974938948
Epoch 99, Loss: 1.1478193214728971
Epoch 100, Loss: 1.1457121307125677
Accuracy: 56.00375634951125%
```

<span style="color:red"> í•™ìŠµë¥  56% </span>

#### ìš”ì•½

- LSTM ë ˆì´ì–´ê°€ 1ê°œì—ì„œ 2ê°œë¡œ ëŠ˜ì–´ë‚˜ê³ , ê° ë ˆì´ì–´ê°€ ì–‘ë°©í–¥ìœ¼ë¡œ êµ¬ì„±ë¨ì— ë”°ë¼ ëª¨ë¸ì˜ ë³µì¡ì„±ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.
- FC ë ˆì´ì–´ë„ 1ê°œì—ì„œ 2ê°œë¡œ ì¦ê°€í•˜ì—¬ ì¶œë ¥ì¸µìœ¼ë¡œì˜ ì—°ê²°ì´ ë” ì„¸ë¶„í™”ë˜ì—ˆìŠµë‹ˆë‹¤.

### 4ì°¨ 4ë ˆì´ì–´ ì¶”ê°€

```py
# LSTM ëª¨ë¸ ì •ì˜ (4 ë ˆì´ì–´)
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)
        self.lstm1 = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)
        self.lstm3 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)  # ì¶”ê°€ëœ ë ˆì´ì–´
        self.lstm4 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)  # ì¶”ê°€ëœ ë ˆì´ì–´
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # ì²« ë²ˆì§¸ ì™„ì „ ì—°ê²°ì¸µ
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # ìµœì¢… ì¶œë ¥ì¸µ

    def forward(self, text):
        embedded = self.embedding(text)
        lstm_out, (hidden, cell) = self.lstm1(embedded.unsqueeze(1))
        lstm_out, (hidden, cell) = self.lstm2(lstm_out)
        lstm_out, (hidden, cell) = self.lstm3(lstm_out)  # 3ë²ˆì§¸ LSTM ë ˆì´ì–´
        lstm_out, (hidden, cell) = self.lstm4(lstm_out)  # 4ë²ˆì§¸ LSTM ë ˆì´ì–´

        # ì–‘ë°©í–¥ì˜ hidden stateë¥¼ ê²°í•©
        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)

        return self.fc2(self.fc1(hidden_cat))


Epoch 1, Loss: 1.4411939945644079
Epoch 2, Loss: 1.438289221480438
Epoch 3, Loss: 1.4380339547635752

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------

Epoch 18, Loss: 1.4378050892019434
Epoch 19, Loss: 1.4376814013048245
Epoch 20, Loss: 1.437378289349657
Epoch 21, Loss: 1.4371888463407654
Epoch 22, Loss: 1.43679916533187
Epoch 23, Loss: 1.4355940978681676
Epoch 24, Loss: 1.4332314667034474
```

<span style="color:red"> í•™ìŠµì˜ ì •ì²´ </span>

- ì´ ëª¨ë¸ì€ 4ê°œì˜ LSTM ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ ë” ê¹Šê³  ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° ë ˆì´ì–´ëŠ” ì–‘ë°©í–¥ìœ¼ë¡œ ì„¤ê³„ë˜ì–´ ìˆì–´ ë” ë§ì€ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
- FC ë ˆì´ì–´ëŠ” ì´ì „ ì½”ë“œì™€ ê°™ì§€ë§Œ, LSTM ë ˆì´ì–´ì˜ ì¶”ê°€ë¡œ ì¸í•´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ì¦ê°€í•˜ê³ , ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
- í•˜ì§€ë§Œ ë ˆì´ì–´ ìˆ˜ê°€ ë§ì•„ì ¸ ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•´ì ¸ì„œ í•™ìŠµì´ ì–´ë ¤ì›Œì§„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í•„ìš” ì´ìƒì˜ íŒŒë¼ë¯¸í„°ê°€ ë§ìœ¼ë©´ ìˆ˜ë ´í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

### 5ì°¨ 2ë ˆì´ì–´ ë³µêµ¬ ë° ì˜µí‹°ë§ˆì´ì € ë³€ê²½

```py
#ì˜µí‹°ë§ˆì´ì €ì˜ í•™ìŠµë¥ ì„ 0.01ì—ì„œ 0.05ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤

optimizer = optim.SGD(model.parameters(), lr=0.05)

# ------------------------------------------- ì¤‘ê°„ ìƒëµ -------------------------------------------
Epoch 93, Loss: 1.0482210473802716
Epoch 94, Loss: 1.0486159132609187
Epoch 95, Loss: 1.046645594578961
Epoch 96, Loss: 1.0441050273159664
Epoch 97, Loss: 1.0450137004103677
Epoch 98, Loss: 1.0456044204405956
Epoch 99, Loss: 1.0434317154282189
Epoch 100, Loss: 1.0428769397247366
Accuracy: 61.032142399795106%
```

<span style="color:red"> í•™ìŠµë¥  61% </span>

- í•™ìŠµë¥ ì´ ì¦ê°€í•˜ë©´, ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê°€ ë” ì»¤ì ¸ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì— ë” ë¹ ë¥´ê²Œ ë„ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ëª¨ë¸ì´ ë” ë¹¨ë¦¬ ìˆ˜ë ´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- íŠ¹ì • ë¬¸ì œì—ì„œëŠ” ë†’ì€ í•™ìŠµë¥ ì´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì§€ì—­ ìµœì†Ÿê°’ì„ íƒìƒ‰í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” í° ì—…ë°ì´íŠ¸ë¡œ ì¸í•´ ëª¨ë¸ì´ ë” ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ ê³µê°„ì„ íƒìƒ‰í•˜ê²Œ ë©ë‹ˆë‹¤.

- ì´ˆê¸° ì—í¬í¬ì—ì„œ ì†ì‹¤ ê°’ì´ ë¹ ë¥´ê²Œ ê°ì†Œí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì´ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ìˆë‹¤ëŠ” ì‹ í˜¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- íŠ¹ì • ë°ì´í„°ì…‹ì´ë‚˜ ëª¨ë¸ êµ¬ì¡°ì—ì„œëŠ” ë†’ì€ í•™ìŠµë¥ ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ LSTMê³¼ ê°™ì€ ë³µì¡í•œ ëª¨ë¸ì—ì„œëŠ” ì¼ë¶€ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ë” í° ë³€í™”ê°€ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</details>
