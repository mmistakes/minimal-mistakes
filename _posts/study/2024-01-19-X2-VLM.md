---
layout: single
title: "X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"
permalink: /studies/paper/X2-VLM
tags: [Paper, LVLM]
categories:
  - ğŸ“„ paper
date: 2024-01-19
use_math: true
---
*ë¹„ì „ ì–¸ì–´ ì‚¬ì „ í•™ìŠµì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œë¶€í„° ë¹„ì „ê³¼ ì–¸ì–´ ê°„ì˜ ì •ë ¬ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ê¸°ì¡´ì˜ ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë“¤ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ë§Œì„ í•™ìŠµí•˜ì§€ë§Œ, ì¼ë¶€ ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ì‚¬ì „ í•™ìŠµëœ ê°ì²´ ê°ì§€ê¸°ë¥¼ í™œìš©í•˜ì—¬ ê°ì²´ ìˆ˜ì¤€ì—ì„œ ë¹„ì „ ì–¸ì–´ ì •ë ¬ì„ í™œìš©í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì¤‘ ì •ë°€ë„ ë¹„ì „ ì–¸ì–´ ì •ë ¬ì„ í†µí•© ì‚¬ì „ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ë™ì‹œì— ë‹¤ì¤‘ ì •ë°€ë„ ì •ë ¬ ë° ë‹¤ì¤‘ ì •ë°€ë„ ìœ„ì¹˜ ê²°ì •ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì œì•ˆí•œë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì‚¬ì „ í•™ìŠµê³¼ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ì‚¬ì „ í•™ìŠµì„ í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ í†µí•©í•˜ëŠ” ìœ ì—°í•œ ëª¨ë“ˆì‹ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ X2-VLMì´ë¼ëŠ” ëª¨ë“  ê²ƒì„ í¬í•¨í•˜ëŠ” ëª¨ë¸ì„ ì œì‹œí•œë‹¤. X2-VLMì€ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ì—°ê´€ëœ ë¬´í•œí•œ ì‹œê°ì  ê°œë…ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¥´ë©´ X2-VLMì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë° ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ì‘ì—… ëª¨ë‘ì—ì„œ ê¸°ë³¸ ë° ëŒ€ê·œëª¨ ê·œëª¨ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì„±ëŠ¥ê³¼ ëª¨ë¸ ê·œëª¨ ì‚¬ì´ì—ì„œ ì¢‹ì€ ì ˆì¶©ì„ ì´ë£¬ë‹¤. ë˜í•œ, X2-VLMì˜ ëª¨ë“ˆì‹ ì„¤ê³„ëŠ” ì–´ë–¤ ì–¸ì–´ë‚˜ ë„ë©”ì¸ì—ì„œë„ í™œìš©ë  ìˆ˜ ìˆë„ë¡ ë†’ì€ ì „ì´ì„±ì„ ì œê³µí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ XLM-Rë¡œ ê°„ë‹¨íˆ êµì²´í•¨ìœ¼ë¡œì¨, X2-VLMì€ ì–´ë– í•œ ë‹¤êµ­ì–´ ì‚¬ì „ í•™ìŠµ ì—†ì´ë„ ìµœì‹ ì˜ ë‹¤êµ­ì–´ ë‹¤ì¤‘ ëª¨ë‹¬ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•œë‹¤. ì½”ë“œì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ github.com/zengyan-97/X2-VLMì—ì„œ ì´ìš© ê°€ëŠ¥í•œë‹¤.*

## ğŸ“‹ Table of Contents

- [1. Introduction](#1-introduction)
- [2. Related Work](#2-related-work)
- [3. Method](#3-method)
- [4. Experiment](#4-experiment)
- [5. Conclusion and Discussion](#5-conclusion-and-discussion)
- [A Appendix](#a-appendix)

## 1. Introduction

## 2. Related Work
### 2.1 Image-Text Pre-training

### 2.2 Video-Text Pre-training

### 2.3 Multilingual Multi-modal Pre-training

## 3. Method
### 3.1 Overview

### 3.2 Unified Vision Encoding

### 3.3 Multi-Grained Vision Language Pre-training

#### 3.3.1 Multi-Grained Aligning

#### 3.3.2 Multi-Grained Localization

## 4. Experiment
### 4.1 Pre-training Datasets

### 4.2 Implementation Details

### 4.3 Image-Text Downstream Tasks
#### 4.3.1 Image-Text Retrieval

#### 4.3.2 Visual Question Answering

#### 4.3.3 Visual Reasoning

#### 4.3.4 Visual Grounding

#### 4.3.5 Image Captioning

#### 4.3.6 Winoground

#### 4.3.7 Open-vocabulary Attribute Detection

### 4.4 Video-Text Downstream Tasks

### 4.5 Multilingual Multi-modal Tasks

### 4.6 Ablation Study

### 4.7 Qualitative Study of Multi-Grained Alignments

## 5. Conclusion and Discussion

## A Appendix
### A.1 Pre-training Datasets

### A.2 Implementation Details

### A.3 Ablation Study

### A.4 Qualitative Study of Multi-Grained Alignments