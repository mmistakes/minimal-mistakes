---
layout: single
title: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"
permalink: /studies/paper/LoRA
tags: [Paper, NLP]
categories:
  - ğŸ“„ paper
date: 2024-02-02
use_math: true
---
*ì‹œê°-ì–¸ì–´ ì‚¬ì „ í•™ìŠµ(Vision-Language Pre-training, VLP)ì€ ë§ì€ ì‹œê°-ì–¸ì–´ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë“¤ì€ ì´í•´ ê¸°ë°˜ ì‘ì—…ì´ë‚˜ ìƒì„± ê¸°ë°˜ ì‘ì—… ì¤‘ í•˜ë‚˜ì—ì„œë§Œ ë›°ì–´ë‚˜ë‹¤. ë˜í•œ, ì„±ëŠ¥ í–¥ìƒì€ ëŒ€ë¶€ë¶„ ì›¹ì—ì„œ ìˆ˜ì§‘ëœ ì¡ìŒì´ ë§ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ ë°ì´í„°ì…‹ì— í™•ì¥í•¨ìœ¼ë¡œì¨ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ìµœì ì˜ ê°ë… ì›ì²œì´ ì•„ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì‹œê°-ì–¸ì–´ ì´í•´ì™€ ìƒì„± ì‘ì—… ëª¨ë‘ì— ìœ ì—°í•˜ê²Œ ì „í™˜í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ VLP í”„ë ˆì„ì›Œí¬ì¸ BLIPì„ ì œì•ˆí•œë‹¤. BLIPì€ ìº¡ì…”ë„ˆê°€ í•©ì„± ìº¡ì…˜ì„ ìƒì„±í•˜ê³  í•„í„°ê°€ ì¡ìŒì´ ë§ì€ ê²ƒë“¤ì„ ì œê±°í•˜ëŠ” ìº¡ì…˜ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ì„ í†µí•´ ì¡ìŒì´ ë§ì€ ì›¹ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰(í‰ê·  recall@1ì—ì„œ +2.7%), ì´ë¯¸ì§€ ìº¡ì…”ë‹(CIDErì—ì„œ +2.8%), VQA(VQA ì ìˆ˜ì—ì„œ +1.6%) ë“± ë‹¤ì–‘í•œ ì‹œê°-ì–¸ì–´ ì‘ì—…ì—ì„œ ìµœê³ ì˜ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆë‹¤. BLIPì€ ë˜í•œ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤-ì–¸ì–´ ì‘ì—…ì— ì§ì ‘ ì „í™˜ë  ë•Œ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì½”ë“œ, ëª¨ë¸, ë°ì´í„°ì…‹ì€ ì´ [HTTP URL](https://github.com/salesforce/BLIP){:target="_blank"}ì—ì„œ ê³µê°œëœë‹¤.*

## ğŸ“‹ Table of Contents

- [1.Introduction](#1introduction)
- [2.Related Work](#2related-work)
- [3.Method](#3method)
- [4.Experiments and Discussions](#4experiments-and-discussions)
- [5.Comparison with State-of-the-arts](#5comparison-with-state-of-the-arts)
- [6.Additional Ablation Study](#6additional-ablation-study)
- [7.Conclusion](#7conclusion)
- [A.Downstream Task Details](#adownstream-task-details)
- [B.Additional Examples of Synthetic Captions](#badditional-examples-of-synthetic-captions)
- [C.Pre-training Dataset Details](#cpre-training-dataset-details)

## 1.Introduction

## 2.Related Work
### 2.1.Vision-language Pre-training

### 2.2.Knowledge Distillation

### 2.3.Data Augmentation

## 3.Method
### 3.1.Model Architecture

### 3.2.Pre-training Objectives

### 3.3.CapFilt

## 4.Experiments and Discussions
#### 4.1.Pre-training Details

### 4.2.Effect of CapFilt

### 4.3.Diversity is Key for Synthetic Captions

### 4.4.Parameter Sharing and Decoupling

## 5.Comparison with State-of-the-arts

### 5.1.Image-Text Retrieval

### 5.2.Image Captioning

### 5.3.Visual Question Answering(VQA)

### 5.4.Natural Language Visual Reasoning(NLVR2)

### 5.5.Visual Dialog (VisDial)

### 5.6.Zero-shot Transfer to Video-Language Tasks

## 6.Additional Ablation Study

## 7.Conclusion

## A.Downstream Task Details

## B.Additional Examples of Synthetic Captions

## C.Pre-training Dataset Details