---
layout: single
title: Creating Tensors
categories:
  - pytorch
tags:
  - 부스트캠프
  - AITech
  - AIBasic
  - PyTorch
author_profile: false
use_math: true
---
## 1. Tensor의 생성
### 1.1 특정한 값으로 초기화된 Tensor 생성, 변환
1. 0으로 초기화된 Tensor 생성: <mark style="background: #FFF3A3A6;">zeros</mark>
    - 1-D Tensor (5) : `a = torch.zeros(5)`
    - 2-D Tensor (2x3) : `b = torch.zeros([2, 3])`
    - 3-D Tensor (3x2x4) : `c = torch.zeros([3, 2, 4])`
    
2. 0으로 초기화되어 있고, 크기와 자료형이 같은 Tensor 생성: <mark style="background: #FFF3A3A6;">zeros_like</mark>
    - Like Tensor e : `g = torch.zeros_like(e)`
    
3. 1로 초기화된 Tensor 생성: <mark style="background: #FFF3A3A6;">ones</mark>
    - 1-D Tensor (5) : `a = torch.ones(5)`
    - 2-D Tensor (2x3) : `b = torch.ones([2, 3])`
    - 3-D Tensor (3x2x4) : `c = torch.ones([3, 2, 4])`
    
4. 1로 초기화되어 있고, 크기와 자료형이 같은 Tensor 생성: <mark style="background: #FFF3A3A6;">ones_like</mark>
    - Liken Tensor b : `h = torch.ones_like(b)`<br><br>


### 1.2 난수로 초기화된 Tensor 생성
1. [0, 1] 구간의 <u>연속균등분포</u> 난수 Tensor 생성: <mark style="background: #FFF3A3A6;">rand</mark>
    - 1-D : `i = torch.rand(3)`
    - 2-D : `j = torch.rand([2, 3])`
    
2. <u>표준정규분포</u>에서 추출한 난수 Tensor 생성: <mark style="background: #FFF3A3A6;">randn</mark>
    - 1-D : `k = torch.randn(3)`
    - 2-D : `I = torch.randn([2, 3])`
        ⇒ dtype을 torch.int로 지정하면 오류 발생
        
3. 연속균등분포에서 추출한 난수로 생성하고, 크기와 자료형이 같은 Tensor 생성: <mark style="background: #FFF3A3A6;">rand_like</mark>
    - Like Tensor k : `m = torch.rand_like(k)`
    
4. 표준정규분포에서 추출한 난수로 생성하고, 크기와 자료형이 같은 Tensor 생성: <mark style="background: #FFF3A3A6;">randn_like</mark>
    - Like Tensor i : `n = torch.randn_like(i)`

5. 연속균등분포 & 표준정규분포 - 자세한 내용은 ML for RecSys 파트에서
	- 연속균등분포
	![image1](../../images/2024-08-06-aitech-week1_3/image1.png)
	- 표준정규분포
	![image2](../../images/2024-08-06-aitech-week1_3/image2.png)

### 1.3 지정된 범위 내에서 초기화된 Tensor 생성
1. 간격이 <mark style="background: #FFF3A3A6;">int(int64)</mark> - 1에서 11까지 2씩 증가하는 1-D Tensor
    - `o = torch.arange(start = 1, end = 11, step = 2)`
2. 간격이 <mark style="background: #FFF3A3A6;">float (float)</mark> - 1에서 4까지 0.5씩 증가하는 1-D Tensor
    - `o = torch.arange(start = 1, end = 4, step = 0.5)`<br><br>

### 1.4 초기화되지 않은 Tensor 생성
1. 초기화되지 않은 Tensor
	- "초기화 되지 않았다"
	    → 생성된 Tensor의 각 요소가 <mark style="background: #FFF3A3A6;">명시적으로 다른 특정 값으로 설정되지 않았음</mark>을 의미
	    → 초기화되지 않은 Tensor 생성 시, 해당 Tensor는 <mark style="background: #FFF3A3A6;">메모리에 이미 존재하는 임의의 값들로 채워짐</mark>
	- 초기화 되지 않은 Tensor를 사용하는 이유
	    1) 성능 향상 : Tensor 생성 직후 <mark style="background: #FFF3A3A6;">곧바로 다른 값으로 덮어쓸 예정</mark>인 경우 → <u>불필요한 자원 소모 감소</u>
	    2) 메모리 사용 최적화 : (큰 Tensor 다룰 때) 불필요한 초기화는 곧 불필요한 자원(메모리 사용량) 소모<br><br>
2. 초기화 되지 않은 Tensor 생성
    - 1-D : `q = torch.empty(5)`
3. 초기화 되지 않은 Tensor를 다른 데이터로 수정
    - 1-D : 기존 Tensor 수정 vs 새로운 Tensor 생성
	    - `q.fill_(3.0)` ⇒ <mark style="background: #FFF3A3A6;">"in-place 연산"</mark> : 기존 Tensor 수정, (당연히) 메모리 주소 유지
		- `q.fill(3.0)` ⇒ <mark style="background: #FFF3A3A6;">"일반 연산"</mark> : 새 Tensor 생성, 메모리 주소 바뀜<br><br>

### 1.5 list, Numpy 데이터로부터 Tensor 생성
- List vs Numpy
	- 공통: 여러 값을 순차적으로 저장할 수 있음
	- 차이: Numpy는 <mark style="background: #FFF3A3A6;">대규모 수치 데이터에 대한 연산 및 조작</mark>에 적합<br><br>
	
    1. List s를 Tensor t로 생성
	    - list는 여러 값을 순차적으로 저장 가능한 <mark style="background: #FFF3A3A6;">가변적인 컨테이너 데이터타입</mark>
	       → list를 tensor로 생성 가능
        - `t = torch.tensor(s)`
    2. 2-D Matrix Numpy u에서 Tensor v로 생성
	    - `u = np.array([[0, 1], [2, 3]])`
	        1. u to v: `v = torch.from_Numpy(u)`
	        2. 정수형 → 실수형 타입 캐스팅: `v = torch.from_Numpy(u).float()`<br><br>

### 1.6 CPU Tensor 생성
1. 정수형 CPU Tensor 생성: <mark style="background: #FFF3A3A6;">FloatTensor</mark>
	- `w = torch.IntTensor([1, 2, 3, 4, 5])`
    
2. 실수형 CPU Tensor 생성: <mark style="background: #FFF3A3A6;">IntTensor</mark>(부호 있는 32bit 실수)
	- `x = torch.FloatTensor([1, 2, 3, 4, 5])`
    
3. 기타 정수, 실수형 CPU Tensor 생성
    1) 8비트 부호 없는 정수형: torch.<mark style="background: #FFF3A3A6;">Byte</mark>Tensor
    2) 8비트 부호 있는 정수형: torch.<mark style="background: #FFF3A3A6;">Char</mark>Tensor
    3) 16비트 부호 있는 정수형: torch.<mark style="background: #FFF3A3A6;">Short</mark>Tensor
    4) 64비트 부호 있는 정수형: torch.<mark style="background: #FFF3A3A6;">Long</mark>Tensor
    5) 64비트 부호 있는 실수형: torch.<mark style="background: #FFF3A3A6;">Double</mark>Tensor<br><br>

### 1.7 Tensor의 복제
- 1-D Tensor x 에 대해,
    1. `y = x.clone()`
    2. `z = x.detach()`  → x를 계산 그래프에서 분리하여 새로운 Tensor z에 저장<br><br>

### 1.8 CUDA Tensor 생성과 변환 → PyTorch 최대장점
- GPU? - "그래픽 처리 장치" 
    -  <mark style="background: #FFF3A3A6;">AI의 대규모 데이터 처리와 복잡한 계산</mark>을 위해 사용
	    -  <mark style="background: #FFF3A3A6;">병렬처리작업</mark>: GPU의 수천개의 코어가 대량의 연산을 동시에 수행
	    - <mark style="background: #FFF3A3A6;">속도</mark>: GPU의 병렬 처리 능력으로, AI 모델의 훈련 및 추론 속도 대량 향상
		    ![image1](../../images/2024-08-06-aitech-week1_3/image3.png)<br><br>

1. Tensor가 위치한 현재 디바이스 확인: <mark style="background: #FFF3A3A6;">device</mark>
	- `a.device`
2. CUDA 기술 사용 가능 환경인지 확인: <mark style="background: #FFF3A3A6;">cuda.is_available()</mark>
    - `torch.cuda.is_available()`
3. CUDA device 이름 확인: <mark style="background: #FFF3A3A6;">get_device_name</mark>
	- `torch.cuda.get_device_name(device=0)`
4. Tensor를 GPU에 할당: <mark style="background: #FFF3A3A6;">.to('cuda') / .cuda()</mark>
    - `b = torch.tensor([1, 2, 3, 4, 5]).to(’cuda’) / .cuda()`
    - `.to('a')`: a 장치로 Tensor 이동
5. 사용 가능한 GPU 개수 확인: <mark style="background: #FFF3A3A6;">cuda.device_count()</mark>
    - `torch.cuda.device_count()`
6. GPU 할당된 Tensor를 CPU Tensor 변환: <mark style="background: #FFF3A3A6;">.to(device = ‘cpu’) / .cpu()</mark>
    - `c = b.to(device = ‘cpu’)`
    - `c = b.cpu()`<br><br>
