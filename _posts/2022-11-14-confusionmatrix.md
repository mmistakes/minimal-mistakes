---
title: '[DL/BASIC] í˜¼ë™í–‰ë ¬ : ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”í•˜ê¸° ğŸ“Š'
layout : single
toc: true
toc_sticky: true
categories:
  - basics
---

## 6. ì˜¤ì°¨í–‰ë ¬ (confusion matrix)

### 6.1 ì˜¤ì°¨í–‰ë ¬ ì•Œì•„ë³´ê¸°
ì˜¤ì°¨ í–‰ë ¬(confusion matrix)ì€ ì´ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¼ ë•Œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. ì˜¤ì°¨ í–‰ë ¬ì˜ ëœ»ì„ í•´ì„í•´ë³´ë©´ í•™ìŠµëœ ë¶„ë¥˜ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰ë©´ì„œ ì–¼ë§ˆë‚˜ í˜¼ë™í•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì§€í‘œë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì˜¤ì°¨ í–‰ë ¬ì€ ë‹¤ìŒ ì´ë¯¸ì§€ì™€ ê°™ì´ 2x2 í¬ê¸°ì˜ ë°°ì—´ì´ë‹¤. ë°°ì—´ì˜ í–‰ì€ ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ê³ , ì—´ì€ ì˜ˆì¸¡ í´ë˜ìŠ¤ì— í•´ë‹¹í•œë‹¤.

ì´ë²ˆ ê¸€ì—ì„œ ì˜¤ì°¨ í–‰ë ¬ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•  ëª¨ë¸ì€ mnist ë°ì´í„°ë¥¼ LogisticRegressionë¡œ ì˜ˆì¸¡í•œ ëª¨ë¸ì´ë‹¤. ì˜¤ì°¨ í–‰ë ¬ì„ ë‹¤ë£¨ëŠ” ê²ƒì´ ê¸€ì˜ ì£¼ì œì´ê¸° ë•Œë¬¸ì— logistic regression ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ìì„¸íˆ ë‹¤ë£¨ì§€ ì•Šê² ë‹¤. ë¨¼ì € ì˜¤ì°¨í–‰ë ¬ì„ ì ìš©í•˜ê¸° ìœ„í•œ ëª¨ë¸ì„ ê°„ë‹¨íˆ ì„¤ì •í•œë‹¤.


```python
# ê¸°ë³¸ ì„¤ì •
# ë…¸íŠ¸ë¶ì´ ì½”ë©ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ ì²´í¬

# ì‚¬ì´í‚·ëŸ° ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜
!pip install -q --upgrade scikit-learn
# mglearnì„ ë‹¤ìš´.
!wget -q -O mglearn.tar.gz https://bit.ly/mglearn-tar-gz
!tar -xzf mglearn.tar.gz
# ë‚˜ëˆ” í°íŠ¸ë¥¼ ì„¤ì¹˜
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import sklearn
from preamble import *
import matplotlib

# ë‚˜ëˆ” í°íŠ¸ ì‚¬ìš©
matplotlib.rc('font', family='NanumBarunGothic')
matplotlib.rcParams['axes.unicode_minus'] = False

```

    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    fonts-nanum is already the newest version (20170925-1).
    The following package was automatically installed and is no longer required:
      libnvidia-common-460
    Use 'sudo apt autoremove' to remove it.
    0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.
    /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs
    /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs
    /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs
    /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs
    /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs
    /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs
    /root/.local/share/fonts: skipping, no such directory
    /root/.fonts: skipping, no such directory
    /var/cache/fontconfig: cleaning cache directory
    /root/.cache/fontconfig: not cleaning non-existent cache directory
    /root/.fontconfig: not cleaning non-existent cache directory
    fc-cache: succeeded



```python
# ëª¨ë¸ì— ì‚¬ìš©í•  ë°ì´í„° ì¤€ë¹„
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits

digits = load_digits()
y = digits.target == 9

X_train, X_test, y_train, y_test = train_test_split(
    digits.data, y, random_state=0)

logreg = LogisticRegression(C=0.1, max_iter=1000).fit(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
pred_logreg = logreg.predict(X_test)
```


```python
from sklearn.metrics import confusion_matrix

confusion = confusion_matrix(y_test,pred_logreg)
print('ì˜¤ì°¨ í–‰ë ¬: \n',confusion)
```

    ì˜¤ì°¨ í–‰ë ¬: 
     [[402   1]
     [  6  41]]


ì¶œë ¥ê°’ì˜ ë°°ì—´ì—ì„œ 4ê°€ì§€ì˜ ê°’ì´ ë°œìƒí–ˆëŠ”ë°, í–‰ì€ ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ê³ , ì—´ì€ ì˜ˆì¸¡ í´ë˜ìŠ¤ì— í•´ë‹¹í•œë‹¤. ê° í•­ëª©ì˜ ìˆ«ìëŠ” ì–¼ë§ˆë‚˜ ë§ì´ ì—´ì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.


```python
mglearn.plots.plot_confusion_matrix_illustration()
```




    
![png](Untitled0_files/Untitled0_5_0.png)
    



ì˜¤ì°¨ í–‰ë ¬ì˜ ëŒ€ê° í–‰ë ¬ì€ ì •í™•íˆ ë¶„ë¥˜ëœ ê²½ìš°ì´ê³ , ë‹¤ë¥¸ í•­ëª©ë“¤ì€ ì˜ëª» ë¶„ë¥˜ëœ ê²½ìš°ê°€ ì–¼ë§ˆë‚˜ ë§ì€ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ìˆ«ì 9ë¥¼ ì–‘ì„±(positive) í´ë˜ìŠ¤ë¡œ ì„¤ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.


```python
mglearn.plots.plot_binary_confusion_matrix()
```




    
![png](Untitled0_files/Untitled0_7_0.png)
    



ìœ„ì˜ ë°°ì—´ì—ì„œ 4ê°€ì§€ì˜ ê°’ì´ ë°œìƒí–ˆëŠ”ë°, TNê³¼ TPëŠ” ê°ê° True Negativeì™€ True Positiveë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ íšŸìˆ˜ì´ê³  FNê³¼ FPëŠ” False Negativeì™€ False Positiveë¡œ ì˜ëª» ì˜ˆì¸¡í•œ íšŸìˆ˜ì´ë‹¤. 

scikit-learnì—ì„œ ì œê³µí•˜ëŠ” ConfusionMatrixDisplay í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ì„œ ì˜¤ì°¨í–‰ë ¬ì„ ê·¸ë¦´ìˆ˜ ìˆë‹¤. ConfusionMatrixDisplay í´ë˜ìŠ¤ëŠ” ì¶”ì •ê¸° ê°ì²´ë¡œë¶€í„° ì˜¤ì°¨í–‰ë ¬ì„ ê·¸ë¦¬ëŠ” from_estimator í•¨ìˆ˜ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¡œë¶€í„° ì˜¤ì°¨ í–‰ë ¬ì„ ê·¸ë¦¬ëŠ” from_predictions í•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤.


```python
# from_estimator í•¨ìˆ˜ ì‚¬ìš©
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(logreg, X_test, y_test,
                                      display_labels=['9 ì•„ë‹˜','9 ë§ìŒ'])
plt.show()
```




    
![png](Untitled0_files/Untitled0_9_0.png)
    




```python
# from_predictions í•¨ìˆ˜ ì‚¬ìš©
ConfusionMatrixDisplay.from_predictions(y_test,pred_logreg,
                                        display_labels = ['9 ì•„ë‹˜','9 ë§ìŒ'])
plt.show()
```




    
![png](Untitled0_files/Untitled0_10_0.png)
    



### 6.2 ì˜¤ì°¨í–‰ë ¬ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì§€í‘œ
ì´ì œ ì´ TP,TN,FP,FN ì´ 4ê°€ì§€ ê°’ë“¤ì„ ê°€ì§€ê³  ì–´ë–¤ ì˜ë¯¸ìˆëŠ” ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.

#### 6.2.1 ì •í™•ë„ (accuracy)
ì •í™•ë„ëŠ” ëª¨ë¸ì´ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ì„œ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë‹¤. ì •í™•ë„ëŠ” (ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ íšŸìˆ˜) / (ì „ì²´ ë°ì´í„°ìˆ˜)ë¡œ ê³„ì‚°í•œë‹¤.
\begin{equation}
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\end{equation}

#### 6.2.2 ì •ë°€ë„ (precision)
ì •ë°€ë„ëŠ” ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡ëœ ê²ƒ ì¤‘ ì–¼ë§ˆë‚˜ ë§ì€ ë°ì´í„°ê°€ ì§„ì§œ ì–‘ì„±ì¸ì§€ë¥¼ ì¸¡ì •í•œë‹¤. 

\begin{equation}
\text{ì •ë°€ë„} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}

ì •ë°€ë„ëŠ” ê±°ì§“ ì–‘ì„±(FP)ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œì¼ ë•Œ ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤.

#### 6.2.3 ì¬í˜„ìœ¨ (recall)
ì¬í˜„ìœ¨ì€ ëª¨ë“  ì „ì²´ ì–‘ì„± ìƒ˜í”Œ ì¤‘ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì€ ìƒ˜í”Œì´ ì–‘ì„± í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ ë˜ì—ˆëŠ”ì§€ë¥¼ ì¸¡ì •í•œë‹¤.

\begin{equation}
\text{ì¬í˜„ìœ¨} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

ì¬í˜„ìœ¨ì€ ëª¨ë“  ì–‘ì„± ìƒ˜í”Œì„ ì‹ë³„í•´ì•¼ í•  ë•Œ (ê±°ì§“ ìŒì„±ì„ í”¼í•˜ëŠ” ê²ƒì´ ì¤‘ìš” í•  ë•Œ) ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤. 

#### 6.2.4 f1 ì ìˆ˜
ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· ì¸ f ì ìˆ˜ëŠ” ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ í•˜ë‚˜ë¡œ ìš”ì•½í•´ì¤€ë‹¤.

\begin{equation}
\text{F} = 2 \cdot \frac{\text{ì •ë°€ë„} \cdot \text{ì¬í˜„ìœ¨}}{\text{ì •ë°€ë„} + \text{ì¬í˜„ìœ¨}}
\end{equation}

ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ ê°™ì´ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì— **ë¶ˆê· í˜•í•œ ì´ì§„ ë¶„ë¥˜ ë°ì´í„°ì…‹ì—ì„œëŠ” ìœ ìš©í•œ ì§€í‘œ**ì´ë‹¤.

classification_report í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì •ë°€ë„, ì¬í˜„ìœ¨, f1 ì ìˆ˜ë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥ ê°€ëŠ¥í•˜ë‹¤.


```python
from sklearn.metrics import classification_report
print(classification_report(y_test, pred_logreg, target_names=['9 ì•„ë‹˜','9 ë§ìŒ'],
                            zero_division=0))
```

                  precision    recall  f1-score   support
    
            9 ì•„ë‹˜       0.99      1.00      0.99       403
            9 ë§ìŒ       0.98      0.87      0.92        47
    
        accuracy                           0.98       450
       macro avg       0.98      0.93      0.96       450
    weighted avg       0.98      0.98      0.98       450
    


### 6.3 ë‹¤ì¤‘ ë¶„ë¥˜ì˜ í‰ê°€ ì§€í‘œ
ì´ì§„ ë¶„ë¥˜ í‰ê°€ì— ëŒ€í•´ ë‹¤ë¤„ ë³´ì•˜ìœ¼ë‹ˆ ì´ì œ ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œë¥¼ ì•Œì•„ë³´ì. ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì§€í‘œëŠ” ì´ì§„ ë¶„ë¥˜ í‰ê°€ ì§€í‘œì—ì„œ ìœ ë„ë˜ì—ˆë‹¤. ë‹¤ë§Œ ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ í‰ê· ì„ ëƒˆë‹¤ëŠ” ê²ƒë§Œ ë‹¤ë¥¸ ì ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í´ë˜ìŠ¤ê°€ ë¶ˆê· í˜•í•  ë•ŒëŠ” ì •í™•ë„ëŠ” ì¢‹ì€ ì§€í‘œê°€ ë˜ì§€ ëª»í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì¤‘ ë¶„ë¥˜ì˜ ê²°ê³¼ëŠ” ì´ì§„ ë¶„ë¥˜ ê²°ê³¼ë³´ë‹¤ ì´í•´í•˜ê¸° ì–´ë µë‹¤. mnist ë°ì´í„°ì…‹ì˜ 10ê°œ ì†ê¸€ì”¨ ìˆ«ìë¥¼ ë¶„ë¥˜í•œ ëª¨ë¸ì— ì˜¤ì°¨ í–‰ë ¬ì„ ì ìš©í•´ë³´ê² ë‹¤.


```python
from sklearn.metrics import accuracy_score
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)
lr = LogisticRegression(max_iter=5000).fit(X_train,y_train)
pred = lr.predict(X_test)
print('ì •í™•ë„ : {:.3f}'.format(accuracy_score(y_test,pred)))
print('ì˜¤ì°¨ í–‰ë ¬ : \n', confusion_matrix(y_test,pred))
```

    ì •í™•ë„ : 0.953
    ì˜¤ì°¨ í–‰ë ¬ : 
     [[37  0  0  0  0  0  0  0  0  0]
     [ 0 40  0  0  0  0  0  0  2  1]
     [ 0  0 41  3  0  0  0  0  0  0]
     [ 0  0  0 44  0  0  0  0  1  0]
     [ 0  0  0  0 37  0  0  1  0  0]
     [ 0  0  0  0  0 46  0  0  0  2]
     [ 0  1  0  0  0  0 51  0  0  0]
     [ 0  0  0  1  1  0  0 46  0  0]
     [ 0  3  1  0  0  0  1  0 43  0]
     [ 0  0  0  0  0  1  0  0  2 44]]



```python
scores_image = mglearn.tools.heatmap(
    confusion_matrix(y_test,pred),xlabel='ì˜ˆì¸¡ í´ë˜ìŠ¤',
    ylabel ='ì •ë‹µ í´ë˜ìŠ¤',xticklabels=digits.target_names,
    yticklabels = digits.target_names, cmap = plt.cm.gray_r, fmt='%d')
plt.title('ì˜¤ì°¨í–‰ë ¬')
plt.gca().invert_yaxis()
```




    
![png](Untitled0_files/Untitled0_15_0.png)
    



ìœ„ì˜ ì˜¤ì°¨ í–‰ë ¬ì€ ì´ì§„ ë¶„ë¥˜ì—ì„œì²˜ëŸ¼ ê° í–‰ì€ ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ë©°, ì—´ì€ ì˜ˆì¸¡ í´ë˜ìŠ¤ì— í•´ë‹¹í•œë‹¤. classification_report í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ì •ë°€ë„, ì¬í˜„ìœ¨ê³¼ f1ì ìˆ˜ë¥¼ ì¶œë ¥í•´ë³´ê² ë‹¤.


```python
print(classification_report(y_test,pred))
```

                  precision    recall  f1-score   support
    
               0       1.00      1.00      1.00        37
               1       0.91      0.93      0.92        43
               2       0.98      0.93      0.95        44
               3       0.92      0.98      0.95        45
               4       0.97      0.97      0.97        38
               5       0.98      0.96      0.97        48
               6       0.98      0.98      0.98        52
               7       0.98      0.96      0.97        48
               8       0.90      0.90      0.90        48
               9       0.94      0.94      0.94        47
    
        accuracy                           0.95       450
       macro avg       0.95      0.95      0.95       450
    weighted avg       0.95      0.95      0.95       450
    


ë‹¤ì¤‘ í´ë˜ìŠ¤ìš© f1ì ìˆ˜ëŠ” í•œ í´ë˜ìŠ¤ë¥¼ ì–‘ì„± í´ë˜ìŠ¤ë¡œ ë‘ê³  ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ë“¤ì„ ìŒã……ì–´ í´ë˜ìŠ¤ë¡œ ê°„ì£¼í•´ì„œ í´ë˜ìŠ¤ë§ˆë‹¤ f1 ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤ìŒ, í´ë˜ìŠ¤ë³„ f1ì ìˆ˜ë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•´ì„œ í‰ê· ì„ ë‚¸ë‹¤.

* macro í‰ê·  : í´ë˜ìŠ¤ë³„ f1 ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì§€ ì•Šê³  í‰ê· ì„ ë‚¸ë‹¤.
* weighted í‰ê·  : í´ë˜ìŠ¤ë³„ ìƒ˜í”Œìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹ ë¥´ ë‘ì–´ f1 ì ìˆ˜ì˜ í‰ê· ì„ ë‚¸ë‹¤.
* micro í‰ê·  : ëª¨ë“  í´ë˜ìŠ¤ì˜ FP,FN,TPì˜ ì´ ìˆ˜ë¥¼ í—¤ì•„ë¦° ë‹¤ìŒ f1 ì ìˆ˜ë¥¼ ì´ ìˆ˜ì¹˜ë¡œ ê³„ì‚°í•œë‹¤.
