## 9. MobilenNet ,MobileNetV2 모델 알아보기

### 9.0 들어가며
이번 글은 MobileNet에 관련된 논문에 설명된 모델에 대해서 알아볼 것이다. 참고한 논문은 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Application이고, [**이 블로그**](https://deep-learning-study.tistory.com/532)의 내용을 참고했다.

### 9.1 MobileNet(V1)
MobileNet은 Depthwise seperable convolution을 활용해서 모델을 경량화하는데 집중했다. MobileNet이 경량화에 집중한 이유는 메모리가 제한된 환경에서 딥러닝을 적용하기 용이한 모델을 만들기 위해서이다. 먼저 Depthwise Seperable Convolution에 대해 알아보자.

#### 9.1.1 Depthwise Seperable Convolution
**Depthwise Seperable Convolution**은 Depthwise Convolution 이후에 Pointwise Convolution을 결합한 것이다. 

이미지1

1. Depthwise Convolution

Depthwise Convolution은 각 입력 채널에 대해 3x3 conv 하나의 필터가 연산을 수행해서 하나의 피쳐맵을 생성한다. 예를 들어 입력 채널 개수가 M개이면 M개의 피쳐맵을 생성한다. 따라서 Depthwise convolution의 연산량은 다음과 같다.

이미지2

여기서 Dk는 입력값 크기, M은 입력값의 채널 수, DF는 피쳐맵 크기이다.

2. Pointwise convolution

Pointwise convolution은 depthwise convolution이 생성한 피쳐맵들을 1x1 conv로 채널 수를 조정한다.1x1 필터는 모든 채널에 대해 연산하기 때문에 cross-channel correlation을 계산하는 역할을 한다.

Pointwise convolution의 연산량은 다음과 같다.

이미지3

여기서 M은 입력 채널 수, N은 출력 채널 수, DF는 피쳐맵 크기이다.

아래 이미지는 기존 CNN 계산과 Depthwise seperable convolution을 비교한 이미지다. 

이미지4

연산량을 비교해보면 기존 conv 연산량은 

이미지5 

이고, Depthwise seperable convolution의 연산량은 Depthwise convolution의 연산량과 Pointwise convolution의 연산량을 더하면 되기 때문에 다음과 같다.

이미지6

이는 기존 conv 계산의 연산량보다 약 8배정도 적기 때문에 경량화가 가능하다.

#### 9.1.2 MobileNet Architecture

MobileNet의 구조는 다음과 같다.

이미지7

첫 번째 conv를 제외하고는 depthwise seperable convolution을 사용하는 것을 볼 수 있다. 또한 마지막 FC층을 제외하고 모든 층에 BN, ReLU를 사용하고, 다운 샘플링은 depthwise convolution층과 첫번째 conv 층에서 사용한다.총 28개의 layer를 갖는다.

#### 9.1.3 하이퍼파라미터
MobileNet 모델의 latency와 accuracy를 조절하는 두개의 하이퍼파라미터가 있다.

1. Width Multiplier : Thinner Models

첫 번째 하이퍼파라미터 α는 모델의 두께를 결정한다. 컨브넷에서 모델의 두께는 각 레이어에서의 필터 수를 의미한다. α는 더 얇은 모델이 필요할 때 사용한다. 입력 채널 M과 출력 채널 N에 적용하면 αM, αN이 되고 연산량은 다음과 같이 된다.

이미지8

α의 범위는 [0,1]이고, α의 값을 낮추면 모델의 파라미터 수가 감소한다. 기본 MobileNet은 1을 사용한다. 다음은 α의 값에 따른 정확도와 파라미터 수를 정리한 표다.

이미지9

2. Resolution Multiplier : Reduced Representation

Resolution Multiplier ρ는 모델의 연산량을 감소시키기 위해 사용한다. ρ는 입력 이미지에 적용해서 해상도를 낮추는 역할을 한다. ρ의 범위 또한 [0,1]이고, 기본 MobileNet은 ρ=1을 사용한다. 다음은 입력 이미지 크기가 224,192,169,128일때 정확도와 파라미터 수를 비교한 표다.

이미지10

#### 9.1.4 다른 모델과 비교

다음은 MobileNet과 다른 유명한 모델들과 정확도와 파라미터 수를 비교한 표다.

이미지 11

### 9.2 MobileNetV2

이름에서 알 수 있듯이 MobileNetV2는 MobileNet(V1)의 후속작이다. MobileNetV2은 수정된 Depthwise Seperable Convolution인 Inverted Residuals과 Linear Bottlenecks를 사용한 Convolution Block을 제안한다. 

#### 9.2.1 Inverted Residuals와 Linear Bottlenecks의 등장 배경

Inverted Residuals와 Linear Bottlenecks은 ReLU 함수를 거치게 되면 정보가 손실되는 것을 최소화하기 위해 제안되었다. ReLU의 정보 손실에 대해 알아보면, 채널 수가 적은 입력 값은 ReLU 함수를 거치면 정보가 손실되지만 채널 수가 많은 입력 값은 ReLU 함수를 거쳐도 정보가 보존된다. 따라서 ReLU 함수를 사용할 때는 해당 layer에 많은 채널 수를 사용하고 해당 레이어에 채널 수가 적다면 ReLU 함수가 아닌 linear 함수를 사용한다는 아이디어가 MobileNetV2에 사용되는 기법들의 등장 배경이다. 아래 이미지는 채널 수에 따른 ReLU 함수를 거쳤을 때의 손실 정도를 실험적으로 증명한 것이다.

이미지 12

#### 9.2.2 Convolution Block for MobileNetV2

1. Linear Bottlenecks 

Bottleneck 구조는 ResNet에서 연산량 감소를 위해 제안된 구조이다. 9.2.1절에서 언급했듯이 채널 수가 적은 레이어에는 linear 함수를 사용한다. 따라서 이 BottleNeck 구조에 채널 수가 적은 레이어에는 linear 함수를 사용한다. 다음은 비선형 함수와 선형 함수를 사용했을 때 성능을 비교한 표다.

이미지13

선형함수를 사용했을 때의 정확도가 더 높은 것을 알 수 있다.

2. Inverted residuals

기존의 residual 구조는 첫번째 1x1 conv에서 채널 수를 감소시키고 3x3 conv로 전달한다. 이렇게 되면 1x1에서 채널 수가 감소되었기 때문에 ReLU 함수를 사용하면 정보 손실이 발생하기 때문에 첫번째 레이어에서 입력값의 채널 수를 증가시키고 3x3 conv 층으로 전달한다.

다음은 기존 residual 구조와 inverted reisual구조를 비교한 이미지다.

이미지14

3. ReLU6

MobileNetV2와 MobileNetV1에서는 ReLU6 함수를 사용한다. 연산량 감소 효과에 도움을 준다고 한다.

ReLU6함수는 다음과 같다.

이미지15

다음은 MobileNet(V1)과 MobileNetV2에서 사용하는 Convolution block을 비교한 이미지다. stride=2을 사용하는 block은 다운 샘플링을 위해 사용한다.

이미지16

#### 9.2.3 MobileNetV2의 연산

최종적으로 k채널 입력 값은 1x1 conv를 거쳐 tk 채널로 확장되고, 3x3 conv에 전달되고 linear 1x1 conv를 거쳐서 k'개로 채널 수가 감소한다. 여기서 t는 expansion factor이라는 이름으로 도입되었고 논문에서의 실험에서는 모두 6으로 지정되어 사용되었다.

이미지17

#### 9.2.4 MobileNetV2 architecture

첫번째 레이어는 일반적인 conv를 사용하고, 그 이후에 18개의 convolution block을 쌓았다. 아래 표에서 n은 같은 layer가 n번 반복해서 쌓였다는 것이고, c는 출력 채널 수를 나타내는 것이고, s는 각 block에서 사용된 strides의 값이다.

이미지18

#### 9.2.5 다른 모델들과 성능비교
다른 경량화 모델들에 비해 높은 성능을 보이고 비교적 적은 parameter를 사용하는 것을 볼 수 있다.

이미지19

