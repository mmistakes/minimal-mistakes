

# TIL: ì„ í˜• íšŒê·€ ë¶„ì„ (Linear Regression)

## ğŸ“Œ ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©
ì„ í˜• íšŒê·€ëŠ” ë…ë¦½ ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ì¤‘ìš”í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

## 1. ë‹¨ìˆœ ì„ í˜• íšŒê·€ (Simple Linear Regression)

### ê°œë…
- í•˜ë‚˜ì˜ ë…ë¦½ ë³€ìˆ˜(X)ë¡œ ì¢…ì† ë³€ìˆ˜(y)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•
- y = wx + b í˜•íƒœì˜ ì§ì„  ë°©ì •ì‹ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ
  - w: ê°€ì¤‘ì¹˜(weight) ë˜ëŠ” ê¸°ìš¸ê¸°(slope)
  - b: í¸í–¥(bias) ë˜ëŠ” yì ˆí¸(intercept)

### êµ¬í˜„ ì˜ˆì‹œ
```python
import pandas as pd
from sklearn.linear_model import LinearRegression

# ë°ì´í„° ì¤€ë¹„
dataset = pd.read_csv('LinearRegressionData.csv')
X = dataset['hour'].values
y = dataset['score'].values

# ëª¨ë¸ í•™ìŠµ
reg = LinearRegression()
reg.fit(X.reshape(-1, 1), y)

# ì˜ˆì¸¡
prediction = reg.predict([[9]])  # 9ì‹œê°„ ê³µë¶€í–ˆì„ ë•Œì˜ ì˜ˆìƒ ì ìˆ˜
```

### í•™ìŠµ ë‚´ìš©
- `.reshape(-1, 1)`ì´ í•„ìš”í•œ ì´ìœ : scikit-learnì€ 2D ë°°ì—´ í˜•íƒœì˜ ì…ë ¥ì„ ìš”êµ¬
- `.fit()`ì€ ëª¨ë¸ í•™ìŠµì„, `.predict()`ëŠ” ì˜ˆì¸¡ì„ ìˆ˜í–‰
- `reg.coef_`ë¡œ ê¸°ìš¸ê¸°, `reg.intercept_`ë¡œ yì ˆí¸ í™•ì¸ ê°€ëŠ¥

## 2. ë°ì´í„°ì…‹ ë¶„ë¦¬

### ê°œë…
- ê³¼ì í•©(Overfitting) ë°©ì§€ë¥¼ ìœ„í•´ ë°ì´í„°ë¥¼ í›ˆë ¨ì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¶„ë¦¬
- ë³´í†µ 8:2 ë˜ëŠ” 7:3 ë¹„ìœ¨ë¡œ ë¶„ë¦¬

### êµ¬í˜„
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,     # í…ŒìŠ¤íŠ¸ì…‹ 20%
    random_state=0     # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œê°’
)
```

## 3. ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)

### ê°œë…
- ë¹„ìš© í•¨ìˆ˜(Cost Function)ë¥¼ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- í•™ìŠµë¥ (Learning Rate)ì— ë”°ë¼ ìˆ˜ë ´ ì†ë„ì™€ ì •í™•ë„ê°€ ë‹¬ë¼ì§

### êµ¬í˜„
```python
from sklearn.linear_model import SGDRegressor

sgd = SGDRegressor(
    max_iter=200,      # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    eta0=1e-4,        # í•™ìŠµë¥ 
    random_state=0
)
```

## 4. ë‹¤ì¤‘ ì„ í˜• íšŒê·€ (Multiple Linear Regression)

### ê°œë…
- ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ ë³€ìˆ˜ë¡œ ì¢…ì† ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡
- ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì²˜ë¦¬ í•„ìš”

### ì›-í•« ì¸ì½”ë”© ì˜ˆì‹œ
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(
    transformers=[('encoder', OneHotEncoder(drop='first'), [2])],
    remainder='passthrough'
)
```

## 5. ëª¨ë¸ í‰ê°€ ì§€í‘œ

### MAE (Mean Absolute Error)
- ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ ì ˆëŒ€ê°’ì˜ í‰ê· 
- ì´í•´í•˜ê¸° ì‰½ê³  ì´ìƒì¹˜ì˜ ì˜í–¥ì´ ì ìŒ

### MSE (Mean Squared Error)
- ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ ì œê³±ì˜ í‰ê· 
- í° ì˜¤ì°¨ì— ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘

### RMSE (Root Mean Squared Error)
- MSEì˜ ì œê³±ê·¼
- ì‹¤ì œ ê°’ê³¼ ê°™ì€ ë‹¨ìœ„ë¡œ í•´ì„ ê°€ëŠ¥

### RÂ² (ê²°ì •ê³„ìˆ˜)
- ëª¨ë¸ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„
- 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸

## ğŸ¤” ì˜¤ëŠ˜ì˜ ê³ ë¯¼
1. ì–¸ì œ ë‹¨ìˆœ ì„ í˜• íšŒê·€ì™€ ë‹¤ì¤‘ ì„ í˜• íšŒê·€ë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œ?
2. ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œ?
3. ê° í‰ê°€ ì§€í‘œì˜ íŠ¹ì„±ì— ë”°ë¥¸ ì ì ˆí•œ ì‚¬ìš© ìƒí™©ì€?

## ğŸ“š ì°¸ê³ ìë£Œ
- scikit-learn ê³µì‹ ë¬¸ì„œ
- Python Machine Learning (Sebastian Raschka)

## ğŸ’¡ ë‹¤ìŒì— í•™ìŠµí•  ë‚´ìš©
- ë‹¤í•­ íšŒê·€ (Polynomial Regression)
- ì •ê·œí™” (Regularization)
- êµì°¨ ê²€ì¦ (Cross Validation)