---
layout: post
title: "Title of the first post"
---
```
import re
from typing import Dict

def parse_markdown_sections(text: str) -> Dict[str, str]:
    """
    Parse markdown text into up to five sections, where two middle sections are optional:
      1. chunk_summary:        from **Chunk Summary** to the first of
                               **Detected Submodules**, **Updated Checklist**, or **Files to Save**
      2. detected_submodules:  from **Detected Submodules** to the next of
                               **Updated Checklist** or **Files to Save**
                               (empty if marker absent)
      3. checklist:            from **Updated Checklist** to **Files to Save**
                               (empty if marker absent)
      4. file_names:           from **Files to Save** to **Next Step**
    
    Raises:
        ValueError: if any of the mandatory markers
                    (**Chunk Summary**, **Files to Save**, **Next Step**)
                    are missing or out of order, or if any optional marker
                    appears outside the chunk_summaryâ€“file_names range.
    """
    # Define markers
    M = {
        "chunk_summary":       "**Chunk Summary**",
        "detected_submodules": "**Detected Submodules**",
        "checklist":           "**Updated Checklist**",
        "file_names":          "**Files to Save**",
        "next_step":           "**Next Step**"
    }

    # Locate mandatory markers (raise if missing)
    idx_chunk = text.index(M["chunk_summary"])
    idx_files = text.index(M["file_names"])
    idx_next  = text.index(M["next_step"])

    # Locate optional markers (find returns -1 if absent)
    idx_ds      = text.find(M["detected_submodules"])
    idx_updated = text.find(M["checklist"])

    # Basic order check for mandatory markers
    if not (idx_chunk < idx_files < idx_next):
        raise ValueError(
            "Required markers (**Chunk Summary**, **Files to Save**, **Next Step**) "
            "must appear in that order."
        )

    # Validate optional markers, if present, lie between chunk and files
    for name, idx in (("Detected Submodules", idx_ds),
                      ("Updated Checklist", idx_updated)):
        if idx != -1 and not (idx_chunk < idx < idx_files):
            raise ValueError(f"Optional marker **{name}** out of expected range")

    # Build list of sections to extract, sorted by their index
    sections = [
        ("chunk_summary", idx_chunk, len(M["chunk_summary"])),
    ]
    if idx_ds != -1:
        sections.append(("detected_submodules", idx_ds, len(M["detected_submodules"])))
    if idx_updated != -1:
        sections.append(("checklist", idx_updated, len(M["checklist"])))
    sections.append(("file_names", idx_files, len(M["file_names"])))

    # Sort by position in text
    sections.sort(key=lambda x: x[1])

    # Extract each section's content up to the next marker (or **Next Step**)
    result: Dict[str, str] = {}
    for i, (key, idx_start, marker_len) in enumerate(sections):
        idx_end = sections[i+1][1] if i+1 < len(sections) else idx_next
        content = text[idx_start + marker_len : idx_end].strip()
        result[key] = content

    # Ensure all expected keys exist (fill missing optional with empty)
    for optional in ("detected_submodules", "checklist"):
        result.setdefault(optional, "")

    return result


# --- Example Usage ---

if __name__ == "__main__":
    sample = """
    **Chunk Summary**
    ì—¬ê¸°ì—ëŠ” ì²­í¬ ìš”ì•½ì´ ìˆìŠµë‹ˆë‹¤.

    **Detected Submodules**
    - submod1
    - submod2

    **Updated Checklist**
    - [ ] Task A
    - [x] Task B

    **Files to Save**
    - `example_report.md`
    - `data.csv`

    **Next Step**
    ë‹¤ìŒ ë‹¨ê³„ ì„¤ëª…...
    """

    sections = parse_markdown_sections(sample)
    for name, content in sections.items():
        print(f"--- {name} ---")
        print(content, "\n")

```
```
from typing import Dict

def parse_markdown_sections(text: str) -> Dict[str, str]:
    """
    Parse markdown text into three sections:
      1. chunk_summary: between **Chunk Summary** and **Updated Checklist**
      2. checklist:     between **Updated Checklist** and **Files to Save**
      3. file_names:    between **Files to Save** and **Next Step**
    
    Raises:
        ValueError: if any marker is missing or out of order.
    """
    # Define the literal markers
    markers = {
        "chunk_summary": "**Chunk Summary**",
        "checklist":     "**Updated Checklist**",
        "file_names":    "**Files to Save**",
        "next_step":     "**Next Step**"
    }
    
    try:
        idx_chunk   = text.index(markers["chunk_summary"])
        idx_updated = text.index(markers["checklist"])
        idx_files   = text.index(markers["file_names"])
        idx_next    = text.index(markers["next_step"])
    except ValueError as e:
        # Marker ìì²´ê°€ ì—†ìœ¼ë©´ ì—¬ê¸°ë¡œ ë¹ ì§‘ë‹ˆë‹¤
        raise ValueError(f"Missing required marker: {e}")

    # ìˆœì„œê°€ ë’¤ë°”ë€Œë©´ ì˜ëª»ëœ ì…ë ¥ìœ¼ë¡œ ê°„ì£¼
    if not (idx_chunk < idx_updated < idx_files < idx_next):
        raise ValueError("Markers are in unexpected order")

    # ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ (ë§ˆì»¤ í…ìŠ¤íŠ¸ ì œì™¸í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°)
    chunk_summary = text[
        idx_chunk + len(markers["chunk_summary"]) : idx_updated
    ].strip()
    checklist = text[
        idx_updated + len(markers["checklist"]) : idx_files
    ].strip()
    file_names = text[
        idx_files + len(markers["file_names"]) : idx_next
    ].strip()

    return {
        "chunk_summary": chunk_summary,
        "checklist": checklist,
        "file_names": file_names
    }


# ì˜ˆì‹œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    sample = """
    some intro text...

    **Chunk Summary**
    - ì´ê³³ì— ì²­í¬ ìš”ì•½ ë‚´ìš©ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.
    ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ë„ ìˆê³ , _markdown_ í˜•ì‹ì´ ì„ì—¬ ìˆì„ ìˆ˜ë„ ìˆì£ .

    **Updated Checklist**
    | Item          | Status |
    |---------------|--------|
    | First check   | âœ…     |
    | Second check  | âŒ     |

    **Files to Save**
    - report.pdf
    - data.csv

    **Next Step**
    ì—¬ê¸°ì— ë‹¤ìŒ ìŠ¤í…ì´ ì„œìˆ ë©ë‹ˆë‹¤.
    """
    sections = parse_markdown_sections(sample)
    print("Chunk Summary:", sections["chunk_summary"])
    print("Checklist:", sections["checklist"])
    print("Files to Save:", sections["file_names"])
```

=========\
```
flowchart LR
  BB[Black Box]:::blackBox
  LLM[ğŸ§  LLM]:::llmBox
  WB[White Box]:::whiteBox

  BB --> LLM
  LLM --> WB

  classDef blackBox fill:#444444,stroke:#000,color:#fff;
  classDef llmBox fill:#4a90e2,stroke:#2a5dab,color:#fff;
  classDef whiteBox fill:#ffffff,stroke:#ccc,color:#333;
```

Text generation and prompting
=============================

Learn how to prompt a model to generate text.

With the OpenAI API, you can use a [large language model](/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com). Models can generate almost any kind of text responseâ€”like code, mathematical equations, structured JSON data, or human-like prose.

Here's a simple example using the [Responses API](/docs/api-reference/responses).

Generate text from a simple prompt

```javascript
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
    model: "gpt-4.1",
    input: "Write a one-sentence bedtime story about a unicorn."
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4.1",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)
```

```bash
curl "https://api.openai.com/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4.1",
        "input": "Write a one-sentence bedtime story about a unicorn."
    }'
```

An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:

```json
[
    {
        "id": "msg_67b73f697ba4819183a15cc17d011509",
        "type": "message",
        "role": "assistant",
        "content": [
            {
                "type": "output_text",
                "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
                "annotations": []
            }
        ]
    }
]
```

**The `output` array often has more than one item in it!** It can contain tool calls, data about reasoning tokens generated by [reasoning models](/docs/guides/reasoning), and other items. It is not safe to assume that the model's text output is present at `output[0].content[0].text`.

Some of our [official SDKs](/docs/libraries) include an `output_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.

In addition to plain text, you can also have the model return structured data in JSON format - this feature is called [**Structured Outputs**](/docs/guides/structured-outputs).

Choosing a model
----------------

A key choice to make when generating content through the API is which model you want to use - the `model` parameter of the code samples above. [You can find a full listing of available models here](/docs/models). Here are a few factors to consider when choosing a model for text generation.

*   **[Reasoning models](/docs/guides/reasoning)** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.
*   **GPT models** are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.
*   **Large and small (mini or nano) models** offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use.

When in doubt, [`gpt-4.1`](/docs/models/gpt-4.1) offers a solid combination of intelligence, speed, and cost effectiveness.

Prompt engineering
------------------

**Prompt engineering** is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.

Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices you can apply to consistently get good results from a model.

Some prompt engineering techniques will work with every model, like using message roles. But different model types (like reasoning versus GPT models) might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you are building more complex applications, we strongly recommend that you:

*   Pin your production applications to specific [model snapshots](/docs/models) (like `gpt-4.1-2025-04-14` for example) to ensure consistent behavior.
*   Build [evals](/docs/guides/evals) that will measure the behavior of your prompts, so that you can monitor the performance of your prompts as you iterate on them, or when you change and upgrade model versions.

Now, let's examine some tools and techniques available to you to construct prompts.

Message roles and instruction following
---------------------------------------

You can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain_of_command) using the `instructions` API parameter or **message roles**.

The `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the `input` parameter.

Generate text with instructions

```javascript
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
    model: "gpt-4.1",
    instructions: "Talk like a pirate.",
    input: "Are semicolons optional in JavaScript?",
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4.1",
    instructions="Talk like a pirate.",
    input="Are semicolons optional in JavaScript?",
)

print(response.output_text)
```

```bash
curl "https://api.openai.com/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4.1",
        "instructions": "Talk like a pirate.",
        "input": "Are semicolons optional in JavaScript?"
    }'
```

The example above is roughly equivalent to using the following input messages in the `input` array:

Generate text with messages using different roles

```javascript
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
    model: "gpt-4.1",
    input: [
        {
            role: "developer",
            content: "Talk like a pirate."
        },
        {
            role: "user",
            content: "Are semicolons optional in JavaScript?",
        },
    ],
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4.1",
    input=[
        {
            "role": "developer",
            "content": "Talk like a pirate."
        },
        {
            "role": "user",
            "content": "Are semicolons optional in JavaScript?"
        }
    ]
)

print(response.output_text)
```

```bash
curl "https://api.openai.com/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4.1",
        "input": [
            {
                "role": "developer",
                "content": "Talk like a pirate."
            },
            {
                "role": "user",
                "content": "Are semicolons optional in JavaScript?"
            }
        ]
    }'
```

Note that the `instructions` parameter only applies to the current response generation request. If you are [managing conversation state](/docs/guides/conversation-state) with the `previous_response_id` parameter, the `instructions` used on previous turns will not be present in the context.

The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how our models give different levels of priority to messages with different roles.

|developer|user|assistant|
|---|---|---|
|developer messages are instructions provided by the application developer, prioritized ahead of user messages.|user messages are instructions provided by an end user, prioritized behind developer messages.|Messages generated by the model have the assistant role.|

A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](/docs/guides/conversation-state).

You could think about `developer` and `user` messages like a function and its arguments in a programming language.

*   `developer` messages provide the system's rules and business logic, like a function definition.
*   `user` messages provide inputs and configuration to which the `developer` message instructions are applied, like arguments to a function.

Reusable prompts
----------------

In the OpenAI dashboard, you can develop reusable [prompts](/playground/prompts) that you can use in API requests, rather than specifying the content of prompts in code. This way, you can more easily build and evaluate your prompts, and deploy improved versions of your prompts without changing your integration code.

Here's how it works:

1.  **Create a reusable prompt** in the [dashboard](/playground/prompts) with placeholders like `{{customer_name}}`.
2.  **Use the prompt** in your API request with the `prompt` parameter. The prompt parameter object has three properties you can configure:
    *   `id` â€” Unique identifier of your prompt, found in the dashboard
    *   `version` â€” A specific version of your prompt (defaults to the "current" version as specified in the dashboard)
    *   `variables` â€” A map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input message types like `input_image` or `input_file`. [See the full API reference](/docs/api-reference/responses/create).

String variables

Generate text with a prompt template

```javascript
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
    model: "gpt-4.1",
    prompt: {
        id: "pmpt_abc123",
        version: "2",
        variables: {
            customer_name: "Jane Doe",
            product: "40oz juice box"
        }
    }
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4.1",
    prompt={
        "id": "pmpt_abc123",
        "version": "2",
        "variables": {
            "customer_name": "Jane Doe",
            "product": "40oz juice box"
        }
    }
)

print(response.output_text)
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "prompt": {
      "id": "pmpt_abc123",
      "version": "2",
      "variables": {
        "customer_name": "Jane Doe",
        "product": "40oz juice box"
      }
    }
  }'
```

Variables with file input

Prompt template with file input variable

```javascript
import fs from "fs";
import OpenAI from "openai";
const client = new OpenAI();

// Upload a PDF we will reference in the prompt variables
const file = await client.files.create({
    file: fs.createReadStream("draconomicon.pdf"),
    purpose: "user_data",
});

const response = await client.responses.create({
    model: "gpt-4.1",
    prompt: {
        id: "pmpt_abc123",
        variables: {
            topic: "Dragons",
            reference_pdf: {
                type: "input_file",
                file_id: file.id,
            },
        },
    },
});

console.log(response.output_text);
```

```python
import openai, pathlib

client = openai.OpenAI()

# Upload a PDF we will reference in the variables
file = client.files.create(
    file=open("draconomicon.pdf", "rb"),
    purpose="user_data",
)

response = client.responses.create(
    model="gpt-4.1",
    prompt={
        "id": "pmpt_abc123",
        "variables": {
            "topic": "Dragons",
            "reference_pdf": {
                "type": "input_file",
                "file_id": file.id,
            },
        },
    },
)

print(response.output_text)
```

```bash
# Assume you have already uploaded the PDF and obtained FILE_ID
curl https://api.openai.com/v1/responses   -H "Authorization: Bearer $OPENAI_API_KEY"   -H "Content-Type: application/json"   -d '{
    "model": "gpt-4.1",
    "prompt": {
      "id": "pmpt_abc123",
      "variables": {
        "topic": "Dragons",
        "reference_pdf": {
          "type": "input_file",
          "file_id": "file-abc123"
        }
      }
    }
  }'
```

Message formatting with Markdown and XML
----------------------------------------

When writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and context data using a combination of [Markdown](https://commonmark.org/help/) formatting and [XML tags](https://www.w3.org/TR/xml/).

Markdown headers and lists can be helpful to mark distinct sections of a prompt, and to communicate hierarchy to the model. They can also potentially make your prompts more readable during development. XML tags can help delineate where one piece of content (like a supporting document used for reference) begins and ends. XML attributes can also be used to define metadata about content in the prompt that can be referenced by your instructions.

In general, a developer message will contain the following sections, usually in this order (though the exact optimal content and order may vary by which model you are using):

*   **Identity:** Describe the purpose, communication style, and high-level goals of the assistant.
*   **Instructions:** Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should [call custom functions](/docs/guides/function-calling).
*   **Examples:** Provide examples of possible inputs, along with the desired output from the model.
*   **Context:** Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.

Below is an example of using Markdown and XML tags to construct a `developer` message with distinct sections and supporting examples.

Example prompt

A developer message for code generation

```text
# Identity

You are coding assistant that helps enforce the use of snake case 
variables in JavaScript code, and writing code that will run in 
Internet Explorer version 6.

# Instructions

* When defining variables, use snake case names (e.g. my_variable) 
  instead of camel case names (e.g. myVariable).
* To support old browsers, declare variables using the older 
  "var" keyword.
* Do not give responses with Markdown formatting, just return 
  the code as requested.

# Examples

<user_query>
How do I declare a string variable for a first name?
</user_query>

<assistant_response>
var first_name = "Anna";
</assistant_response>
```

API request

Send a prompt to generate code through the API

```javascript
import fs from "fs/promises";
import OpenAI from "openai";
const client = new OpenAI();

const instructions = await fs.readFile("prompt.txt", "utf-8");

const response = await client.responses.create({
    model: "gpt-4.1",
    instructions,
    input: "How would I declare a variable for a last name?",
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

with open("prompt.txt", "r", encoding="utf-8") as f:
    instructions = f.read()

response = client.responses.create(
    model="gpt-4.1",
    instructions=instructions,
    input="How would I declare a variable for a last name?",
)

print(response.output_text)
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "instructions": "'"$(< prompt.txt)"'",
    "input": "How would I declare a variable for a last name?"
  }'
```

#### Save on cost and latency with prompt caching

When constructing a message, you should try and keep content that you expect to use over and over in your API requests at the beginning of your prompt, **and** among the first API parameters you pass in the JSON request body to [Chat Completions](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses). This enables you to maximize cost and latency savings from [prompt caching](/docs/guides/prompt-caching).

Few-shot learning
-----------------

Few-shot learning lets you steer a large language model toward a new task by including a handful of input/output examples in the prompt, rather than [fine-tuning](/docs/guides/model-optimization) the model. The model implicitly "picks up" the pattern from those examples and applies it to a prompt. When providing examples, try to show a diverse range of possible inputs with the desired outputs.

Typically, you will provide examples as part of a `developer` message in your API request. Here's an example `developer` message containing examples that show a model how to classify positive or negative customer service reviews.

```text
# Identity

You are a helpful assistant that labels short product reviews as
Positive, Negative, or Neutral.

# Instructions

* Only output a single word in your response with no additional formatting
  or commentary.
* Your response should only be one of the words "Positive", "Negative", or
  "Neutral" depending on the sentiment of the product review you are given.

# Examples

<product_review id="example-1">
I absolutely love this headphones â€” sound quality is amazing!
</product_review>

<assistant_response id="example-1">
Positive
</assistant_response>

<product_review id="example-2">
Battery life is okay, but the ear pads feel cheap.
</product_review>

<assistant_response id="example-2">
Neutral
</assistant_response>

<product_review id="example-3">
Terrible customer service, I'll never buy from them again.
</product_review>

<assistant_response id="example-3">
Negative
</assistant_response>
```

Include relevant context information
------------------------------------

It is often useful to include additional context information the model can use to generate a response within the prompt you give the model. There are a few common reasons why you might do this:

*   To give the model access to proprietary data, or any other data outside the data set the model was trained on.
*   To constrain the model's response to a specific set of resources that you have determined will be most beneficial.

The technique of adding additional relevant context to the model generation request is sometimes called **retrieval-augmented generation (RAG)**. You can add additional context to the prompt in many different ways, from querying a vector database and including the text you get back into a prompt, or by using OpenAI's built-in [file search tool](/docs/guides/tools-file-search) to generate content based on uploaded documents.

#### Planning for the context window

Models can only handle so much data within the context they consider during a generation request. This memory limit is called a **context window**, which is defined in terms of [tokens](https://blogs.nvidia.com/blog/ai-tokens-explained) (chunks of data you pass in, from text to images).

Models have different context window sizes from the low 100k range up to one million tokens for newer GPT-4.1 models. [Refer to the model docs](/docs/models) for specific context window sizes per model.

Prompting GPT-4.1 models
------------------------

GPT models like [`gpt-4.1`](/docs/models/gpt-4.1) benefit from precise instructions that explicitly provide the logic and data required to complete the task in the prompt. GPT-4.1 in particular is highly steerable and responsive to well-specified prompts. To get the most out of GPT-4.1, refer to the prompting guide in the cookbook.

[

GPT-4.1 prompting guide

Get the most out of prompting GPT-4.1 with the tips and tricks in this prompting guide, extracted from real-world use cases and practical experience.

](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)

#### GPT-4.1 prompting best practices

While the [cookbook](https://cookbook.openai.com/examples/gpt4-1_prompting_guide) has the best and most comprehensive guidance for prompting this model, here are a few best practices to keep in mind.

Building agentic workflows

### System Prompt Reminders

In order to best utilize the agentic capabilities of GPT-4.1, we recommend including three key types of reminders in all agent prompts for persistence, tool calling, and planning. As a whole, we find that these three instructions transform the model's behavior from chatbot-like into a much more "eager" agent, driving the interaction forward autonomously and independently. Here are a few examples:

```text
## PERSISTENCE
You are an agent - please keep going until the user's query is completely
resolved, before ending your turn and yielding back to the user. Only
terminate your turn when you are sure that the problem is solved.

## TOOL CALLING
If you are not sure about file content or codebase structure pertaining to
the user's request, use your tools to read files and gather the relevant
information: do NOT guess or make up an answer.

## PLANNING
You MUST plan extensively before each function call, and reflect
extensively on the outcomes of the previous function calls. DO NOT do this
entire process by making function calls only, as this can impair your
ability to solve the problem and think insightfully.
```

#### Tool Calls

Compared to previous models, GPT-4.1 has undergone more training on effectively utilizing tools passed as arguments in an OpenAI API request. We encourage developers to exclusively use the tools field of API requests to pass tools for best understanding and performance, rather than manually injecting tool descriptions into the system prompt and writing a separate parser for tool calls, as some have reported doing in the past.

#### Diff Generation

Correct diffs are critical for coding applications, so we've significantly improved performance at this task for GPT-4.1. In our cookbook, we open-source a recommended diff format on which GPT-4.1 has been extensively trained. That said, the model should generalize to any well-specified format.

Using long context

GPT-4.1 has a performant 1M token input context window, and will be useful for a variety of long context tasks, including structured document parsing, re-ranking, selecting relevant information while ignoring irrelevant context, and performing multi-hop reasoning using context.

#### Optimal Context Size

We show perfect performance at needle-in-a-haystack evals up to our full context size, and we've observed very strong performance at complex tasks with a mix of relevant and irrelevant code and documents in the range of hundreds of thousands of tokens.

#### Delimiters

We tested a variety of delimiters for separating context provided to the model against our long context evals. Briefly, XML and the format demonstrated by Lee et al. ([ref](https://arxiv.org/pdf/2406.13121)) tend to perform well, while JSON performed worse for this task. See our cookbook for prompt examples.

#### Prompt Organization

Especially in long context usage, placement of instructions and context can substantially impact performance. In our experiments, we found that it was optimal to put critical instructions, including the user query, at both the top and the bottom of the prompt; this elicited marginally better performance from the model than putting them only at the top, and much better performance than only at the bottom.

Prompting for chain of thought

As mentioned above, GPT-4.1 isn't a reasoning model, but prompting the model to think step by step (called "chain of thought") can be an effective way for a model to break down problems into more manageable pieces. The model has been trained to perform well at agentic reasoning and real-world problem solving, so it shouldn't require much prompting to do well.

We recommend starting with this basic chain-of-thought instruction at the end of your prompt:

```text
First, think carefully step by step about what documents are needed to answer the query. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.
```

From there, you should improve your CoT prompt by auditing failures in your particular examples and evals, and addressing systematic planning and reasoning errors with more explicit instructions. See our cookbook for a prompt example demonstrating a more opinionated reasoning strategy.

Instruction following

GPT-4.1 exhibits outstanding instruction-following performance, which developers can leverage to precisely shape and control the outputs for their particular use cases. However, since the model follows instructions more literally than its predecessors, may need to provide more explicit specification around what to do or not do, and existing prompts optimized for other models may not immediately work with this model.

#### Recommended Workflow

Here is our recommended workflow for developing and debugging instructions in prompts:

*   Start with an overall "Response Rules" or "Instructions" section with high-level guidance and bullet points.
*   If you'd like to change a more specific behavior, add a section containing more details for that category, like `## Sample Phrases`.
*   If there are specific steps you'd like the model to follow in its workflow, add an ordered list and instruct the model to follow these steps.
*   If behavior still isn't working as expected, check for conflicting, underspecified, or incorrect instructions and examples. If there are conflicting instructions, GPT-4.1 tends to follow the one closer to the end of the prompt.
*   Add examples that demonstrate desired behavior; ensure that any important behavior demonstrated in your examples are also cited in your rules.
*   It's generally not necessary to use all-caps or other incentives like bribes or tips, but developers can experiment with this for extra emphasis if so desired.

#### Common Failure Modes

These failure modes are not unique to GPT-4.1, but we share them here for general awareness and ease of debugging.

*   Instructing a model to always follow a specific behavior can occasionally induce adverse effects. For instance, if told "you must call a tool before responding to the user," models may hallucinate tool inputs or call the tool with null values if they do not have enough information. Adding "if you don't have enough information to call the tool, ask the user for the information you need" should mitigate this.
*   When provided sample phrases, models can use those quotes verbatim and start to sound repetitive to users. Ensure you instruct the model to vary them as necessary.
*   Without specific instructions, some models can be eager to provide additional prose to explain their decisions, or output more formatting in responses than may be desired. Provide instructions and potentially examples to help mitigate.

See our cookbook for an example customer service prompt that demonstrates these principles.

Prompting reasoning models
--------------------------

There are some differences to consider when prompting a [reasoning model](/docs/guides/reasoning) versus prompting a GPT model. Generally speaking, reasoning models will provide better results on tasks with only high-level guidance. This differs from GPT models, which benefit from very precise instructions.

You could think about the difference between reasoning and GPT models like this.

*   A reasoning model is like a senior co-worker. You can give them a goal to achieve and trust them to work out the details.
*   A GPT model is like a junior coworker. They'll perform best with explicit instructions to create a specific output.

For more information on best practices when using reasoning models, [refer to this guide](/docs/guides/reasoning-best-practices).

Next steps
----------

Now that you known the basics of text inputs and outputs, you might want to check out one of these resources next.

[

Build a prompt in the Playground

Use the Playground to develop and iterate on prompts.

](/playground)[

Generate JSON data with Structured Outputs

Ensure JSON data emitted from a model conforms to a JSON schema.

](/docs/guides/structured-outputs)[

Full API reference

Check out all the options for text generation in the API reference.

](/docs/api-reference/responses)

----------------------------------------------

2ê°œì˜ IP ë¹„êµ ë¶„ì„ prompt êµ¬ì¡°ë„
```
---
config:
  layout: elk
---
flowchart LR
    A["Prompt A both IPs: Top-level Analysis"] --> B{"Prompt B (IP1 Submodules)\nIterative Chunking/Analysis"}
    B --> B & C{"Prompt B (IP2 Submodules)\nIterative Chunking/Analysis"} & D["Collect summary_IP1"]
    C --> C & E["Collect summary_IP2"]
    D --> F["Prompt C: Compare IPs"]
    E --> F
    F --> G["Generate: \n- Comparative Summary\n- Diff-focused Dataflow Diagram\n- Highlight structural/power diff"]

```

IP ë¶„ì„ Prompt êµ¬ì¡°ë„
```
flowchart LR
  A[Prompt A: Top Module ë¶„ì„] --> B{"Prompt B: Submodule ë¶„ì„ ë°˜ë³µ"}
  B --> C[Prompt B: ë‹¤ìŒ Submodule]
  C --> B
  B --> D[Prompt C: ìµœì¢… IP Dataflow ì‹œê°í™”]

```

Hierarchical chunking ê°œë…ë„
```
flowchart LR
  A[Full RTL/IP File] --> B["Split into chunks"]
  B --> C["LLM input: Chunk 1"]
  B --> D["LLM input: Chunk 2"]
  B --> E["LLM input: Chunk N"]
  
  C --> F["LLM outputs summary for Chunk 1"]
  D --> G["LLM outputs summary for Chunk 2"]
  E --> H["LLM outputs summary for Chunk N"]
  
  F --> I["Collect all chunk summaries"]
  G --> I
  H --> I
  
  I --> J["Merge summaries into global IP summary"]
  J --> K["Final dataflow analysis + visualization"]

```


ì „ì²´ ê°œë…ë„
```
flowchart LR
  A[RTL Input] --> B[ğŸ§  Prompt Design]
  B --> C[LLM: Execute Prompt + RTL]
  C --> D[LLM: Dataflow Analysis]
  D --> E[Output: IP Summary + Visualized Dataflow]

  subgraph PromptDesign ["Prompt ì„¤ê³„ ë‹¨ê³„"]
    direction TB
    B1[Define Goal: Understand IP dataflow]
    B2[Design Prompt A: Topâ€‘level analysis]
    B3[Design Prompt B: Submodule iterative analysis]
    B4[Design Prompt C: Final summary & visualization]
    B1 --> B2 --> B3 --> B4
  end

  style PromptDesign fill:#f0faff,stroke:#0088cc,stroke-width:2px
```


IP ë¶„ì„ê³¼ì • ì •ë¦¬
```
flowchart TD
  subgraph Stage_A ["Prompt A: Top Module ë¶„ì„"]
    A1[Input: Top module chunk(s)]
    A2{Chunks > 1?}
    A3[Loop: summarize each chunk]
    A4[Combine chunk summaries â†’ summary_top.md]
    A5[Generate checklist of submodules]
  end

  subgraph Stage_B ["Prompt B: Submodule ë¶„ì„"]
    B1[Input: Selected submodule chunk(s) + checklist.md]
    B2{Chunks > 1?}
    B3[Loop: summarize each chunk]
    B4[Combine into summary_sub.md]
    B5[Update checklist: Done + new Pending entries]
  end

  subgraph Stage_C ["Prompt C: ìµœì¢… IP Dataflow ë¶„ì„"]
    C1[Input: maybe empty checklist.md or all summary_*.md]
    C2[Generate final hierarchical summary]
    C3[Render Mermaid dataflow diagram]
    C4[Highlight data/control paths + optimizations]
  end

  %% Flow arrows
  A1 --> A2
  A2 -- Yes --> A3
  A2 -- No --> A4
  A3 --> A4
  A4 --> A5
  A5 --> B1

  B1 --> B2
  B2 -- Yes --> B3
  B2 -- No --> B4
  B3 --> B4
  B4 --> B5
  B5 --> B1
  B5 -- all Done --> C1

  C1 --> C2 --> C3 --> C4

```


Data flow ì¢…í•©í•˜ëŠ” prompt
```
<role>
You are a senior hardware verification engineer specializing in hierarchical RTL dataflow analysis.
</role>

<goal>
Generate a final summary of the complete IP dataflow and module hierarchyâ€”after all submodules are processed.
</goal>

<task>
Using either:
- An empty checklist.md file (indicating all modules are done), or
- A list of summary_&lt;module&gt;.md files for all submodules,

perform the following:
1. Confirm that all submodules have been analyzed.
2. Produce a final hierarchical dataflow summary for the top module.
3. Create a concise Mermaid diagram showing up to 3 hierarchy levels.
4. Highlight main data/control flows and any identified power-saving techniques.
</task>

<input>
- checklist.md (may be empty)  
- OR summary_&lt;module&gt;.md files  
- (Optional) power_techniques.md
</input>

<constraints>
- Proceed only if checklist is empty (all done) or summaries cover all modules.  
- Mermaid diagram must be maximum 3 levels deep.  
- Final summary should clearly integrate power-saving insights.
</constraints>

<think>
1. Check that checklist is empty or all summaries are present.  
2. Merge individual summaries into a cohesive overview.  
3. Construct a Mermaid flowchart, escaping XML characters.  
4. Annotate power-saving techniques per module.  
5. Produce final structured summary with diagram.
</think>

<format>
**Final Hierarchical Dataflow Summary**  
- **TopModule**: describe core function and overall data/control flow  
  - **childA**: role, key signals, any power-saving notes  
  - **childB**: role, â€¦  
    - **childB1**: â€¦

**Mermaid Diagram**  
'''mermaid
flowchart TD
  TopModule[TopModule]
  childA[childA]
  childB[childB]
  childB1[childB1]

  TopModule --&gt; childA
  TopModule --&gt; childB
  childB --&gt; childB1
'''
Key Insights
	â€¢	Dataflow: Inputs â†’ processing â†’ outputs
	â€¢	Control: FSMs, enables, handshakes
	â€¢	Power-saving: Techniques implemented per module
	â€¢	Verification: Confirm all modules are included
</format>

<answer>
</answer>

```



RTL file sizeê°€ 40KBì´ìƒì¼ ë•Œ ë¶„í• í•˜ëŠ” powershell script (window)
```powershell
param(
    [Parameter(Mandatory)]
    [string]$InputFile,
    [int]$MaxBytes = 40960,
    [double]$OverlapRatio = 0.1
)

# 1. ì›ë³¸ íŒŒì¼ ë¼ì¸ ë¡œë“œ
$lines = Get-Content -Path $InputFile -Raw -Encoding UTF8 -ErrorAction Stop | 
         ForEach-Object { $_ -split "`r?`n" }

# 2. íŒŒì¼ í¬ê¸° í™•ì¸
$info = Get-Item $InputFile
$totalBytes = $info.Length
if ($totalBytes -le $MaxBytes) {
    Copy-Item -Path $InputFile -Destination ("{0}_pt1.v" -f ($info.BaseName))
    Write-Host "File size within limit; copied as single chunk."
    return
}

# 3. í‰ê·  ë¼ì¸ ë°”ì´íŠ¸ ê³„ì‚°
$avgBytes = [math]::Max(1, [math]::Floor($totalBytes / $lines.Count))

# 4. ì²­í¬ ë° ì˜¤ë²„ë© ê³„ì‚°
$linesPerChunk = [math]::Max(1, [math]::Floor($MaxBytes / $avgBytes))
$overlap = [math]::Max(1, [math]::Floor($linesPerChunk * $OverlapRatio))
$step = $linesPerChunk - $overlap
$totalChunks = [math]::Ceiling(($lines.Count - $overlap) / $step)

# 5. ì²­í¬ ìƒì„± & ì €ì¥
for ($i = 0; $i -lt $totalChunks; $i++) {
    $start = $i * $step
    $end = [math]::Min($start + $linesPerChunk, $lines.Count)
    $chunkLines = $lines[$start..($end - 1)]
    $outName = "{0}_pt{1}.v" -f $info.BaseName, ($i + 1)
    $header = "// Chunk {0} of {1} from {2}`r`n" -f ($i + 1), $totalChunks, $info.BaseName
    $header | Out-File -FilePath $outName -Encoding UTF8
    $chunkLines | Out-File -FilePath $outName -Encoding UTF8 -Append
    Write-Host "Saved chunk $outName"
}
```

=========
IP Analysis for a large RTL file

ì•„ë˜ëŠ” Prompt Aì™€ Bë¥¼ ëŒ€ìš©ëŸ‰ submoduleì„ ê³ ë ¤í•˜ì—¬ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤. Chunking ì „ëµê³¼ ë©”íƒ€ë°ì´í„° í¬í•¨ ë°©ì‹ì„ ì ìš©í–ˆê³ , ê° ë‹¨ê³„ì—ì„œ íŒŒì¼ ì €ì¥ ì§€ì¹¨ë„ ëª…ì‹œí–ˆìŠµë‹ˆë‹¤. ì›¹ ìë£Œë¥¼ í† ëŒ€ë¡œ chunkâ€‘overlap ë°©ë²•ë„ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤().

â¸»

ğŸ”¹ Prompt A (Top/current module ë¶„ì„ + ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±)
```
<role>
You are a senior hardware verification engineer specializing in hierarchical RTL analysis.
</role>

<goal>
Analyze the provided RTL chunk (which may be the full module or a chunk if too large) and create a submodule checklist.
</goal>

<task>
1. If the input chunk is part of a larger module, treat it as â€œChunk N of Mâ€ and detect continuity.
2. Summarize its dataflow and control behavior.
3. Detect instantiated submodules in this chunk.
4. Generate/update checklist entries with statuses.
5. Specify files to save summary and updated checklist.
6. If more chunks remain, ask for the next chunk; otherwise ask which submodule to inspect next.
</task>

<input>
- RTL chunk labeled as â€œChunk N of M for <module>â€  
- (Optional) Existing checklist file
</input>

<constraints>
- If the module is split across chunks, ensure 10â€“20% overlap per best practice.  
- Auto-identify control signals.  
- Keep analysis concise.
</constraints>

<think>
1. Recognize chunk metadata (N/M, overlap).  
2. Summarize chunk-level behavior, inclusive of module-level context.  
3. Add any submodules to checklist as â€œPendingâ€.  
4. Save/update:
   - `summary_<module>.md` (append or accumulate chunk summary)
   - `checklist.md`
5. If N < M, ask for Chunk N+1; else ask user to select next submodule.
</think>

<format>
**âœ“ Module Chunk**: <module>, Chunk N of M  
**Chunk Summary (partial)**  
- â€¦

**Detected Submodules (so far)**  
| Submodule | Role | Status |
|---|---|---|
| childA | â€¦ | Pending |
| childB | â€¦ | Pending |

**Files to Save**  
- `summary_<module>.md` (append this chunkâ€™s summary)  
- `checklist.md` (update statuses)

**Next Step**  
<If N < M>  
â€œPlease provide Chunk N+1 for <module>.â€  
<Else>  
â€œPlease choose a Pending submodule and provide its RTL.â€>
</format>

<answer>
</answer>
```

â¸»

ğŸ”¹ Prompt B (Submodule ë¶„ì„ + ì²´í¬ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸)
```
<role>
You are a senior hardware verification engineer specializing in hierarchical RTL analysis.
</role>

<goal>
Process the given RTL chunk (or full submodule) and update the checklist accordingly.
</goal>

<task>
1. Detect if this is part of a multiâ€‘chunk submodule; handle overlap.
2. Summarize dataflow/control for the chunk.
3. Detect further sub-submodules.
4. Update checklist: mark this as Done, add new modules as Pending.
5. Specify files to save summary and updated checklist.
6. If more chunks remain, prompt for next chunk; otherwise ask for next submodule.
</task>

<â€¯input>
- RTL chunk labeled â€œChunk N of M for <submodule>â€  
- `checklist.md`
</input>

<constraints>
- Use 10â€“20% overlap between chunks.  
- Keep summaries clear and concise.  
- If no submodules found, mark â€œchildlessâ€.
</constraints>

<think>
1. Interpret chunk metadata.  
2. Summarize this chunkâ€™s behavior.  
3. Append to `summary_<submodule>.md`.  
4. Update `checklist.md`: mark current as Done, append new child modules as Pending.  
5. If chunks left, ask for Chunk N+1; else ask for selection of next Pending submodule or type â€œnoneâ€.
</think>

<format>
**Processing Chunk**: <submodule>, Chunk N of M  
**Chunk Summary (partial)**  
- â€¦

**Updated Checklist**  
| Submodule | Role | Status |
|-----------|------|--------|
| <this submodule> | â€¦ | Done |
| childX | â€¦ | Pending |
| childY | â€¦ | Pending |

**Files to Save**  
- `summary_<submodule>.md`  
- `checklist.md`

**Next Step**  
<If N < M>  
â€œPlease provide Chunk N+1 for <submodule>.â€  
<Else>  
â€œPlease choose a Pending submodule or type â€˜noneâ€™ if finished.â€>
</format>

<answer>
</answer>
```

â¸»

âœ… ìš”ì•½
	â€¢	Chunk with overlap: large files are split, with overlaps to preserve continuity ï¿¼
	â€¢	Meta metadata: chunk indices (N/M) ensure correct ordering
	â€¢	Checklist-driven: hierarchy exploration is tracked interactively
	â€¢	File outputs: summaries and checklist are externalized per chunk/module
	â€¢	Smooth flow: user guided stepâ€‘byâ€‘step until full coverage






â€œTop ëª¨ë“ˆë„ ì‚¬ì‹¤ ë” í° IP ë‚´ë¶€ì˜ submoduleì´ë©°, hierarchical êµ¬ì¡°ë¥¼ ëª¨ë¥´ëŠ” ìƒíƒœì—ì„œ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ë ¤ í•œë‹¤. ì´ ê³¼ì •ì—ì„œ LLMì´ ë§¤ ë‹¨ê³„ë§ˆë‹¤ summaryë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , ë‹¤ìŒì— ì–´ë–¤ submoduleì„ ë¶„ì„í• ì§€ ì œì•ˆ ë° checklist í˜•íƒœë¡œ ê´€ë¦¬í•´ì•¼ í•œë‹¤.â€

ì´ êµ¬ì¡°ëŠ” hierarchical chunking + interactive checklist íë¦„ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, LLMì´ í˜„ì¬ ìƒíƒœë¥¼ ìš”ì•½í•˜ê³  ë‹¤ìŒ í›„ë³´ë¥¼ ì¶”ì²œí•˜ë©° checklistë¥¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ì„¤ê³„í•˜ë©´ ì•ˆì •ì„±ê³¼ ê°€ì‹œì„±ì„ ë™ì‹œì— í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ().

â¸»

âœ… ì œì•ˆ í”„ë¡¬í”„íŠ¸: Prompt A & B (checklist í¬í•¨)

ğŸ”¹ Prompt A: Top ëª¨ë“ˆ ë¶„ì„ + ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±
```
<role>
You are a senior hardware verification engineer specializing in hierarchical RTL analysis.
</role>

<goal>
Analyze the provided RTL chunk (top module) and produce a checklist of submodules for next-level inspection.
</goal>

<task>
Given the RTL chunk for the current module:
1. Summarize its dataflow and control structure.
2. Detect instantiated submodules.
3. Create a checklist table with:
   â€¢ Submodule name
   â€¢ Inferred role/purpose
   â€¢ Status = â€œPendingâ€
4. Ask the user to provide the RTL chunk of the chosen submodule.
5. Indicate clearly what outputs should be saved to file: â€œsummaryâ€ and â€œchecklistâ€ (in Markdown).
</task>

<input>
- RTL chunk for current module (initially top)
- (Optional) Existing checklist file
</input>

<constraints>
- Do not assume known key signals; derive them from RTL.
- Only functional/control-level analysis.
- Keep summaries concise and list checklist cleanly.
</constraints>

<think>
1. Parse RTL chunk to extract I/O, internal signals, control paths.
2. Trace data movement and control mechanisms.
3. List first-level instantiated submodules.
4. Create checklist table.
5. Specify exactly which outputs to save as files.
6. Prompt user for next RTL chunk.
</think>

<format>
**Module Summary**  
- â€¦ (bullet)

**Detected Submodules & Checklist**

| Submodule | Role | Status |
|----------|------|--------|
| childA   | â€¦    | Pending |
| childB   | â€¦    | Pending |

**Files to Save**  
- `summary_top.md`: contains â€œModule Summaryâ€  
- `checklist_top.md`: contains the checklist table

**Next Step**  
Please select one submodule and provide its RTL chunk.
</format>

<answer>
</answer>
```

â¸»

ğŸ”¹ Prompt B: Submodule ë¶„ì„ + Checklist ì—…ë°ì´íŠ¸
```
<role>
You are a senior hardware verification engineer specializing in hierarchical RTL analysis.
</role>

<goal>
Analyze the selected submodule RTL chunk and update the checklist accordingly.
</goal>

<task>
Using the provided RTL chunk and the existing checklist file:
1. Summarize the submoduleâ€™s dataflow and control behaviors.
2. Detect any lower-level submodules.
3. Update checklist:
   â€¢ Mark current submodule as â€œDoneâ€
   â€¢ Add new sub-submodules as â€œPendingâ€
4. Indicate which files to update: submodule summary and updated checklist.
5. Prompt for the next submodule RTL or signal completion.
</task>

<input>
- `checklist_top.md` file
- RTL chunk for selected submodule
</input>

<constraints>
- Derive key signals automatically.
- Keep bullet summaries and clean table updates.
- If no submodules remain, state â€œChecklist complete â€” ready for final summary.â€
</constraints>

<think>
1. Load existing checklist.  
2. Analyze current RTL chunk as in Prompt A.  
3. Identify instantiated sub-submodules.  
4. Mark current as Done; append new Pending entries.  
5. Specify file updates.  
6. Ask next RTL or signal complete.
</think>

<format>
**Submodule Summary: childA**  
- â€¦ (bullet)

**Updated Checklist**

| Submodule | Role | Status |
|----------|------|--------|
| childA   | â€¦    | Done   |
| childB   | â€¦    | Pending |
| childA1  | â€¦    | Pending |

**Files to Save**  
- `summary_childA.md`
- `checklist_top.md` (updated)

**Next Step**  
Please select a Pending submodule and provide its RTL chunk, or type â€œnoneâ€ to finalize.
</format>

<answer>
</answer>

```

â¸»

ğŸ“š Why this works
	â€¢	Hierarchical chunking: ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ë¶„ì„í•˜ê³ , context overflowë¥¼ ì˜ˆë°©í•©ë‹ˆë‹¤  ï¿¼ ï¿¼.
	â€¢	Interactive checklist: LLMì´ ì§„í–‰ ê³¼ì •ê³¼ ë‚¨ì€ ë¶„ì„ í•­ëª©ì„ ëª…í™•íˆ ì œì‹œí•˜ë©°, ì‚¬ìš©ìê°€ ì§ì ‘ ì‘ì—… íë¦„ì„ ì¡°ìœ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	â€¢	ì§€ì†ì  ìš”ì•½ & ì—…ë°ì´íŠ¸: ê° ë‹¨ê³„ì—ì„œ summaryë¥¼ ëˆ„ì í•˜ê³ , checklistë¥¼ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€í•¨ìœ¼ë¡œì¨ ì „ì²´ IP ë¶„ì„ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	â€¢	ìì—°ìŠ¤ëŸ¬ìš´ í˜‘ì—… íë¦„: LLMì´ ì§ˆë¬¸í•˜ê³  ì‚¬ìš©ìê°€ ë‹µí•¨ìœ¼ë¡œì¨ ë‹¨ê³„ë³„ ë¶„ì„ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§‘ë‹ˆë‹¤.

â¸»

ì´ êµ¬ì¡°ë¥¼ í†µí•´ IP ëª¨ë“ˆ êµ¬ì¡°ë¥¼ ëª¨ë¥´ë”ë¼ë„, Top-levelë¶€í„° Submoduleê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í•˜ë©´ì„œ ìš”ì•½í•˜ê³  ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì›í•˜ì‹œë©´ Prompt C/D ë“± ë‹¤ìŒ ë‹¨ê³„ ë²„ì „ìœ¼ë¡œë„ í™•ì¥ ì„¤ê³„ ê°€ëŠ¥í•´ìš”! ğŸ˜Š



Final Step
```
<role>
You are a senior RTL power optimization engineer skilled in structural analysis and technique inference.
</role>

<task>
Before analyzing RTL differences, confirm whether the Baseline and Revised RTL source files are still present in your current context. If they remain accessible, proceed to detect power-saving RTL changes for each module in the Power Delta Table. If not, request reattachment.
</task>

<input>
- Power Delta Table: modules with Baseline & Revised power values  
- (Optional) Baseline and Revised RTL source files  
</input>

<constraints>
- If RTL files are missing from context, ask:  
  â€œIt appears I canâ€™t access the RTL source files for Baseline and/or Revised IP. Could you please reattach them?â€  
- Once confirmed, analyze only modules from the Power Delta Table.
- Detect:
    * Clock gating (`*_en` signals)
    * Operand isolation
    * FSM encoding changes
    * Pipeline register modifications
    * Logic/mux simplification
- If no change is detected, annotate â€œNo structural change detected.â€
- Provide for each:
    * RTL change or â€œNo changeâ€
    * Technique or â€œâ€“â€
    * Code snippet (if change)
    * Explanation linking structure to power delta
</constraints>

<think>
1. Check if RTL source files are still in context.  
   - If not present, ask the user to reattach and stop.  
2. For each module in the Power Delta Table:
   a. Locate corresponding Baseline & Revised RTL.  
   b. Scan for structural changes matching listed low-power patterns.  
   c. If found, extract snippet, identify technique, explain impact.  
   d. If none, mark â€œNo structural change detected.â€  
3. Compile structured results for all modules.
</think>

<format>
"""plaintext
Module: core/accumulator
RTL Change: Added `acc_en` gating
Snippet:
"""verilog
always_ff @(posedge clk) if (acc_en) acc_reg <= acc_in;

```

CSV Power data analysis
Baseline CSV reading
```
<role>
You are a senior RTL power optimization engineer specialized in hotspot extraction.
</role>

<task>
Extract the high-power modules under a given subtree from the Baseline CSV:
- Filter modules whose path starts with the subtree path.
- Normalize module names by stripping leading prefixes (keep suffix after subtree).
- Sort by Total power descending.
- Return top 10 modules.
</task>

<input>
- Baseline CSV: `[Attach baseline_power.csv]`  
- Subtree path: `[e.g., top/core/processing_block]`
</input>

<constraints>
- Request missing inputs immediately.
- Output must include a Markdown table:
  | Rank | Module (normalized path) | Total (mW) | Dyn (%) | Clk (%) |
</constraints>

<think>
1. Ensure both inputs are present.  
2. Read Baseline CSV, filter rows matching subtree.  
3. Normalize module names: remove prefix up to subtree.  
4. Sort by Total descending.  
5. Present first 10 entries.
</think>

<format>
**Baseline Topâ€‘10 Modules**

| Rank | Module | Total (mW) | Dyn (%) | Clk (%) |
|:----:|--------|-----------:|--------:|--------:|
</format>

<answer>
</answer>

```

Revised CSV reading

```
<role>
You are a senior RTL power optimization engineer specialized in differential analysis.
</role>

<task>
For the Topâ€‘10 modules from Promptâ€¯1, compute power deltas using Revised CSV:
- Filter Revised CSV under the same subtree.
- Normalize names like before.
- Align module names using suffix match or fuzzy matching (e.g., Levenshtein).
- Compute Î”Total, Î”Dyn, Î”Clk (mW & %).
- Sort by Î”Total descending, exclude parent-child overlaps, select top 5.
</task>

<input>
- Baseline Topâ€‘10 table (from Promptâ€¯1)  
- Revised CSV: `[Attach revised_power.csv]`
</input>

<constraints>
- Prompt if Revised CSV is missing.
- Match modules using suffix-match or fuzzy matching when names differ slightly.  
- Exclude overlapping module paths (parent-child).  
- Tie-break selection using Î”Dyn%.  
- Output a Markdown table and brief summary.
</constraints>

<think>
1. Confirm Revised CSV input.  
2. Load Revised CSV, filter by subtree path.  
3. Normalize module names.  
4. Align modules:
   a. If normalized names identical, pair directly.  
   b. Else use fuzzy matching (e.g., Levenshtein >= threshold) .  
5. For each match, compute Baseline/Revised absolute, Î”Total (mW, %), Î”Dyn, Î”Clk.  
6. Sort by Î”Total descending.  
7. Exclude parent-child duplicates, pick top 5.  
8. Format results.
</think>

<format>
**Power Delta Table (Topâ€‘5)**

| Rank | Module | Base (mW) | Rev (mW) | Î”Total (mW,â€¯%) | Î”Dyn (%) | Î”Clk (%) |
|:----:|--------|----------:|---------:|----------------:|----------:|----------:|

**Summary**

- **moduleX**: baseline Aâ€¯mW â†’ revised Bâ€¯mW (Î” = Câ€¯mW, Dâ€¯%), Î”Dyn = Eâ€¯%, Î”Clk = Fâ€¯%.
</format>

<answer>
</answer>
```



IP dataflow analysis
```
<role>
You are a senior hardware verification engineer specializing in RTL architecture analysis.
</role>

<goal>
Understand and visualize the dataflow of the given RTL in order to grasp its functional behavior and internal operation sequence.
</goal>

<task>
Based on the provided RTL module descriptions, hierarchy, and signal definitions, do the following:
1. Analyze the RTL to describe the dataflow and control signal interactions.
2. Identify key modules, input/output signal paths, and internal signal transformations.
3. Determine how modules exchange data and how control signals affect data movement.
4. Generate a Mermaid flowchart that visualizes the module-level dataflow.
</task>

<input>
- Top module name: [e.g., core_top]
- RTL hierarchy: [summarized or full module list]
- Key signals: [clock/reset, input/output ports, internal buses if available]
</input>

<constraints>
- Do not assume implementation technology (ASIC/FPGA).
- Do not include physical layout assumptions.
- Focus only on functional-level RTL behavior and structural data/control signal relationships.
</constraints>

<think>
1. Identify data producers and consumers: ALUs, FSMs, buffers, memory blocks.
2. Trace data movement from inputs through intermediate logic to outputs.
3. Highlight gating conditions, FSM states, enable signals, or handshake protocols.
4. Represent this information as a flowchart using Mermaid syntax.
</think>

<format>
Return your findings in four parts:

1. **High-level Dataflow Summary**
   - Use bullet points to describe the overall flow of data and control.

2. **Module Interaction Graph**
   - Indented hierarchy or directional relationships showing how modules pass data.

3. **Control Mechanisms**
   - Bullet list of key control signals, FSM triggers, enable/ready signals, etc.

4. **Mermaid Diagram**
'''mermaid
flowchart TD
    input1[Input: data_in] --> moduleA[Module: Decoder]
    moduleA --> moduleB[Module: ALU]
    moduleB --> moduleC[Module: Accumulator]
    moduleC --> output1[Output: data_out]

    clk[Clock] --> moduleB
    rst[Reset] --> moduleA
    ctrl[Control FSM] --> moduleA
    ctrl --> moduleB
'''
```

Power Reduction Scheme
```
<role>
You are a senior hardware reverse-engineering and power optimization analyst, skilled at integrating structural and quantitative data.
</role>

<goal>
Compare Baseline and Revised IPs to understand how power reduction was achieved, based on available RTL and power data.
</goal>

<task>
You will receive:
- Baseline and Revised RTL summaries files.
- Baseline and Revised PowerArtist CSV files.

Your tasks:
1. Ensure all necessary inputs are provided.
2. Parse provided files to align modules and compute module-level power deltas.
3. Correlate structural changes in RTL with power reductions.
4. Identify top-5 modules by total power reduction, avoiding parentâ€“child duplicates.
5. Visualize results via Mermaid bar chart.
</task>

<input>
- Baseline RTL summary: `[Attach baseline_rtl.txt]`  
- Revised RTL summary:  `[Attach revised_rtl.txt]`  
- Baseline power CSV:  `[Attach baseline_power.csv]`  
- Revised power CSV:   `[Attach revised_power.csv]`
</input>

<constraints>
- **If any required input is missing** (e.g., one of the files is not attached), respond **only** with a concise clarifying request specifying exactly what's missing, e.g., â€œPlease provide the Revised RTL summary file.â€ Do not proceed with analysis until all inputs are present.
- Only analyze modules present in all four inputs.
- Select exactly 5 modules with largest total power reduction; exclude parentâ€“child duplication; tie-break by dynamic power reduction.
- For each selected module, report Î”Total (mW/%), Î”Dyn (%), Î”Clk (%), structural change note, and inferred technique.
- Output must include:
  1. Markdown table  
  2. Text summary  
  3. Mermaid bar chart code snippet
</constraints>

<think>
1. Check for presence of all four inputs; if any missing â†’ ask user.
2. If complete, load CSVs and RTL summaries.
3. Align modules, compute power deltas.
4. Sort, exclude parent/child, pick top-5.
5. Map structural changes and infer techniques.
6. Build Mermaid bar chart.
</think>

<format>
1. **Markdown table**:

| Rank | Module | Î”Total (mW,â€¯%) | Î”Dynâ€¯(%) | Î”Clkâ€¯(%) | Structural Change | Inferred Technique |
|------|--------|------------------|-----------|-----------|--------------------|--------------------|

2. **Text Summary**  
- **ModuleX**: narrative explanation...

3. **Mermaid bar chart**:

```mermaid
barChart
    title Power Reduction by Module
    x-axis "Module"
    y-axis "Î” Total Power (mW)"
    "mod1": value1
    "mod2": value2
    ...
</format>
```
<answer>
</answer>

IP Comparison ver2 (RTL summary + RTL files)
```
<role>You are a senior hardware reverse-engineer specializing in RTL dataflow and low-power architecture.</role>

<goal>
Understand and compare the dataflow and structure of Baseline IP and Revised IP, using both RTL summary files and actual RTL code, to prepare for later power-optimization analysis.
</goal>

<task>
Using the provided RTL summaries and RTL source files for both IPs:
1. Analyze the dataflow and control logic of each IP.
2. Highlight differences in datapaths, control signals, pipeline stages, muxing, buffering, and FSM behavior.
3. Validate summary descriptions against actual RTL implementation.
4. Identify structural or functional differences that may lead to power savings (e.g., added clock gating, logic pruning, operand isolation, FSM encoding change).
</task>

<input>
- Baseline RTL summary: `[Attach baseline_dataflow.txt]`
- Revised RTL summary: `[Attach revised_dataflow.txt]`
- Baseline RTL files: `[Attach all Baseline RTL files (e.g., .v/.sv)]`
- Revised RTL files: `[Attach all Revised RTL files]`
</input>

<constraints>
- Do not use power report data at this stage.
- Use RTL summaries for overview and RTL source for verification and structural detail.
- Identify all functionally meaningful changes, including subtle RTL differences.
- Be precise, especially regarding pipeline depth, register enable signals, clock gating, FSM state transitions.
- Use concise formatting: bullet points or tables preferred.
</constraints>

<think>
1. Parse both summaries: identify modules and control/data paths.
2. Use RTL source to verify summaries, and extract additional structural differences if summaries are vague or missing.
3. Compare matched modules across Baseline and Revised IP.
4. Focus on differences likely to affect activity toggles or enable future power optimization.
5. If RTL summary and source disagree, trust the RTL file.
</think>

<format>
Return the following three sections:

1. **Baseline IP Dataflow Summary**  
   - Bullet list overview of modules, control flow, key datapaths.

2. **Revised IP Dataflow Summary**  
   - Same format as above for the Revised version.

3. **Detected Differences**  
   | Module/Submodule | Change Description                        | Likely Effect on Power |
   |------------------|--------------------------------------------|------------------------|
   | core/fsm         | FSM encoded as one-hot â†’ binary (verified in fsm.v) | Fewer flops, lower toggle |
   | core/acc         | Clock gating via acc_en added (seen in acc.v)     | Dynamic + clock power reduced |

<answer>
</answer>
```

IP Comparison ver1 (RTL summary only)
```
<role>You are a senior hardware reverseâ€‘engineer specializing in RTL dataflow and lowâ€‘power architecture.</role>

<goal>
Understand and compare the dataflow of Baseline IP and Revised IP to prepare for powerâ€‘optimization analysis.
</goal>

<task>
Using the provided RTL summary files for both IPs, do the following:
1. Analyze and summarize the dataflow of each IP.
2. Highlight differences in data paths, control signals, pipeline stages, and buffering between Baseline and Revised versions.
3. Point out structural or functional changes that may lead to power savings (e.g., added clock gating, pipeline reordering, operand isolation).
Note: Power data will be analyzed later; for now, focus on functional/dataflow comparison.
</task>

<input>
- Baseline IP RTL summary: `[Attach baseline_dataflow.txt here]`
- Revised IP RTL summary:  `[Attach revised_dataflow.txt here]`
</input>

<constraints>
- Do **not** reference or use power report data at this stage.
- Focus exclusively on RTL dataflow and control structure differences.
- Identify all notable changes, even if subtle (e.g., signal gating, redundant mux removal).
- Maintain clarity and brevity; use bullet lists or tables.
</constraints>

<think>
1. Parse both RTL summaries: extract modules, datapaths, control signals.
2. Compare corresponding modules/submodules.
3. Note added/removed pipeline registers, gating, bus changes.
4. Map differences that likely impact switching activity.
</think>

<format>
Return three sections:
1. **Baseline Dataflow Summary** â€“ concise bullet overview.
2. **Revised Dataflow Summary** â€“ concise bullet overview.
3. **Differences** â€“ table comparing:
   | Module/Submodule | Change Description | Likely Effect on Power |
   |------------------|--------------------|--------------------------|
</format>

<answer>
</answer>
```

```
<role>You are a senior lowâ€‘power RTL design analyst with deep expertise in PowerArtist report interpretation and RTL structural comparison.</role>

<goal>
Analyze and compare the Baseline IP and New IP to identify which power reduction schemes were applied, using both RTL structure and measured power data.
</goal>

<task>
You will be given two CSV files (Baseline and New IP Power reports) and two RTL summary files. Perform the following:
1. Parse each CSVâ€”fields: Module, Total(mW), Dynamic(mW), Clock(mW), Leakage(mW).
2. Parse each RTL summary fileâ€”listing modules, their control/data flow, and noted structural differences.
3. Align modules present in both IP versions.
4. For each aligned module, compute power deltas (absolute & percentage) for total, dynamic, and clock.
5. Cross-reference structural changes from RTL summaries.
6. Infer and list likely power reduction techniques applied (e.g., clock gating, operand isolation, FSM encoding).
</task>

<input>
- Baseline Power report:  `[Attach baseline_power.csv here]`
- New IP Power report:      `[Attach new_power.csv here]`
- Baseline RTL summary:     `[Attach baseline_rtl.txt here]`
- New IP RTL summary:      `[Attach new_rtl.txt here]`
</input>

<constraints>
- Only analyze modules present in **both** CSVs and summaries.
- Report only modules with any total power reduction â‰¥10â€¯%.
- Exactly list **5 modules** with the largest power gains.
- Tie-break: prefer modules with largest dynamicâ€‘power drops.
- Must link each inferred scheme to **both** power drop and documented structural change.
- Output must be in **Markdown table** format.
</constraints>

<think>
1. Load CSVs and summaries.
2. Identify common modules.
3. Compute Î” total, dyn, clk power and percentage reductions.
4. Sort by total power reduction descending.
5. Take topâ€‘5 modules, resolving ties by dyn drop.
6. For each, map structural note (e.g. â€œclock gate addedâ€, â€œFSM re-encodedâ€) â†’ known low-power technique.
</think>

<format>
Markdown table with columns:

| Rank | Module | Î” Total (mW, %) | Î” Dyn (%) | Î” Clk (%) | Structural Change | Inferred Technique |
|------|--------|------------------|-----------|-----------|--------------------|--------------------|
| 1    | core/acc | â€“4.2â€¯mW (â€“57%) | â€“76%      | â€“36%      | Clock gating added to accumulator register | Clock gating + operand isolation |
| 2    | core/fsm | â€“2.1â€¯mW (â€“30%) | â€“10%      | â€“55%      | FSM encoding changed to binary | FSM re-encoding |
| ...  |        |                  |           |           |                    |                    |
</format>

<answer>
</answer>
```

```
<role>You are a senior low-power RTL design analyst.</role>

<goal>
Identify and explain the power reduction scheme(s) applied in the New IP compared to the Baseline IP.
</goal>

<task>
Given summarized RTL structure and power report for both IPs, determine:
- What changes in RTL structure or control logic led to power reductions.
- Which power-saving techniques were applied (e.g., clock gating, FSM re-encoding, pipeline insertion).
</task>

<input>
- Baseline IP summary:
  â€¢ RTL: Topâ†’ALUâ†’Acc
    â€“ FSM: 6-state one-hot, no clock gating
  â€¢ Power: ALU 12.3â€¯mW, Acc 7.4â€¯mW, Ctrl 6.1â€¯mW

- New IP summary:
  â€¢ RTL: Topâ†’ALUâ†’Acc
    â€“ FSM: 3-bit binary, clock gating on Accumulator register via `acc_en`
  â€¢ Power: ALU 10.1â€¯mW, Acc 3.2â€¯mW, Ctrl 5.8â€¯mW
</input>

<constraints>
- Do not speculate beyond provided summaries.
- Identify schemes only if supported by both structural and power differences.
</constraints>

<think>
Compare both IPs module by module. Note power drop and link to structural change. Map each change to known low-power technique.
</think>

<format>
Return a markdown table:

| Module    | Power (Baselineâ†’New) | Structural Change             | Power-Saving Technique       | Notes              |
|-----------|-----------------------|-------------------------------|------------------------------|--------------------|
| ALU       | 12.3 â†’ 10.1â€¯mW        | â€”                             | â€”                            | Minor drop  
| Acc       | 7.4 â†’ 3.2â€¯mW          | Binary FSM, Acc clock gated   | Clock gating, FSM re-encoding | Significant gain  
</format>

<answer>
</answer>
```


```
<role>You are a senior hardware verification engineer specializing in RTL architecture analysis.</role>

<goal>
Understand the dataflow of the given RTL in order to grasp its functional behavior and internal operation sequence.
</goal>

<task>
Based on the provided RTL module descriptions and hierarchy, analyze and describe:
- The direction and transformation of data signals
- The control signals that trigger or gate data movement
- How the modules interact to implement the overall functionality
</task>

<input>
- Top module name: [e.g., core_top]
- RTL hierarchy: [summarized or full module list]
- Key signals: [clock/reset, input/output ports, internal buses if available]
</input>

<constraints>
- Do not make assumptions beyond the provided RTL.
- Do not speculate on implementation technology (ASIC/FPGA).
- Focus only on RTL-level functional interactions and data transformations.
</constraints>

<think>
Start by identifying major data producers (e.g., ALUs, decoders, memory interfaces).
Track how data flows from input ports through intermediate modules to the output.
Note any pipelining, buffering, or feedback mechanisms.
Include control signals (e.g., enables, FSM states) that influence the flow.
</think>

<format>
Return your findings in three parts:
1. High-level dataflow summary (bullet points)
2. Module-level data transfer graph (indented structure or list)
3. Notable control mechanisms affecting flow (if any)
</format>

<answer>
[Your output goes here]
</answer>
```

<role>You are a senior hardware verification engineer specializing in RTL architecture analysis.</role>

<goal>
Understand the dataflow of the given RTL in order to grasp its functional behavior and internal operation sequence.
</goal>

<task>
Based on the provided RTL module descriptions and hierarchy, analyze and describe:
- The direction and transformation of data signals
- The control signals that trigger or gate data movement
- How the modules interact to implement the overall functionality
</task>

<input>
- Top module name: [e.g., core_top]
- RTL hierarchy: [summarized or full module list]
- Key signals: [clock/reset, input/output ports, internal buses if available]
</input>

<constraints>
- Do not make assumptions beyond the provided RTL.
- Do not speculate on implementation technology (ASIC/FPGA).
- Focus only on RTL-level functional interactions and data transformations.
</constraints>

<think>
Start by identifying major data producers (e.g., ALUs, decoders, memory interfaces).
Track how data flows from input ports through intermediate modules to the output.
Note any pipelining, buffering, or feedback mechanisms.
Include control signals (e.g., enables, FSM states) that influence the flow.
</think>

<format>
Return your findings in three parts:
1. High-level dataflow summary (bullet points)
2. Module-level data transfer graph (indented structure or list)
3. Notable control mechanisms affecting flow (if any)
</format>

<answer>
[Your output goes here]
</answer>


---

# MatFormer ë…¼ë¬¸ Abstract í•œì¤„ì”© ë²ˆì—­ ë° ì£¼ìš” ë‚´ìš© ì—°ê²° ì„¤ëª…

## 1. Abstract Line-by-Line ë²ˆì—­

| ì›ë¬¸ (ì˜ë¬¸) | í•œê¸€ ë²ˆì—­ | ë…¼ë¬¸ ì£¼ìš” ë‚´ìš©ê³¼ì˜ ì—°ê²° ì„¤ëª… |
|---|---|---|
| 1. Foundation models are applied in a broad spectrum of settings with different inference constraints, from massive multi-accelerator clusters to resource-constrained standalone mobile devices. | íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ë©€í‹°-ê°€ì†ê¸° í´ëŸ¬ìŠ¤í„°ë¶€í„° ìì›ì´ ì œí•œëœ ë…ë¦½í˜• ëª¨ë°”ì¼ ê¸°ê¸°ê¹Œì§€, ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì¶”ë¡  ì œì•½ ì¡°ê±´ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. | ëŒ€í˜• AI ëª¨ë¸ì´ í´ë¼ìš°ë“œë¿ ì•„ë‹ˆë¼ ëª¨ë°”ì¼Â·ì—£ì§€ í™˜ê²½ ë“± ë‹¤ì–‘í•œ ê³³ì—ì„œ ì‚¬ìš©ë¨ì„ ê°•ì¡°í•˜ë©°, ì‹¤ë¬´ì—ì„œ ìš”êµ¬ë˜ëŠ” ìœ ì—°ì„±ì„ ë…¼ë¬¸ì˜ ì¶œë°œì ìœ¼ë¡œ ì‚¼ìŒ. |
| 2. However, the substantial costs associated with training these models often limit the number of unique model sizes that can be offered. | í•˜ì§€ë§Œ ì´ëŸ¬í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ë“œëŠ” ë§‰ëŒ€í•œ ë¹„ìš© ë•Œë¬¸ì—, ì œê³µí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ í¬ê¸°(ì‚¬ì´ì¦ˆ)ì˜ ì¢…ë¥˜ê°€ ì œí•œë©ë‹ˆë‹¤. | ë‹¤ì–‘í•œ í™˜ê²½ì— ë§ëŠ” ì—¬ëŸ¬ í¬ê¸°ì˜ ëª¨ë¸ì„ ì œê³µí•˜ê¸° ì–´ë µë‹¤ëŠ” í˜„ì‹¤ì  ë¬¸ì œë¥¼ ì§€ì í•¨. |
| 3. Consequently, practitioners are compelled to select a model that may not be optimally aligned with their specific latency and cost requirements. | ê·¸ ê²°ê³¼, ì‹¤ì œ ì‚¬ìš©ìëŠ” ìì‹ ì´ ì›í•˜ëŠ” ì§€ì—° ì‹œê°„ì´ë‚˜ ë¹„ìš©ì— ìµœì í™”ë˜ì§€ ì•Šì€ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ë°–ì— ì—†ìŠµë‹ˆë‹¤. | ì‚¬ìš©ìê°€ í™˜ê²½ì— ë§ëŠ” ìµœì  ëª¨ë¸ì„ ì„ íƒí•˜ê¸° ì–´ë ¤ì›Œ ë¹„íš¨ìœ¨ì´ ë°œìƒí•¨ì„ ì„¤ëª…í•¨. |
| 4. We present MatFormer, a novel Transformer architecture designed to provide elastic inference across diverse deployment constraints. | ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ë°°í¬ ì œì•½ ì¡°ê±´ì—ì„œ íƒ„ë ¥ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ìƒˆë¡œìš´ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì¸ MatFormerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. | ë…¼ë¬¸ì˜ í•µì‹¬ì¸ MatFormer ì•„í‚¤í…ì²˜ê°€ ë“±ì¥, 'íƒ„ë ¥ì  ì¶”ë¡ (elastic inference)'ì´ ì£¼ìš” ëª©í‘œì„ì„ ë°í˜. |
| 5. MatFormer achieves this by incorporating a nested Feed Forward Network (FFN) block structure within a standard Transformer model. | MatFormerëŠ” í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë‚´ì— ì¤‘ì²©ëœ Feed Forward Network(FFN) ë¸”ë¡ êµ¬ì¡°ë¥¼ ë„ì…í•˜ì—¬ ì´ë¥¼ ì‹¤í˜„í•©ë‹ˆë‹¤. | ì¤‘ì²©(nested) FFN ë¸”ë¡ì´ MatFormerì˜ í•µì‹¬ êµ¬ì¡°ì  í˜ì‹ ì„ì„ ëª…ì‹œí•¨. |
| 6. During training, we optimize the parameters of multiple nested FFN blocks with varying sizes, enabling the extraction of hundreds of accurate smaller models without incurring additional computational costs. | í•™ìŠµ ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì¤‘ì²© FFN ë¸”ë¡ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì‹œì— ìµœì í™”í•˜ì—¬, ì¶”ê°€ ì—°ì‚° ë¹„ìš© ì—†ì´ ìˆ˜ë°± ê°œì˜ ì •í™•í•œ ì†Œí˜• ëª¨ë¸ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | í•œ ë²ˆì˜ í•™ìŠµìœ¼ë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆëŠ” êµ¬ì¡°ì  ì¥ì ì´ ì„¤ëª…ë¨. |
| 7. We empirically validate the efficacy of MatFormer across different model classes (decoders and encoders) and modalities (language and vision), demonstrating its potential for real-world deployment. | ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ ìœ í˜•(ë””ì½”ë”, ì¸ì½”ë”)ê³¼ ëª¨ë‹¬ë¦¬í‹°(ì–¸ì–´, ë¹„ì „)ì—ì„œ MatFormerì˜ íš¨ê³¼ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•˜ì—¬, ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. | ì–¸ì–´Â·ë¹„ì „ ë“± ì—¬ëŸ¬ ë¶„ì•¼ì— ì ìš© ê°€ëŠ¥í•˜ë©°, ì‹¤í—˜ì ìœ¼ë¡œ íš¨ê³¼ê°€ ê²€ì¦ë¨ì„ ê°•ì¡°í•¨. |
| 8. We show that a 850M decoder-only MatFormer language model (MatLM) allows us to extract multiple smaller models spanning from 582M to 850M parameters, each exhibiting better validation loss and one-shot downstream evaluations than independently trained counterparts. | 850M íŒŒë¼ë¯¸í„°ì˜ ë””ì½”ë” ì „ìš© MatFormer ì–¸ì–´ ëª¨ë¸(MatLM)ì—ì„œ 582M~850M í¬ê¸°ì˜ ë‹¤ì–‘í•œ ì†Œí˜• ëª¨ë¸ì„ ì¶”ì¶œí•  ìˆ˜ ìˆê³ , ì´ë“¤ ê°ê°ì´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ê²€ì¦ ì†ì‹¤ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. | ì‹¤ì œë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì´ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì¶”ì¶œ ê°€ëŠ¥í•˜ë©°, ì„±ëŠ¥ë„ ë…ë¦½ í•™ìŠµ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•¨. |
| 9. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. | ë˜í•œ, ë²”ìš© MatFormer ê¸°ë°˜ ViT(MatViT) ì¸ì½”ë”ì—ì„œ ì¶”ì¶œí•œ ì†Œí˜• ì¸ì½”ë” ì—­ì‹œ ëŒ€ê·œëª¨ ê²€ìƒ‰ì— ì í•©í•œ ë©”íŠ¸ë¦­ ê³µê°„ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•¨ì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤. | ë¹„ì „(ì´ë¯¸ì§€) ëª¨ë¸ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë©°, ê²€ìƒ‰ ë“± ì‹¤ë¬´ ì‘ìš©ì—ì„œ ì¤‘ìš”í•œ íŠ¹ì„±ì´ ìœ ì§€ë¨ì„ ê°•ì¡°í•¨. |
| 10. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from MatFormer can lead to significant reduction in inference latency. | ë§ˆì§€ë§‰ìœ¼ë¡œ, MatFormerì—ì„œ ì¶”ì¶œí•œ ì •í™•í•˜ê³  ì¼ê´€ëœ ì„œë¸Œëª¨ë¸ì„ í™œìš©í•œ speculative decodingì´ ì¶”ë¡  ì§€ì—° ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. | Mixâ€™nâ€™Match ì„œë¸Œëª¨ë¸ì„ í™œìš©í•œ speculative decodingìœ¼ë¡œ ì¶”ë¡  ì†ë„ê¹Œì§€ ê°œì„ ë¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í•¨. |
| 11. Project website: https://devvrit.github.io/matformer/ | í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸: https://devvrit.github.io/matformer/ | ë…¼ë¬¸ ë° ì½”ë“œ, ì¶”ê°€ ìë£Œ ì œê³µ. |

---

## 2. ë…¼ë¬¸ ì£¼ìš” ë‚´ìš©ê³¼ Abstract ì—°ê²° ìš”ì•½

| ì£¼ìš” ë‚´ìš© | Abstract ì—°ê²° ì„¤ëª… | ì‹¤ì§ˆì  ì˜ë¯¸ |
|---|---|---|
| ë‹¤ì–‘í•œ í™˜ê²½ ëŒ€ì‘ | 1, 2, 3 | ëª¨ë¸ ë°°í¬ í™˜ê²½(í´ë¼ìš°ë“œ~ëª¨ë°”ì¼)ì˜ ì œì•½ì„ ê·¹ë³µ |
| ì¤‘ì²© FFN êµ¬ì¡° | 4, 5, 6 | í•œ ë²ˆì˜ í•™ìŠµìœ¼ë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ì„ ë½‘ì•„ë‚´ëŠ” í•µì‹¬ êµ¬ì¡° |
| ì‹¤í—˜ì  ê²€ì¦ | 7, 8, 9 | ì–¸ì–´Â·ë¹„ì „ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ íš¨ê³¼ ì…ì¦, ì„œë¸Œëª¨ë¸ ì„±ëŠ¥ ìš°ìˆ˜ |
| ì¶”ë¡  íš¨ìœ¨ì„± | 10 | Mixâ€™nâ€™Match, speculative decoding ë“±ìœ¼ë¡œ ì¶”ë¡  ì†ë„ ê°œì„  |
| ì‹¤ë¬´ ì ìš©ì„± | ì „ì²´ | ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œ ìœ ì—°í•˜ê³  íš¨ìœ¨ì ì¸ ì¶”ë¡  ê°€ëŠ¥ì„± ì œì‹œ |

---

## 3. ìµœì¢… ìš”ì•½ í…Œì´ë¸”

| Abstract í•µì‹¬ ë¬¸ì¥ | í•œê¸€ ë²ˆì—­ | ë…¼ë¬¸ ì£¼ìš” ë‚´ìš© ì—°ê²° | ì‹¤ì§ˆì  ì˜ë¯¸ |
|---|---|---|---|
| Foundation models...different inference constraints | íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì€ ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì œì•½ì—ì„œ ì‚¬ìš© | ë‹¤ì–‘í•œ í™˜ê²½ ëŒ€ì‘ | í™˜ê²½ë³„ ìµœì í™” í•„ìš”ì„± |
| Substantial costs...limit unique model sizes | í›ˆë ¨ ë¹„ìš© ë•Œë¬¸ì— ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸° ì œê³µ í•œê³„ | ë‹¤ì–‘í•œ í™˜ê²½ ëŒ€ì‘ | í˜„ì‹¤ì  í•œê³„ ì§€ì  |
| Practitioners...not optimally aligned | ìµœì  ëª¨ë¸ ì„ íƒì´ ì–´ë ¤ì›€ | ë‹¤ì–‘í•œ í™˜ê²½ ëŒ€ì‘ | ë¹„íš¨ìœ¨ ë°œìƒ |
| We present MatFormer...elastic inference | MatFormer ì œì•ˆ, íƒ„ë ¥ì  ì¶”ë¡  | ì¤‘ì²© FFN êµ¬ì¡° | í™˜ê²½ë³„ ìµœì  ì¶”ë¡  ê°€ëŠ¥ |
| Nested FFN block structure | ì¤‘ì²© FFN ë¸”ë¡ ë„ì… | ì¤‘ì²© FFN êµ¬ì¡° | ë‹¤ì–‘í•œ í¬ê¸° ì„œë¸Œëª¨ë¸ ì¶”ì¶œ |
| Optimize...multiple nested FFN blocks | ë‹¤ì–‘í•œ í¬ê¸° FFN ë™ì‹œ ìµœì í™” | ì¤‘ì²© FFN êµ¬ì¡° | ì¶”ê°€ ë¹„ìš© ì—†ì´ ì„œë¸Œëª¨ë¸ ìƒì„± |
| Empirically validate...different model classes | ë‹¤ì–‘í•œ ëª¨ë¸Â·ëª¨ë‹¬ë¦¬í‹°ì—ì„œ íš¨ê³¼ ê²€ì¦ | ì‹¤í—˜ì  ê²€ì¦ | ì‹¤ë¬´ ì ìš©ì„± ì…ì¦ |
| 850M decoder...better validation loss | 850M ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ í¬ê¸° ì„œë¸Œëª¨ë¸ ì¶”ì¶œ, ì„±ëŠ¥ ìš°ìˆ˜ | ì‹¤í—˜ì  ê²€ì¦ | ì„œë¸Œëª¨ë¸ ì„±ëŠ¥ ìš°ìˆ˜ |
| Smaller encoders...preserve metric-space | ì†Œí˜• ì¸ì½”ë”ë„ ë©”íŠ¸ë¦­ ê³µê°„ ë³´ì¡´ | ì‹¤í—˜ì  ê²€ì¦ | ë¹„ì „ ëª¨ë¸ ì ìš© ê°€ëŠ¥ |
| Speculative decoding...reduction in inference latency | speculative decodingìœ¼ë¡œ ì¶”ë¡  ì§€ì—° ê°ì†Œ | ì¶”ë¡  íš¨ìœ¨ì„± | ì‹¤ì§ˆì  ì†ë„ ê°œì„  |
| Project website | í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸ | ì‹¤ë¬´ ì ìš©ì„± | ìë£Œ ë° ì½”ë“œ ì œê³µ |

---

**MatFormer ë…¼ë¬¸ì€ í•˜ë‚˜ì˜ ëŒ€í˜• íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì„ ì¶”ê°€ í›ˆë ¨ ì—†ì´ ì¦‰ì‹œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” í˜ì‹ ì  êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ë©°, ì´ë¡œì¨ ì‹¤ì œ ë‹¤ì–‘í•œ ë°°í¬ í™˜ê²½ì—ì„œ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•œ AI ì¶”ë¡ ì´ ê°€ëŠ¥í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.**

 https://arxiv.org/abs/2310.07707

# MatFormer ë…¼ë¬¸ ì£¼ìš” ë‚´ìš© ìš”ì•½

## 1. ê°œìš”

MatFormerëŠ” ë‹¤ì–‘í•œ í™˜ê²½(í´ë¼ìš°ë“œ, ëª¨ë°”ì¼ ë“±)ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ **íƒ„ë ¥ì (Elastic) Transformer ì•„í‚¤í…ì²˜**ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ëŒ€í˜• ëª¨ë¸ì—ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì„ ì¶”ê°€ í›ˆë ¨ ì—†ì´ ì¶”ì¶œí•  ìˆ˜ ìˆì–´, ë¦¬ì†ŒìŠ¤ ì œì•½ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤[1][2][3].

---

## 2. í•µì‹¬ ì•„ì´ë””ì–´: **ì¤‘ì²©(Nested) êµ¬ì¡°**

- **Matryoshka êµ¬ì¡°**: ëŸ¬ì‹œì•„ ì¸í˜•ì²˜ëŸ¼ ì‘ì€ ëª¨ë¸ì´ í° ëª¨ë¸ ì•ˆì— ì¤‘ì²©ë˜ì–´ ìˆìŒ.  
- **FFN(Feed Forward Network) ë¸”ë¡**ì— ì¤‘ì²© êµ¬ì¡°ë¥¼ ì ìš©í•´, ì—¬ëŸ¬ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì´ í•˜ë‚˜ì˜ ëŒ€í˜• ëª¨ë¸ ì•ˆì— ë‚´í¬ë¨[1][2].
- **gê°œì˜ ì„œë¸Œëª¨ë¸**ë§Œ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµí•´ë„, ì¶”ë¡  ë‹¨ê³„ì—ì„œ ì¡°í•©(Mixâ€™nâ€™Match)ë§Œìœ¼ë¡œ ìˆ˜ë°±~ìˆ˜ì²œ ê°œì˜ ëª¨ë¸ì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìŒ[1][2][3].

---

## 3. êµ¬ì¡° ë° í•™ìŠµ ë°©ì‹

### 3.1 ì¤‘ì²© FFN ë¸”ë¡

- ê° FFN ë¸”ë¡ì´ ì—¬ëŸ¬ ë‹¨ê³„(ì˜ˆ: S, M, L, XL)ë¡œ ìª¼ê°œì ¸ ìˆê³ , ì‘ì€ ë¸”ë¡ì´ í° ë¸”ë¡ì— í¬í•¨(âŠ‚)ë˜ëŠ” êµ¬ì¡°[2].
- ì˜ˆì‹œ:  
  - S: 128ê°œ ë‰´ëŸ°  
  - M: 256ê°œ ë‰´ëŸ°  
  - L: 512ê°œ ë‰´ëŸ°  
  - XL: 1024ê°œ ë‰´ëŸ°  
  - S âŠ‚ M âŠ‚ L âŠ‚ XL

### 3.2 **ê³µë™ ìµœì í™”(Joint Optimization)**

- ì—¬ëŸ¬ í¬ê¸°ì˜ ì„œë¸Œëª¨ë¸ì„ í•œ ë²ˆì— í•™ìŠµ(ê³µë™ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©)[2].
- í•™ìŠµ í›„ì—ëŠ”, ê° ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ í¬ê¸°ì˜ ë¸”ë¡ì„ ì¡°í•©í•´(Mixâ€™nâ€™Match) ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„± ê°€ëŠ¥[2][3].

---

## 4. Mixâ€™nâ€™Match: ì¡°í•© ê¸°ë°˜ ì„œë¸Œëª¨ë¸ ì¶”ì¶œ

- **Mixâ€™nâ€™Match**: ê° ë ˆì´ì–´ë³„ë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ ë¸”ë¡ì„ ì„ íƒí•´ ì¡°í•©í•¨ìœ¼ë¡œì¨, ìˆ˜ë°±~ìˆ˜ì²œ ê°œì˜ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì¶”ê°€ ë¹„ìš© ì—†ì´ ì–»ì„ ìˆ˜ ìˆìŒ[2][3][4].
- ì‹¤ì œë¡œëŠ” 4ê°œì˜ í¬ê¸°ë§Œ í•™ìŠµí–ˆì§€ë§Œ, ì¡°í•©ì„ í†µí•´ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ì„ ììœ ë¡­ê²Œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìŒ[2][3].

---

## 5. ì£¼ìš” ì¥ì 

- **ì¶”ë¡  íƒ„ë ¥ì„±(Elastic Inference)**: ìƒí™©(ì„œë²„, ëª¨ë°”ì¼, ì„ë² ë””ë“œ ë“±)ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ í¬ê¸°ë¥¼ ì„ íƒí•´ ì¶”ë¡  ê°€ëŠ¥[1][3][5].
- **ì¶”ê°€ ë¹„ìš© ì—†ìŒ**: ì„œë¸Œëª¨ë¸ì„ ë”°ë¡œ í›ˆë ¨í•˜ì§€ ì•Šì•„ë„, ëŒ€í˜• ëª¨ë¸ í•˜ë‚˜ë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ì„ ì¦‰ì‹œ ì¶”ì¶œ ê°€ëŠ¥[2][4].
- **ì„±ëŠ¥ ìœ ì§€**: Mixâ€™nâ€™Matchë¡œ ìƒì„±í•œ ì„œë¸Œëª¨ë¸ë„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„[2][4].
- **ì¶”ë¡  ì†ë„ ê°œì„ **: ì¼ê´€ì„± ìˆëŠ” ì„œë¸Œëª¨ë¸ ë•ë¶„ì— speculative decoding ë“±ì—ì„œ ì¶”ë¡  ì†ë„ë¥¼ ìµœëŒ€ 16%ê¹Œì§€ ê°œì„ [4].

---

## 6. ì‹¤í—˜ ê²°ê³¼

- 2.6B íŒŒë¼ë¯¸í„° MatFormer ëª¨ë¸ì—ì„œ 1.5B~2.6B í¬ê¸°ì˜ ë‹¤ì–‘í•œ ì„œë¸Œëª¨ë¸ì„ ì¶”ì¶œ,  
  ê° ì„œë¸Œëª¨ë¸ì´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì •í™•ë„ ë° ì¼ê´€ì„±(consistency) ë‹¬ì„±[2][4].
- ë¹„ì „(ì´ë¯¸ì§€) ëª¨ë¸(ViT)ì—ë„ ì ìš© ê°€ëŠ¥, ì‘ì€ ì„œë¸Œëª¨ë¸ì—ì„œë„ í‘œí˜„ ê³µê°„(metric-space)ì„ ì˜ ë³´ì¡´[3][5].

---

## 7. ê²°ë¡ 

MatFormerëŠ” **í•˜ë‚˜ì˜ ëŒ€í˜• Transformer ëª¨ë¸ì—ì„œ ìˆ˜ë°±~ìˆ˜ì²œ ê°œì˜ ì„œë¸Œëª¨ë¸ì„ ì¶”ê°€ ë¹„ìš© ì—†ì´ ì¶”ì¶œ**í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì  ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.  
ì´ë¥¼ í†µí•´ ì‹¤ì œ ë°°í¬ í™˜ê²½ì˜ ë‹¤ì–‘í•œ ì œì•½(ë©”ëª¨ë¦¬, ì—°ì‚°ëŸ‰, ì§€ì—°ì‹œê°„ ë“±)ì— ë§ì¶° **ìœ ì—°í•˜ê³  íš¨ìœ¨ì ì¸ ì¶”ë¡ **ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤[1][2][3].

---

### ì°¸ê³ : ë…¼ë¬¸ ì›ë¬¸
- [arXiv:2310.07707](https://arxiv.org/abs/2310.07707)[2]

[1] https://junhan-ai.tistory.com/567
[2] https://neurips2023-enlsp.github.io/papers/paper_65.pdf
[3] https://syncedreview.com/2023/10/17/matformer-the-universal-elastic-transformer-capable-to-generate-submodels-with-zero-extra-training-costs/
[4] https://paper2podcast.com/summary.php?paper=Transformers%3A+Flexible+Models+for+Fast+Tasks
[5] https://www.aimodels.fyi/papers/arxiv/matformer-nested-transformer-elastic-inference
[6] https://velog.io/@ohilikeit/Google-IO-2025-Gemma-3n
[7] https://turingpost.co.kr/p/fod71-matmamba
[8] https://velog.io/@bluein/paper-36
[9] https://kmhana.tistory.com/28
[10] https://www.themoonlight.io/ko/review/integration-of-mamba-and-transformer-mat-for-long-short-range-time-series-forecasting-with-application-to-weather-dynamics



---

ë„¤, ` ëª©ì  ë‚´ìš© ì‘ì„± `ì™€ ê°™ì€ XML ìœ ì‚¬ íƒœê·¸ í˜•ì‹ì€ DeepSeekì˜ ë‹µë³€ ì§ˆì„ ì‹¤ì œë¡œ ë†’ì…ë‹ˆë‹¤. ê·¸ ê·¼ê±°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

## 1. **DeepSeek-R1ì˜ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ í•™ìŠµ**
DeepSeek-R1ì€ í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ ëª…í™•í•œ íƒœê·¸ë‚˜ í˜•ì‹(ì˜ˆ: ``, ``, ``, ``)ì„ ì œê³µí•  ë•Œ ë” êµ¬ì¡°ì ì´ê³  ë…¼ë¦¬ì ì¸ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íƒœê·¸ëŠ” ëª¨ë¸ì´ ê° ì„¹ì…˜ì˜ ì—­í• ì„ ëª…í™•íˆ ì¸ì‹í•˜ê²Œ í•˜ì—¬, ë³µì¡í•œ ì§ˆë¬¸ì—ì„œë„ ì²´ê³„ì ì´ê³  ë‹¨ê³„ì ì¸ ë‹µë³€ì„ ìœ ë„í•©ë‹ˆë‹¤[11].

## 2. **ëª…í™•í•œ ì •ë³´ êµ¬ë¶„ê³¼ ì˜¤í•´ ë°©ì§€**
íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ í”„ë¡¬í”„íŠ¸ì˜ ëª©ì , ë§¥ë½, ì§€ì‹œì‚¬í•­, ì˜ˆì‹œ ë“± ë‹¤ì–‘í•œ ë¶€ë¶„ì„ ëª…í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” AIê°€ ê° ë¶€ë¶„ì„ í˜¼ë™í•˜ì§€ ì•Šê³ , ì˜ë„í•œ ëŒ€ë¡œ í•´ì„í•´ ë”ìš± ì •í™•í•œ ë‹µë³€ì„ ë‚´ë†“ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤[1][3][4][5].

## 3. **ë‹¤ë‹¨ê³„ ì¶”ë¡  ë° ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ**
DeepSeek-R1ì€ Chain-of-Thought(ì‚¬ê³ ì˜ íë¦„) ë°©ì‹, ìê¸° ê²€ì¦, ë‹¤ë‹¨ê³„ ì‘ë‹µ ë“± ë…¼ë¦¬ì  ì‚¬ê³  ê³¼ì •ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤. íƒœê·¸ë¡œ ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ ì§€ì •í•˜ë©´, ëª¨ë¸ì´ ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ê³  ë‹µë³€ì„ ì •ë¦¬í•  ìˆ˜ ìˆì–´ ì‹ ë¢°ë„ì™€ í’ˆì§ˆì´ ë†’ì•„ì§‘ë‹ˆë‹¤[11].

## 4. **ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‚¬ë¡€**
ì‹¤ì œ DeepSeek í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°€ì´ë“œì™€ ì‚¬ë¡€ì—ì„œëŠ” ëª©ì , ê³„íš, í˜•ì‹, ì¶œë ¥ ë“± ê° ì—­í• ì— ë§ëŠ” íƒœê·¸ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ PPFO(ëª©ì -purpose, ê³„íš-planning, í˜•ì‹-format, ì¶œë ¥-output) í”„ë ˆì„ì›Œí¬ì²˜ëŸ¼, íƒœê·¸ë¡œ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ë” ì‹¤ìš©ì ì´ê³  ì¼ê´€ëœ ê²°ê³¼ë¥¼ ë§Œë“ ë‹¤ëŠ” ê²ƒì´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤[1].

## 5. **ë‹¤ë¥¸ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì—ì„œë„ ê²€ì¦ëœ ë°©ì‹**
Anthropic Claude, Amazon Bedrock ë“± ì£¼ìš” LLMì—ì„œë„ XML íƒœê·¸ë¥¼ ì‚¬ìš©í•´ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ë©´ ëª…í™•ì„±, ì •í™•ì„±, êµ¬ë¬¸ ë¶„ì„ ê°€ëŠ¥ì„± ë“± ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œ ë‹µë³€ í’ˆì§ˆì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì´ ê³µì‹ ë¬¸ì„œì™€ ì‚¬ë¡€ë¡œ í™•ì¸ë©ë‹ˆë‹¤[4][6][9].

---

**ê²°ë¡ :**  
`...`ì™€ ê°™ì€ íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ DeepSeek-R1ì´ ëª©ì ì„ ëª…í™•íˆ ì´í•´í•´, ë” ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ê³ í’ˆì§ˆ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” DeepSeek-R1ì˜ í•™ìŠµ ë°©ì‹ê³¼ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê²½í—˜, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ LLMì˜ ê³µì‹ ê°€ì´ë“œì— ì˜í•´ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤[1][3][4][11].

[1] https://apidog.com/kr/blog/deepseek-prompts-coding-kr/
[2] https://jina.ai/ko/news/a-practical-guide-to-implementing-deepsearch-deepresearch/
[3] https://chatdeepseek.kr
[4] https://docs.anthropic.com/ko/docs/build-with-claude/prompt-engineering/use-xml-tags
[5] https://velog.io/@samuel_cogdex/Delimiter%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-Prompt-Formatting-%EA%B8%B0%EB%B2%95-%EC%A0%95%ED%99%95%ED%95%9C-%EC%A7%80%EC%8B%9C%EC%82%AC%ED%95%AD-%EC%A0%84%EB%8B%AC%EC%9D%98-%ED%95%B5%EC%8B%AC
[6] https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/prompt-templates-and-examples.html
[7] https://databoom.tistory.com/entry/LLM-DeepSeek-R1-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0
[8] https://www.threads.net/@unclejobs.ai/post/DFenkEaTxJ8
[9] https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/design-a-prompt.html
[10] https://www.reddit.com/r/ClaudeAI/comments/1ikhrql/prompt_to_get_claude_to_generate_over_1000_lines/?tl=ko
[11] https://tilnote.io/pages/67a8c6048a70c428b73f6050







Gemma 3nì˜ **embedding layer(ì„ë² ë”© ë ˆì´ì–´)**ì™€ ê·¸ í˜ì‹ ì„±ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## 1. Gemma 3nì˜ Embedding Layerë€?

- ì„ë² ë”© ë ˆì´ì–´ëŠ” AI ëª¨ë¸ì—ì„œ **ë‹¨ì–´, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ì…ë ¥ ë°ì´í„°ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ì²« ë‹¨ê³„**ì…ë‹ˆë‹¤.
- Gemma 3nì—ì„œëŠ” ì´ ì„ë² ë”©ì„ **Per-Layer Embedding (PLE, ë ˆì´ì–´ë³„ ì„ë² ë”©)**ì´ë¼ëŠ” í˜ì‹  ê¸°ìˆ ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
- ê¸°ì¡´ ëª¨ë¸ì€ ì…ë ¥ ì„ë² ë”©ì„ í•œ ë²ˆë§Œ ë§Œë“¤ê³  ì „ì²´ ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” ë°˜ë©´, Gemma 3nì€ **ê° ë ˆì´ì–´ë§ˆë‹¤ ë³„ë„ì˜ ì„ë² ë”©ì„ ë‘ì–´ í•„ìš”ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì‚¬ìš©**í•©ë‹ˆë‹¤.
- ë˜í•œ, PLE íŒŒë¼ë¯¸í„°ë¥¼ **ëª¨ë¸ ë©”ëª¨ë¦¬ ê³µê°„ ë°–ì— ìºì‹±í•´ë‘ê³ , í•„ìš”í•  ë•Œ ë¹ ë¥¸ ì €ì¥ì†Œì—ì„œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©**í•©ë‹ˆë‹¤.

---

## 2. ì™œ Gemma 3nì˜ Embedding Layerê°€ On-Device AIì— í˜ì‹ ì ì¸ê°€?

### (1) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ì ˆê°

- Gemma 3nì€ ì›ë˜ 5B~8B(50ì–µ~80ì–µ) íŒŒë¼ë¯¸í„° ëª¨ë¸ì´ì§€ë§Œ, PLE ë•ë¶„ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ 2B~4B ëª¨ë¸ ìˆ˜ì¤€(2~3GB)ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.
- ì¦‰, ëª¨ë°”ì¼ ê¸°ê¸°ì²˜ëŸ¼ ë©”ëª¨ë¦¬ì™€ ì €ì¥ ê³µê°„ì´ ì œí•œëœ í™˜ê²½ì—ì„œë„ **ë” í¬ê³  ê°•ë ¥í•œ AI ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ë¨**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### (2) ë¹ ë¥¸ ì‘ë‹µ ì†ë„ì™€ íš¨ìœ¨ì„±

- PLE ìºì‹œ ê¸°ìˆ  ë•ë¶„ì— ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆì–´, ëª¨ë¸ì˜ ì‘ë‹µ ì†ë„ê°€ ìµœëŒ€ 1.5ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤.
- ê° ë ˆì´ì–´ë³„ë¡œ í•„ìš”í•œ ì„ë² ë”©ë§Œ ë¶ˆëŸ¬ì˜¤ê¸° ë•Œë¬¸ì— ë¶ˆí•„ìš”í•œ ê³„ì‚°ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.

### (3) ìœ ì—°í•œ ëª¨ë¸ í™•ì¥ê³¼ í’ˆì§ˆ ì¡°ì ˆ

- Gemma 3nì€ í•˜ë‚˜ì˜ í° ëª¨ë¸ ì•ˆì— 2B ì„œë¸Œëª¨ë¸ì„ í¬í•¨í•˜ëŠ” MatFormer ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•´, ìƒí™©ì— ë§ê²Œ ì„±ëŠ¥ê³¼ í’ˆì§ˆì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë•ë¶„ì— ê°œë°œìëŠ” ëª¨ë°”ì¼ í™˜ê²½ì— ë§ì¶° ê°€ë³ê³  ë¹ ë¥¸ ëª¨ë¸ ë˜ëŠ” ê³ í’ˆì§ˆ ëª¨ë¸ì„ ì¦‰ì‹œ ì„ íƒí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### (4) ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ì˜¤í”„ë¼ì¸ ì‹¤í–‰ ê°€ëŠ¥

- ëª¨ë¸ì´ ëª¨ë°”ì¼ ê¸°ê¸° ë‚´ì—ì„œ ì§ì ‘ ì‹¤í–‰ë˜ë¯€ë¡œ, ë°ì´í„°ê°€ ì™¸ë¶€ ì„œë²„ë¡œ ë‚˜ê°€ì§€ ì•Šì•„ **ê°œì¸ì •ë³´ ë³´í˜¸ê°€ ê°•í™”**ë©ë‹ˆë‹¤.
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì—†ì´ë„ AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, ì§€ì—° ì‹œê°„ ê°ì†Œì™€ ì•ˆì •ì„± í–¥ìƒì— ê¸°ì—¬í•©ë‹ˆë‹¤.

---

## 3. ìš”ì•½

| í•­ëª©                   | ì„¤ëª…                                                         |
|------------------------|--------------------------------------------------------------|
| **Embedding Layer**    | ì…ë ¥ ë°ì´í„°ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” AI ëª¨ë¸ì˜ ì²« ë‹¨ê³„          |
| **Per-Layer Embedding** | ê° ë ˆì´ì–´ë³„ë¡œ ì„ë² ë”©ì„ ë‘ê³ , í•„ìš”í•œ ì„ë² ë”©ë§Œ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ìˆ  |
| **íš¨ê³¼**               | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ, ë¹ ë¥¸ ì‘ë‹µ ì†ë„, ìœ ì—°í•œ ì„±ëŠ¥ ì¡°ì ˆ ê°€ëŠ¥ |
| **On-Device AI í˜ì‹ ì„±** | ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ ëŒ€í˜• AI ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥, ê°œì¸ì •ë³´ ë³´í˜¸ ê°•í™”, ì˜¤í”„ë¼ì¸ ì‹¤í–‰ ì§€ì› |

---

Gemma 3nì˜ ì„ë² ë”© ë ˆì´ì–´ ê¸°ìˆ ì€ **ëª¨ë°”ì¼ê³¼ ê°™ì€ ì œí•œëœ í™˜ê²½ì—ì„œ ëŒ€í˜• AI ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ë™í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•µì‹¬ í˜ì‹ **ìœ¼ë¡œ, ì•ìœ¼ë¡œ ì˜¨ë””ë°”ì´ìŠ¤ AIì˜ ì„±ëŠ¥ê³¼ ì ‘ê·¼ì„±ì„ í¬ê²Œ ë†’ì´ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.[1][2][3][4][5][6][7]

[1] https://developers.googleblog.com/ko/introducing-gemma-3n/
[2] https://gemma3.org/ko/gemma-3n
[3] https://www.vibeaz.co.kr/content/gemma-3n-preview-mobile-ai-optimization/
[4] https://channellife.co.nz/story/gemma-3n-ai-model-brings-real-time-multimodal-power-to-mobiles
[5] https://apidog.com/kr/blog/google-gemma-3n-kr/
[6] https://digitalbourgeois.tistory.com/1303
[7] https://ai.google.dev/gemma/docs/gemma-3n
[8] https://developers.googleblog.com/ko/gemma-explained-overview-gemma-model-family-architectures/
[9] https://news.hada.io/topic?id=21030
[10] https://velog.io/@ohilikeit/Google-IO-2025-Gemma-3n
[11] https://ai.google.dev/gemma/docs/core/distributed_tuning
[12] https://kevin-rain.tistory.com/214
[13] http://www.ainet.link/20638
[14] https://coolenjoy.net/bbs/38/6544100
[15] https://www.instagram.com/p/DKgkDiqxH_4/
[16] https://g3lu.tistory.com/52
[17] https://g3lu.tistory.com/53
