---
layout: single
title:  "cuda"
categories: pytorch
tag: [pytorch, cuda]
---



#### amp

Using amp in pytorch mostly mean that using **torch.cuda.amp.autocast** and **torch.cuda.amp.GradScale**.

They help the training time be lowered without affecting training performance -> Less GPU Usage, Better GPU Speed.



#### mixed precision

Using both datatype float16 and float32 to lower neural network's runtime and memory usages.

```python
import torch

scaler = torch.cuda.amp.autucast(enabled = True):
  outputs = model(inputs, targets)

loss = outputs["total_loss"]

opitimizer.zero_grads()
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```



------

