---
layout: single
title:  "8/4 NLP"
categories: [Programming, python, NLP, NLTK, 자연어]
tag: [Programming, python, NLP, NLTK, 자연어]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

[kocw 자연어처리 강의](http://www.kocw.net/home/cview.do?cid=e10c735a0b712c7d)

이 포스팅은 위의 강의와 자료를 보고 정리한 게시글입니다.



# corpus



* nltk 라이브러리, corpus import 하기

```python
import nltk # nltk 호출
from nltk.corpus import brown as cb # brown corpus 호출
from nltk.corpus import gutenberg as gb # gutenberg corpus 호출
```



* corpus파일 다운로드

```python
nltk.download('brown') 
nltk.download('gutenberg')
# corpus 파일의 용량이 크기때문에 download()를 이용해 따로 다운받아줘야 한다.
```



* fileids()

```python
cb.fileids()
gb.fileids()
```

NLTK 라이브러리에서 fileids()는 특정 corpus의 identifier의 목록을 검색하는 메소드이다. 이 메소드를 사용하면 문자열들의 리스트를 반환하고 각 문자열들은 corpus의 특정 문서나 텍스트 파일의 identifier을 나타낸다.



* categories()

```python
cb.categories() # brown corpus의 문장들의 카테고리들을 반환한다.
```



```python
brown_words = cb.words() # brown corpus의 총 단어개수
print(cb.words(categories = "adventure")) # adventure 카테고리에 있는 단어 출력
print(cb.words(fileids = "ca01")) # ca01 fileids에 있는 단어 출력
```



* raw()

```python
# text 전체를 가져옴
raw_text = cb.raw(fileids = "ca01")
raw_text1 = cb.raw(categories = "adventure")
```



# NLTK Tokenizing

* Sentence Tokenizing

```python
# 문단 => 문장
from nltk.tokenize import sent_tokenizing

# gutenberg corpus
raw_text = gb.raw(fileids = "austen-emma.txt")

# sentence tokenizer
len(sent_tokenize(raw_text)) # 문장개수
sents = sent_tokenize(raW_text) # 문장으로 tokenize, 문장으로 이루어진 리스트
```



* Word Tokenizing

```python
# 문장 => 단어
from nltk.tokenize import word_tokenize
word_tokenize(sents[0])
```



* Word Punct Tokenizer

```python
from nltk.tokenize import WordPunctTokenizer
wpt = WordPunctTokenizer()
wpt.tokenize(sents[0])
```

wordPunctTokenizer은 구두점을 별도로 분류하는 특징이 있다.

예를들어, 전자(word Tokenize)의 경우 Don't를 Do와 n't로 구분을 하지만 후자의 경우 Don't를 'Don'와 " ' "그리고 't'로 구분한다. 그때 그때 용도에 맞는 메소드를 사용하면 된다.



# NLTK Stemming

 : 동사의 어간을 추출하기 위한 stemming 과정

``` python
from nltk.temp import PorterStemmer # stemmer중 가장 유명한 PorterStemmer
```

```python
ps = PorterStemmer()

stem_tokens = []
# 첫번째 방법
for token in tokens :
    stem_token = ps.stem(token) # tokens의 각 원소둘울
    stem_tokens.append(Stem_token) # stem_tokens list에 append
    
# 두번째 방법
stem_tokens = [ps.stem(token) for token in tokens]    
```



```python
# stemming을 통해 단어 바뀐것 추출
import numpy as np
mask = [token != stem_token for token, stem_token in zip(tokens, stem_tokens)]
masked_tokens = np.array(tokens)[mask]
masked_stem_tokens = np.array(stem_tokens)[mask]
np.stack([masked_tokens, masked_stem_tokens], -1)
```

