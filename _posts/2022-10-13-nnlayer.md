---
title : '[DL/BASIC] 신경망의 구조 이해하기 🧠'
layout : single
---

# 3. 신경망의 구조 이해하기 



### 3.1 층 : 딥러닝의 구성요소
신경망의 기본 데이터 구조는 **층(layer)**이다. 층은 하나 이상의 텐서를 입력으로 받고 , 하나 이상의 텐서를 출력하는 데이터 처리 모듈이다. 대부분의 층은 **가중치(weight)**라는 상태를 가진다.
가중치는 SGD로 학습되는 하나 이상의 텐서이며 가중치에 신경망이 학습한 지식이 담겨있다.

층마다 적절한 텐서 포맷과 데이터 처리 방식이 다르다. 


*   (samples,features) 크기의 랭크-2 텐서에 저장된 간단한 벡터 데이터 =>  **밀집 연결층**(또는 **완전 연결층**)
*   (samples,timesteps,features) 랭크-3 텐서에 저장된 시퀀스 데이터 => LSTM 같은 **순환층**또는 **1D 합성곱층**
*   랭크-4 텐서에 저장된 이미지 데이터 =>  **2D 합성곱층**

### 3.2 모델 : 층에서 모델로
케라스에서 모델을 만드는 방법은 크게 두가지다.


1.   직접 Model 클래스의 서브클래스 만들기
2.   함수형 API 사용하기

위의 두가지 방법에 대해서는 뒤에서 다루겠다.

### 3.3 컴파일 단계 : 학습 과정 설정하기
모델 구조를 정의한 후 다음 세가지를 더 설정해야한다.


1.   손실함수 (목적함수) : 훈련 과정에서 최소화해야하는 값. 현재 작업에 대한 성공의 척도
2.   옵티마이저 : 손실함수를 기반으로 최적화 방식을 결정한다. 특정 종류의 SGD로 구현된다.
3.   측정지표 : 훈련과 검증 과정에서 모니터링하는 성공의 척도. 손실함수와 달리 직접 최적화 되지 않는다. 따라서 미분가능하지 않아도 OK




```python
from tensorflow import keras
from tensorflow.keras import models
from tensorflow.keras import layers

model = keras.Sequential([               # 층을 정의한다. 여기서는 밀집연결층(Dense)를 사용했다.
    layers.Dense(32,activation='relu'),
    layers.Dense(32)
])
model.compile(optimizer='rmsprop',         # 옵티마이저 이름을 정의한다.
              loss = 'mean_squared_error', # 손실함수 이름을 지정한다.
              metrics = ['accuracy'])      # 측정 지표를 '리스트'로 정의한다.


```

위의 compile() 매서드에서 옵티마이저,손실,측정지표의 매개변수 값을 문자열로 지정했다. 이런 문자열은 편의를 위한 단축어이며, 해당 파이썬 객체로 변환된다. 예를 들어 'rmsprop'은 keras.optimizers.RMSprop()이 된다.

이는 사용자 정의의 손실이나 측정지표를 사용하고 싶을 때 유용하다. 예를 들어서 옵티마이저의 학습률 매개변수를 변경할 수 있습니다.


```python
model.compile(optimizer = keras.optimizers.RMSprop(learning_rate= 0.0001), # 옵티마이저 학습률 변경
              loss= my_custom_loss,    # 사용자 정의 손실함수 사용
              metrics = [my_custom_metrics_1 , my_custom_metrics_2] # 사용자 정의 측정지표 사용
              )
```

하지만 일반적으로 자신만의 손실함수 , 측정지표 , 옵티마이저를 만들 필요는 없다.
케라스에서 다양한 옵션을 기본적으로 제공하기 때문이다.

### 3.4 fit( ) 매서드 : 훈련 시작!
compile( ) 다음에 fit( ) 매서드를 호출한다. fit( ) 매서드는 다음의 매개변수를 가지고 훈련 루프를 구현한다.


*   훈련할 데이터(입력,타깃) : 일반적으로 넘파이 배열 또는 텐서플로 Dataset 객체를 전달.
*   훈련할 에포크(epoch) 횟수 : 훈련 루프를 몇번이나 반복할지 결정
*   각 에포크에서 사용할 배치 크기 : 가중치 업데이트 단계에서 그레디언트를 계산하는데 사용할 샘플개수 결정.




```python
history = model.fit(
    inputs,              # 입력(넘파이 배열)
    targets,             # 훈련 타깃 (넘파이 배열)
    epochs = 5,          # 이 데이터에서 훈련 루프를 5번 반복
    batch_size = 128     # 128개의 샘플 배치로 데이터 순회
)
```

fit( )을 호출하면 History 객체가 반환된다. 이 객체는 history 딕셔너리 속성을 가지고 있다. 이 객체는 'loss' 또는 특정 치표 이름의 키와 각 에포크 값의 리스트를 매핑한다.


```python
history.history

{'loss': [9.200462341308594,
  8.842827796936035,
  8.541104316711426,
  8.250469207763672,
  7.967159271240234],
 'binary_accuracy': [0.996999979019165,
  0.996999979019165,
  0.996999979019165,
  0.996999979019165,
  0.9965000152587891]}
```

### 3.5 검증 데이터에서 모니터링 : 훈련이 아닌 검증에 맞추기
머신러닝의 목표는 훈련에 맞는 모델이 아닌 범용적으로 잘 작동하는 모델을 얻는 것이다. 모델이 훈련 샘플과 타깃의 관계를 외워버릴수 있기 때문에 모델이 처음 보는 데이터의 타깃을 예측해야한다.

새로운 데이터에 대한 모델의 성능을 예상하기 위해 훈련데이터의 일부를 **검증데이터(validation data)**로 떼어 놓는다. 검증 데이터로는 훈련하지 않지만 검증 데이터를 사용해서 손실과 측정 지표를 계산한다. fit( ) 매서드의 validation_data 매개변수를 사용한다.




```python
import numpy as np

model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])

indices_permutation = np.random.permutation(len(inputs))  
shuffled_inputs = inputs[indices_permutation]                 # 검증 데이터에 한 클래스의 샘플만 포함되는 것을 막기 위해 입력을 섞는다
shuffled_targets = targets[indices_permutation]               # 검증 데이터에 한 클래스의 샘플만 포함되는 것을 막기 위해 타깃을 섞는다

num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]         # 훈련 입력의 30%를 검증용으로 떼어 놓는다.
val_targets = shuffled_targets[:num_validation_samples]       # 훈련 타깃의 30%를 검증용으로 떼어 놓는다.
training_inputs = shuffled_inputs[num_validation_samples:]    
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    training_inputs,
    training_targets,
    epochs=5,
    batch_size=16,
    validation_data=(val_inputs, val_targets)                 # 검증 데이터는 검증 손실과 측정 지표를 모니터링하는 데만 사용한다.
)
```


```python
Epoch 1/5
88/88 [==============================] - 1s 3ms/step - loss: 0.2335 - binary_accuracy: 0.9321 - val_loss: 0.1123 - val_binary_accuracy: 0.9467
Epoch 2/5
88/88 [==============================] - 0s 2ms/step - loss: 0.0715 - binary_accuracy: 0.9479 - val_loss: 0.0314 - val_binary_accuracy: 0.9933
Epoch 3/5
88/88 [==============================] - 0s 2ms/step - loss: 0.0743 - binary_accuracy: 0.9557 - val_loss: 0.0347 - val_binary_accuracy: 0.9967
Epoch 4/5
88/88 [==============================] - 0s 2ms/step - loss: 0.0730 - binary_accuracy: 0.9600 - val_loss: 0.0435 - val_binary_accuracy: 0.9933
Epoch 5/5
88/88 [==============================] - 0s 2ms/step - loss: 0.0722 - binary_accuracy: 0.9600 - val_loss: 0.0714 - val_binary_accuracy: 0.9933
<keras.callbacks.History at 0x7fb9d8c49a20>
```

훈련이 끝난 후 검증 손실과 측정지표를 계산하고 싶다면 evaluate( ) 메서드를 사용하면 된다.

### 3.6 추론 : 새로운 데이터를 예측하기
모델을 훈련하고 이 모델을 사용해서 새로운 데이터에서 예측을 만든다. 이를 **추론**이라고 한다.
predict( ) 메서드를 활용하면 된다. predict( )은 데이터를 작은 배치로 순회하여 넘파이 배열로 예측을 반환한다. predict( )를 검증 데이터로 호출하면 각 입력 샘플에 대한 모델의 예측 점수를 스칼라로 얻게 된다.


```python
predictions = model.predict(val_inputs, batch_size=128) # predict( )를 검증 데이터로 호출
print(predictions[:10])
```


```python
5/5 [==============================] - 0s 1ms/step
[[0.1564705 ]     # 모델의 예측 점수를 스칼라로 얻게 된다.
 [0.25314882]
 [0.21407542]
 [0.20786709]
 [0.3141666 ]
 [0.8283688 ]
 [0.25783038]
 [0.51289797]
 [0.2800762 ]
 [0.7729265 ]]
```

출처:

프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 , 136-150쪽
