---
title: '[Kaggle/CV] Protein Atlas - í•œ ë‹¨ê³„ ë°œì „ëœ ëª¨ë¸ ğŸ”¬'
toc: true
toc_sticky: true
categories:
  - kaggle-imageclassification
---
## 2. Protein Atlas

### 2.0 ë“¤ì–´ê°€ë©°

[**ì €ë²ˆ ê¸€**](https://haminchang.github.io/kaggle/kaggleprotein1/)ì—ì„œëŠ” Protein Atlas ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ ë§Œë“¤ì—ˆëŠ”ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ëŠ” ëª»í–ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” í•œ ë‹¨ê³„ ë°œì „í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ Protein Atlas ë¬¸ì œê°€ ì˜ë„í•˜ëŠ”ëŒ€ë¡œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•´ ë³¼ ê²ƒì´ë‹¤.

### 2.1 íƒ€ê¹ƒ wish list ë§Œë“¤ê¸°

ëª¨ë“  íƒ€ê¹ƒê°’ì„ ì˜ˆì¸¡í•˜ê¸°ëŠ” ì–´ë µê¸° ë•Œë¬¸ì— í•œë°œì§ ë¬¼ëŸ¬ì„œì„œ ì¼ë‹¨ ê°€ì¥ í”í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” í´ë˜ìŠ¤ë“¤ì¸ nucleoplasm, cytosol, plasma membraneì„ ì˜ˆì¸¡í•´ë³´ì. 



```python
wishlist = ['Nucleoplasm','Cytosol','Plasma membrane']
```

íƒ€ê¹ƒ wishlistë¥¼ ìœ„í•œ datageneratorë¥¼ ì •ì˜í•œë‹¤. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ DataGenerator í´ë˜ìŠ¤ë¥¼ overwriteí•´ì„œ ìƒˆë¡œìš´ datageneratorë¥¼ ì •ì˜í• ê±´ë°, íƒ€ê¹ƒ wishlistë¥¼ ìœ„í•œ datageneratorë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë² ì´ìŠ¤ë¼ì¸ì˜ DataGenerator í´ë˜ìŠ¤f get_targets_per_imageë¥¼ ìƒˆë¡œ ì •ì˜í•´ì„œ ìƒˆë¡œìš´ datageneratorë¥¼ ë§Œë“ ë‹¤.


```python
class ImprovedDataGenerator(DataGenerator):
    # ë² ì´ìŠ¤ë¼ì¸ì˜ DataGeneratorì™€ ë‹¤ë¥´ê²Œ initì— target wishlistë¥¼ ì¶”ê°€
    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):
        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)
        self.target_wishlist = target_wishlist
        
    def get_targets_per_image(self, identifier):
        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values

```

### 2.2 ì—¬ëŸ¬ê°€ì§€ features ì¶”ê°€í•˜ê¸°

#### 2.2.1 í‰ê°€ ì§€í‘œ ì¶”ê°€í•˜ê¸°

ìš°ë¦¬ëŠ” ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì—ì„œ accuracy ì ìˆ˜ëŠ” ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œ ì˜ˆì¸¡ì„ í–ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆëŠ” ì§€í‘œê°€ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤. F1 macro score ì§€í‘œë¥¼ ì¶”ê°€í• ê±´ë°, ì´ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¬¸ì œì— ì‚¬ìš©í•˜ëŠ” F1 ì§€í‘œë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. í•˜ì§€ë§Œ F1 macro scoreëŠ” í´ë˜ìŠ¤ì˜ ë¹„ì¤‘ì— ê´€ê³„ì—†ì´ í‰ê·  ì§€í‘œë¥¼ ë‚´ëŠ”ë°, ì´ëŠ” í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì˜ í¸ì°¨ê°€ í° ì´ë²ˆ ë°ì´í„°ì…‹ì—ì„œëŠ” ì¢‹ì€ ë°©ë²•ì€ ì•„ë‹Œê²ƒ ê°™ê¸° ë•Œë¬¸ì—, F1 min ë˜ëŠ” F1 maxì™€ F1 stdë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•´ë³¼ ê²ƒì´ë‹¤.


```python
import keras.backend as K

def base_f1(y_true, y_pred):
    y_pred = K.round(y_pred)
    tp = K.sum(K.cast(y_true*y_pred, 'float'),axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred),'float'),axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'),axis=0)
    fn = K.sum(K.cast(y_true * (1-y_pred), 'float'), axis=0)
    
    precision = tp / (tp+fp+K.epsilon())
    recall = tp / (tp+fn+K.epsilon())
    
    f1 = 2 * precision * recall / (precision + recall + K.epsilon())
    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
    return f1

def f1_min(y_true, y_pred):
    f1 = base_f1(y_true, y_pred)
    return K.min(f1)

def f1_max(y_true, y_pred):
    f1 = base_f1(y_true, y_pred)
    return K.max(f1)

def f1_mean(y_true, y_pred):
    f1 = base_f1(y_true, y_pred)
    return K.mean(f1)

def f1_std(y_true, y_pred):
    f1 = base_f1(y_true, y_pred)
    return K.std(f1)
```

#### 2.2.2 ì†ì‹¤ ì¶”ì í•˜ë©° epochs ì¡°ì ˆí•˜ê¸°
ë” ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ìµœì ì˜ epochs ìˆ˜ë¥¼ ì•Œì•„ì•¼í•œë‹¤. ìµœì†Œí•œ epochs ìˆ˜ê°€ ë„ˆë¬´ ì ì–´ì„œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ë‹¤ ëª»í•˜ì§€ëŠ” ì•Šì•„ì•¼í•œë‹¤. ê·¸ë˜ì„œ ì†ì‹¤ì„ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ì„œ ì†ì‹¤ì´ ì¤„ì–´ë“œëŠ”ì§€ ì¶”ì í•  ìˆ˜ ìˆë„ë¡ í•´ë³´ì.


```python
class TrackHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
```

ìœ„ì—ì„œ ì •ì˜í•œ ì†ì‹¤ ì¶”ì  í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•œë‹¤.


```python
class ImprovedModel(BaseLineModel):
    def __init__(self, modelparameter, use_dropout, my_metrics = [f1_mean,f1_min,f1_max,f1_std]):
        super().__init__(modelparameter)
        self.my_metrics = my_metrics
        self.use_dropout = use_dropout
        
    def learn(self):
        self.history = TrackHistory()
        return self.model.fit_generator(generator=self.training_generator,
                                       validataion_data=self.validation_generator,
                                       epochs = self.params.n_epochs,
                                       use_multiprocessing=True,
                                       workers =8,
                                       callbacks=[self.history])
    
    def build_model(self):
        self.model = Sequential()
        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,
                             kernel_initializer=VarianceScaling(seed=0),))
        self.model.add(Conv2D(32, (3, 3), activation='relu',
                             kernel_initializer=VarianceScaling(seed=0),))
        self.model.add(MaxPooling2D(pool_size=(2, 2)))
        if self.use_dropout:
            self.model.add(Dropout(0.25))
        self.model.add(Flatten())
        self.model.add(Dense(64, activation='relu',
                            kernel_initializer=VarianceScaling(seed=0),))
        if self.use_dropout:
            self.model.add(Dropout(0.5))
        self.model.add(Dense(self.num_classes, activation='sigmoid'))
```

ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë³´ë‹¤ ë” ë§ì€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ë” ë§ì€ í›ˆë ¨ì„ ëª¨ë¸ì´ í•˜ê¸° ìœ„í•´ ë” í° epochsìˆ˜ì™€ ì‘ì€ batch_sizeë¥¼ ì‚¬ìš©í•´ë³´ì.


```python
parameter = ModelParameter(train_path, num_classes=(wishlist),n_epochs=5,
                          batch_size=64)
preprocessor = ImagePreprocessor(parameter)
labels = train_labels
```

ì´ì œ ë°œì „ëœ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° generatorë¥¼ ì¤€ë¹„í•˜ê³  ëª¨ë¸ì„ compileí•˜ê³  ë³¸ê²©ì ì¸ í›ˆë ¨ì„ í•´ë³´ì.


```python
training_generator = ImprovedDataGenerator(partition['train'],labels,parameter,preprocessor,wishlist)
validation_generator = ImprovedDataGenerator(partition['validation'],
                                            labels,parameter,preprocessor,wishlist)
predict_generator = PredictGenerator(partition['validation'],preprocessor,train_path)

test_preprocessor = ImagePreprocessor(parameter)
submission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)
```


```python

if kernelsettings.fit_improved_baseline == True:
    model = ImprovedModel(parameter, use_dropout=use_dropout)
    model.build_model()
    model.compile_model()
    model.set_generators(training_generator, validation_generator)
    epoch_history = model.learn()
    proba_predictions = model.predict(predict_generator)
    #model.save("improved_model.h5")
    
    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)
    improved_proba_predictions.to_csv("improved_predictions.csv")
    improved_losses = pd.DataFrame(epoch_history.history["loss"], columns=["train_loss"])
    improved_losses["val_loss"] = epoch_history.history["val_loss"]
    improved_losses.to_csv("improved_losses.csv")
    improved_batch_losses = pd.DataFrame(model.history.losses, columns=["batch_losses"])
    improved_batch_losses.to_csv("improved_batch_losses.csv")
    
    improved_submission_proba_predictions = model.predict(submission_predict_generator)
    improved_test_labels = test_labels.copy()
    improved_test_labels.loc[:, wishlist] = improved_submission_proba_predictions
    improved_test_labels.to_csv("improved_submission_proba.csv")
# ì‹œê°„ì„ ì•„ë¼ê¸° ìœ„í•´ Falseë¡œ ì„¤ì •í•´ì„œ ì´ë¯¸ í›ˆë ¨ëœ csv ì‚¬ìš©
else:
    improved_proba_predictions = pd.read_csv("../input/proteinatlaseabpredictions/improved_predictions.csv", index_col=0)
    improved_losses= pd.read_csv("../input/proteinatlaseabpredictions/improved_losses.csv", index_col=0)
    improved_batch_losses = pd.read_csv("../input/proteinatlaseabpredictions/improved_batch_losses.csv", index_col=0)
    improved_test_labels = pd.read_csv("../input/proteinatlaseabpredictions/improved_submission_proba.csv",index_col=0)
```

ëª¨ë¸ì´ í›ˆë ¨í•œ ì†ì‹¤ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•´ë³´ì.


```python
fig, ax = plt.subplots(2,1,figsize=(20,13))
ax[0].plot(np.arange(1,6), improved_losses["train_loss"].values, 'r--o', label="train_loss")
ax[0].plot(np.arange(1,6), improved_losses["val_loss"].values, 'g--o', label="validation_loss")
ax[0].set_xlabel("Epoch")
ax[0].set_ylabel("Loss")
ax[0].set_title("Loss evolution per epoch")
ax[0].legend()
ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label="train_batch_losses")
ax[1].set_xlabel("Number of update steps in total")
ax[1].set_ylabel("Train loss")
ax[1].set_title("Train loss evolution per batch");
```


    
![protein2_1](https://user-images.githubusercontent.com/77332628/203475800-1221f0c7-f464-459c-ae0d-5537612c98bc.png)
    


ìš°ë¦¬ëŠ” í›ˆë ¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ë©´ì„œ í›ˆë ¨ ìŠ¤í…(learning rate)ë¥¼ í¬ê²Œ í–ˆë‹¤. ê·¸ ê²°ê³¼ ë¹ ë¥¸ í›ˆë ¨ ì†ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤. í•˜ì§€ë§Œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì˜€ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ gradientë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ìˆ˜ê°€ ì ì—ˆê³  ê·¸ ê²°ê³¼ ëª¨ë¸ì´ ì¶©ë¶„í•œ í•™ìŠµì„ í•˜ì§€ ëª»í–ˆë‹¤. ê·¸ë¦¬ê³  í›ˆë ¨ì˜ ìŠ¤í…ì´ ë„ˆë¬´ ì»¤ì„œ ì†ì‹¤ì˜ ìµœì†Œê°’ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ì§€ì—­ì„ ì°¾ì§€ ëª»í–ˆë‹¤. ê·¸ëŸ¼ ëª¨ë¸ì´ í•™ìŠµì„ ì‹œì‘í•˜ê¸°ëŠ” í•œ ê±¸ê¹Œ? í•œë²ˆ í™•ì¸í•´ë³´ì. 


```python
fig, ax = plt.subplots(3,1,figsize=(25,15))
sns.distplot(improved_proba_predictions.values[:,0], color="Orange", ax=ax[0])
ax[0].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[0]))
ax[0].set_xlim([0,1])
sns.distplot(improved_proba_predictions.values[:,1], color="Purple", ax=ax[1])
ax[1].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[1]))
ax[1].set_xlim([0,1])
sns.distplot(improved_proba_predictions.values[:,2], color="Limegreen", ax=ax[2])
ax[2].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[2]))
ax[2].set_xlim([0,1]);
```


    
![protein2_2](https://user-images.githubusercontent.com/77332628/203475809-bbfb8c0b-bdfd-4fca-87b8-d06ad809d660.png)
    


ì¶œë ¥ ê°’ì„ ë³´ë©´ Nucleoplasmê³¼ Cytosol í´ë˜ìŠ¤ëŠ” ë‹¤ë´‰ ë¶„í¬ì˜ í˜•íƒœë¥¼ ë„ë©´ì„œ í™•ì‹¤íˆ 0ê³¼ 1ì„ ë¶„ë¥˜í•˜ëŠ” í•™ìŠµì„ í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ epochs ìˆ˜ëŠ” ëŠ˜ë¦¬ê³  batch ì‚¬ì´ì¦ˆëŠ” ì¤„ì¸ê²Œ ë„ì›€ì´ ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤. 



### 2.3 Batch size, epochs ë‹¤ì‹œ ì¡°ì ˆí•˜ê¸°
ì´ì œ ëª¨ë¸ì„ ë” ë°œì „ì‹œì¼œë³´ì. ì´ì „ ëª¨ë¸ì—ì„œëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì‘ì•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì¶©ë¶„íˆ í›ˆë ¨í•˜ì§€ ëª»í–ˆë‹¤. ì´ë²ˆì—ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ 128ë¡œ ë‹¤ì‹œ í‚¤ìš°ê³ , epochsë¥¼ 10ìœ¼ë¡œ ì„¤ì •í•˜ê³  ì†ì‹¤ì„ ì¶œë ¥í•´ë³´ì.


```python
parameter = ModelParameter(train_path, num_classes = len(wishlist), n_epochs=10, batch_size=128)
preprocessor = ImagePreprocessor(parameter)
labels = train_labels

training_generator = ImprovedDataGenerator(partition['train'],labels,parameter,preprocessor,wishlist)
validation_generator = ImprovedDataGenerator(partition['validation'],labels,parameter,preprocessor,wishlist)
predict_generator = PredictGenerator(partition['validation'],preprocessor,train_path)

```


```python
if kernelsettings.fit_improved_higher_batchsize == True:
    model = ImprovedModel(parameter, use_dropout=True)
    model.build_model()
    model.compile_model()
    model.set_generators(training_generator, validation_generator)
    epoch_history = model.learn()
    proba_predictions = model.predict(predict_generator)
    #model.save("improved_model.h5")
    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)
    improved_proba_predictions.to_csv("improved_hbatch_predictions.csv")
    improved_losses = pd.DataFrame(epoch_history.history["loss"], columns=["train_loss"])
    improved_losses["val_loss"] = epoch_history.history["val_loss"]
    improved_losses.to_csv("improved_hbatch_losses.csv")
    improved_batch_losses = pd.DataFrame(model.history.losses, columns=["batch_losses"])
    improved_batch_losses.to_csv("improved_hbatch_batch_losses.csv")
# ê³„ì‚° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ í›ˆë ¨ëœ ê²°ê³¼ê°’ ì‚¬ìš©
else:
    improved_proba_predictions = pd.read_csv(
        "../input/proteinatlaseabpredictions/improved_hbatch_predictions.csv", index_col=0)
    improved_losses= pd.read_csv(
        "../input/proteinatlaseabpredictions/improved_hbatch_losses.csv", index_col=0)
    improved_batch_losses = pd.read_csv("../input/proteinatlaseabpredictions/improved_hbatch_batch_losses.csv", index_col=0)
```


```python
improved_losses["train_loss"].values
```




    array([0.57145964, 0.52602202, 0.51316622, 0.50722028, 0.49814213,
           0.48810789, 0.47650666, 0.4633443 , 0.44778025, 0.42932816])




```python
fig, ax = plt.subplots(2,1,figsize=(20,13))
ax[0].plot(np.arange(1,11), improved_losses["train_loss"].values, 'r--o', label="train_loss")
ax[0].plot(np.arange(1,11), improved_losses["val_loss"].values, 'g--o', label="validation_loss")
ax[0].set_xlabel("Epoch")
ax[0].set_ylabel("Loss")
ax[0].set_title("Loss evolution per epoch")
ax[0].legend()
ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label="train_batch_losses")
ax[1].set_xlabel("Number of update steps in total")
ax[1].set_ylabel("Train loss")
ax[1].set_title("Train loss evolution per batch");
```


    
![protein2_3](https://user-images.githubusercontent.com/77332628/203475812-d300ade6-97ac-4699-acb2-e9793924740a.png)
    



```python
fig, ax = plt.subplots(3,1,figsize=(25,15))
sns.distplot(improved_proba_predictions.values[:,0], color="Orange", ax=ax[0])
ax[0].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[0]))
ax[0].set_xlim([0,1])
sns.distplot(improved_proba_predictions.values[:,1], color="Purple", ax=ax[1])
ax[1].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[1]))
ax[1].set_xlim([0,1])
sns.distplot(improved_proba_predictions.values[:,2], color="Limegreen", ax=ax[2])
ax[2].set_xlabel("Predicted probabilites of {}".format(improved_proba_predictions.columns.values[2]))
ax[2].set_xlim([0,1]);
```


    
![protein2_4](https://user-images.githubusercontent.com/77332628/203475814-8f112c55-c86e-470c-a3a7-dd50ecff57d5.png)
    


ìœ„ì˜ ì¶œë ¥ëœ ê·¸ë˜í”„ë“¤ì„ ë³´ë©´ ê²€ì¦ ì†ì‹¤ë„ ê³„ì† ì¤„ì–´ë“¤ë‹¤ê°€ ë‹¤ì‹œ ëŠ˜ì–´ë‚˜ëŠ” ì•½ê°„ì˜ ê³¼ëŒ€ì í•©ì„ ë³´ì´ê³  ìˆê¸°ë„ í•˜ê³  ë°” ê·¸ë˜í”„ë“¤ì€ ë”ìš± í™•ì‹¤í•œ ë¶„ë¥˜ë¥¼ í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤! ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦° ê²ƒì´ íš¨ê³¼ê°€ ìˆëŠ” ê²ƒ ê°™ë‹¤!

### 2.4 DropOut ì‚¬ìš©í•˜ê¸°
ë³´í†µì€ Dropoutì˜ íš¨ê³¼ë¡œ ê³¼ëŒ€ì í•©ì„ ìµœì†Œí™”í•œë‹¤ëŠ” ê²ƒì„ ìƒê°í•œë‹¤. í•˜ì§€ë§Œ Dropoutì€ ë˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê°•ì•„ì§€ì™€ ê³ ì–‘ì´ì˜ ì‚¬ì§„ì„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ í‘¼ë‹¤ê³  í•´ë³´ì. ë§Œì•½ í•˜ë‚˜ì˜ ë°°ì¹˜ì— ê³ ì–‘ì´ë¡œë§Œ ê°€ë“ì°¼ë‹¤ë©´ ëª¨ë¸ì€ ê³ ì–‘ì´ì˜ ì‚¬ì§„ë§Œ ì˜ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë  ë¿ë§Œ ì•„ë‹ˆë¼ ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ ë‘ í´ë˜ìŠ¤ê°€ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— ê°•ì•„ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ”ë°ëŠ” ì•…ì˜í–¥ì„ ë¯¸ì¹œë‹¤. í•˜ì§€ë§Œ ì´ë•Œ Dropoutì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´ í™•ë¥ ì ìœ¼ë¡œ ì¼ë¶€ë¶„ë§Œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ê³ ì–‘ì´ë¡œë§Œ ê°€ë“ì°¬ ë°°ì¹˜ë¥¼ í•™ìŠµí•˜ë”ë¼ë„ ê°•ì•„ì§€ë¥¼ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” ë°”ë€Œì§€ ì•Šì„ ê²ƒì´ê¸° ë•Œë¬¸ì— ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ë‹¤. ì´ë ‡ë“¯ Dropoutì€ ê³¼ëŒ€ì í•©ì„ ìµœì†Œí™”í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê¸°ë„ í•˜ì§€ë§Œ **ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹**ì—ì„œ í•™ìŠµí•  ë•Œë„ ë„ì›€ì´ ëœë‹¤. ìœ„ì˜ ì½”ë“œë“¤ì„ ë³´ë©´ ì•Œì•˜ê² ì§€ë§Œ ì´ë¯¸ Dropoutì¸µì„ ì‚¬ìš©í•˜ê³  ìˆì—ˆë‹¤. í•˜ì§€ë§Œ Dropout ì¸µì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë¬´ì¡°ê±´ ì¢‹ì•„ì§„ë‹¤ëŠ” ë³´ì¥ì€ í•˜ì§€ ëª»í•œë‹¤. ì™œëƒí•˜ë©´ Dropout ì¸µì€ í™•ë¥ ì ìœ¼ë¡œ ì¼ë¶€ ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì— ë„ì›€ì„ ì£¼ëŠ” ë‰´ëŸ°ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ë„, ì„±ëŠ¥ì— ì•…ì˜í–¥ì„ ì£¼ëŠ” ë‰´ëŸ°ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ìœ„ì˜ ê·¸ë˜í”„ë¥¼ ë³´ë©´ Dropoutì¸µì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ëª¨ë¸ í›ˆë ¨ ê²°ê³¼ì™€ ë¹„êµí•´ë³´ë”ë¼ë„ Droptoutì¸µì„ ì‚¬ìš©í•œ ê²ƒì´ ë¬´ì¡°ê±´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•˜ê¸° í˜ë“¤ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.


```python
# Run computation and store results as csv
if kernelsettings.fit_improved_without_dropout == True:
    model = ImprovedModel(parameter, use_dropout=False)
    model.build_model()
    model.compile_model()
    model.set_generators(training_generator, validation_generator)
    epoch_history = model.learn()
    proba_predictions = model.predict(predict_generator)
    #model.save("improved_model.h5")
    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)
    improved_proba_predictions.to_csv("improved_nodropout_predictions.csv")
    improved_losses = pd.DataFrame(epoch_history.history["loss"], columns=["train_loss"])
    improved_losses["val_loss"] = epoch_history.history["val_loss"]
    improved_losses.to_csv("improved_nodropout_losses.csv")
    improved_batch_losses = pd.DataFrame(model.history.losses, columns=["batch_losses"])
    improved_batch_losses.to_csv("improved_nodropout_batch_losses.csv")
# If you already have done a baseline fit once, 
# you can load predictions as csv and further fitting is not neccessary:
else:
    improved_proba_predictions_no_dropout = pd.read_csv(
        "../input/proteinatlaseabpredictions/improved_nodropout_predictions.csv", index_col=0)
    improved_losses_no_dropout= pd.read_csv(
        "../input/proteinatlaseabpredictions/improved_nodropout_losses.csv", index_col=0)
    improved_batch_losses_no_dropout = pd.read_csv(
        "../input/proteinatlaseabpredictions/improved_nodropout_batch_losses.csv", index_col=0)
```


```python
fig, ax = plt.subplots(2,1,figsize=(20,13))
ax[0].plot(np.arange(1,11), improved_losses["train_loss"].values, 'r--o', label="train_loss_dropout")
ax[0].plot(np.arange(1,11), improved_losses_no_dropout["train_loss"].values, 'r-o', label="train_loss_no_dropout")
ax[0].plot(np.arange(1,11), improved_losses["val_loss"].values, 'g--o', label="validation_loss")
ax[0].plot(np.arange(1,11), improved_losses_no_dropout["val_loss"].values, 'g-o', label="validation_loss_no_dropout")
ax[0].set_xlabel("Epoch")
ax[0].set_ylabel("Loss")
ax[0].set_title("Loss evolution per epoch")
ax[0].legend()
ax[1].plot(improved_batch_losses.batch_losses.values[-800::], 'r-+', label="train_batch_losses_dropout")
ax[1].plot(improved_batch_losses_no_dropout.batch_losses.values[-800::], 'b-+',
           label="train_batch_losses_no_dropout")
ax[1].set_xlabel("Number of update steps in total")
ax[1].set_ylabel("Train loss")
ax[1].set_title("Train loss evolution per batch");
ax[1].legend();
```


    
![protein2_5](https://user-images.githubusercontent.com/77332628/203475817-99907287-c113-4b11-82ab-ddc692e75139.png)


ìœ„ì˜ ì¶œë ¥ëœ ê·¸ë˜í”„ë“¤ì„ ë³´ë©´ dropoutì¸µì„ ì‚¬ìš©í•œ í›ˆë ¨ ê²°ê³¼ê°’ì´ ì‚¬ìš©í•˜ì§€ ì•Šì€ í›ˆë ¨ ê²°ê³¼ê°’ë³´ë‹¤ ë¬´ì¡°ê±´ ì¢‹ë‹¤ê³  í•˜ê¸°ëŠ” í˜ë“¤ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ë²ˆ ê¸€ì—ì„œëŠ” ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì—ì„œ ëª‡ê°€ì§€ featureë“¤ì„ ì¶”ê°€í•´ì„œ ì„±ëŠ¥ì„ ì‚´ì§ ë†’ì—¬ë´¤ë‹¤. ë‹¹ì—°íˆ 3ê°€ì§€ íƒ€ê¹ƒë§Œ ì˜ˆì¸¡ì„ í–ˆê¸° ë•Œë¬¸ì— ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ëŠ” ëª»í•˜ì§€ë§Œ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ìˆ˜ì¤€ê¹Œì§€ëŠ” ì˜¬ë ¤ë†¨ë‹¤. ë‹¤ìŒ ê¸€ì—ì„œëŠ” ì¡°ê¸ˆ ë” ë°œì „ëœ ëª¨ë¸ë“¤ì„ êµ¬ì¶•í•´ë³´ê² ë‹¤.
