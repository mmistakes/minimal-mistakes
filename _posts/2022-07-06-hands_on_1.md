---
layout: single
title:  "핸즈온 머신러닝 - 1"
categories : hands-on
tag : [hands-on, study, machine-learning, python]
toc: true
toc_sticky: true
---

![header](https://capsule-render.vercel.app/api?type=waving&color=a2dcec&height=300&section=header&text=핸즈온 머신러닝 - 1&fontSize=40&animation=fadeIn&fontAlignY=38&fontColor=FFFFFF)

- 참고 : [핸즈온 머신러닝 2판](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791162242964)

------------------------------------------------------



- 지도학습 : 알고리즘에 주입하는 훈련 데이터에 레이블 이라는 원하는 답이 포함된 학습
  - 분류 : 전형적인 지도 학습 작업, 스팸 필터처럼 많은 메일 샘플과 정보가 훈련되어 스팸, 정상으로 분류
  - 회귀 : 변수들의 특성을 사용해 target을 예측하는 것, 변수와 레이블이 포함된 데이터가 많이 필요

- 일부 회귀 알고리즘은 분류에서도 사용이 가능, 반대의 경우도 가능
- 로지스틱 회귀는 분류에서 주로 사용되며 클래스에 속할 확률을 출력함

&nbsp;

    <주요한 지도 학습 알고리즘>
      - k-최근접 이웃 (k-nearest neighbors)
      - 선형 회귀 (linear regression)
      - 로지스틱 회귀 (logistic regression)
      - 서포트 벡터 머신 (support vector machine, SVM)
      - 결정 트리와 랜덤 포레스트 (decision tree, random forest)
      - 신경망 (neural networks)

&nbsp;

- 비지도 학습 : 훈련 데이터에 레이블이 포함되지 않은 데이터

&nbsp;

    <주요한 비지도 학습 알고리즘>
      - 군집 (clustering) : 각 그룹을 더 작은 그룹으로 세분화 할수 있음
        - k-평균 (k-means)
        - DBSCAN
        - 계층 군집 분석 (hierarchical cluster analysis, HCA)
        - 이상치 탐지(outlier detection)와 특이치 탐지(novelty detection) 
        - 원-클래스 (one class, SVM)
        - 아이솔레이션 포레스트 (isolation forest)
    
      - 시각화 (visualization)와 차원 축소 (dimensionality reduction) : 시각화는 고차원의 데이터를 도식화 가능한 2D, 3D 표현을 생성, 
      너무 많은 정보를 잃지 않으면서 데이터 간소화 하는 것이 차원 축소
        - 주성분 분석 (principal component analysis, PCA)
        - 커널 (kernel , PCA)
        - 지역적 선형 임베딩 (locally-linear embedding, LLE)
        - t-SNE (t-distributed stochastic neighbor embedding)
    
      - 연관 규칙 학습 (association rule learning) : 특성 간의 흥미로운 관계를 찾는 것 
        - 어프라이어리 (Apriori)
        - 이클렛 (Eclat)

&nbsp;

- 준지도 학습 : 주로 공장같은곳에서 이상 데이터가 많이없을때 정상데이터 만으로 학습시키는 방법

- 강화 학습 : 학습자와 관찰자로 나누고 환경을 관찰하면서 행동을 실행해 결과로 보상을 받게함, 시간이 지나면서 가장 큰 보상을 얻기 위해 정책이라고 부르는 최적의 전략을 스스로 학습하는 것 

&nbsp;

-----------------------------------------------------------------------------

&nbsp;

- 배치 학습
  - 시스템이 점진적으로 학습할 수 없음
  - 가용한 데이터를 모두 사용해 학습
  - 일반적으로 시간과 자원을 많이 소모하기 때문에 보통 오프라인에서 진행
  - 먼저 시스템 훈련시키고 그후 적용 더이상의 학습은 없음

&nbsp;

- 온라인 학습
  - 데이터를 순차적으로 한 개씩 또는 미니배치라 부르는 작은 묶음 단위로 주입하여 시스템을 훈련시킴
  - 매 학습 단계가 빠르고 비용이 적게 듦
  - 학습률 조절이 중요함, 학습률 높게하면 빠르게 학습 + 금방 잊음, 반대는 반대

&nbsp;

-------------------------------------------------------------------------------

&nbsp;

- 머신러닝에서 일반화가 어떻게 되는지에 따라 분류가 가능
  - 사례 기반 학습
    - 유사도 측정을 사용해 새로운 데이터와 학습한 샘플을 비교하는 방법으로 일반화
  - 모델 기반 학습
    - 샘플들의 모델을 만들어 예측하는 것

&nbsp;

## 예제 


```python
# Python ≥3.5 이상이 권장됩니다
import sys
assert sys.version_info >= (3, 5)
```


```python
# Scikit-Learn ≥0.20 이상이 권장됩니다
import sklearn
assert sklearn.__version__ >= "0.20"
```

책에 있는 코드는 데이터 파일이 현재 디렉토리에 있다고 가정합니다. 여기에서는 `datasets/lifesat` 안에서 파일을 읽어 들입니다.


```python
import os
datapath = os.path.join("datasets", "lifesat", "")
```


```python
# 주피터에 그래프를 깔끔하게 그리기 위해서
%matplotlib inline
import matplotlib as mpl
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
```


```python
# 데이터 다운로드
import urllib.request
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/rickiepark/handson-ml2/master/"
os.makedirs(datapath, exist_ok=True)
for filename in ("oecd_bli_2015.csv", "gdp_per_capita.csv"):
    print("Downloading", filename)
    url = DOWNLOAD_ROOT + "datasets/lifesat/" + filename
    urllib.request.urlretrieve(url, datapath + filename)
```

    Downloading oecd_bli_2015.csv
    Downloading gdp_per_capita.csv



```python
# 예제 코드
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.linear_model

# 데이터 적재
oecd_bli = pd.read_csv(datapath + "oecd_bli_2015.csv", thousands=',')
gdp_per_capita = pd.read_csv(datapath + "gdp_per_capita.csv",thousands=',',delimiter='\t',
                             encoding='latin1', na_values="n/a")

# 데이터 준비
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
X = np.c_[country_stats["GDP per capita"]]
y = np.c_[country_stats["Life satisfaction"]]

# 데이터 시각화
country_stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')
plt.show()

# 선형 모델 선택
model = sklearn.linear_model.LinearRegression()

# 모델 훈련
model.fit(X, y)

# 키프로스에 대한 예측
X_new = [[22587]]  # 키프로스 1인당 GDP
print(model.predict(X_new)) # 출력 [[ 5.96242338]]
```


​    ![png](/images/2022-07-06-hands_on_1/output_9_0.png)
​    


    [[5.96242338]]


이전 코드에서 선형 회귀 모델을 k-최근접 이웃 회귀(이 경우 k=3)로 바꾸려면 간단하게 다음 두 라인만 바꾸면 됩니다:

```python
import sklearn.linear_model
model = sklearn.linear_model.LinearRegression()
```

아래와 같이 바꿉니다:

```python
import sklearn.neighbors
model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)
```


```python
# 3-최근접 이웃 회귀 모델로 바꿉니다.
import sklearn.neighbors
model1 = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)

# 모델을 훈련합니다.
model1.fit(X,y)

# 키프로스에 대한 예측을 만듭니다.
print(model1.predict(X_new)) # 출력 [[5.76666667]]
```

    [[5.76666667]]


- 이 처럼 전체 작업을 다시 파악하면
  1. 데이터를 분석
  2. 모델을 선택
  3. 훈련 데이터로 모델을 훈련
  4. 마지막으로 새로운 데이터에 모델을 적용해 예측을 하고 잘 일반화 되는지 파악

&nbsp;

--------------------------------------------------------------------------------

&nbsp;

**나쁜 데이터 사례**

- 충분하지 않은 양의 데이터 : 좋은 머신러닝 모델을 만들기 위해서는 많은양의 데이터가 필요함

- 대표성 없는 훈련 데이터 : 일반화가 잘되기 위해 일반화하기 원하는 새로운 사례를 훈련 데이터가 잘 대표하는 것이 중요.
  - 대표성이 없는 훈련 데이터로 만든 모델은 새로운 데이터에 적절하게 반응하지 못함

- 낮은 품질의 데이터 : 훈련 데이터가 에러, 이상치, 잡음으로 가득하다면 머신러닝 시스템이 내재된 패턴을 찾기 어려워 잘 작동하지 않음, 이 때문에 데이터 정제에 시간을 투자함

- 관련 없는 특성 : 훈련 데이터에 관련없는 특성이 적고 관련 있는 특성이 충분해야 시스템이 좋게 학습이 가능
  - 특성 선택 : 가지고 있는 특성 중에서 훈련에 가장 유용한 특성을 선택
  - 특성 추출 : 특성을 결합하여 더 유용한 특성을 만듦 (PCA)
  - 새로운 데이터 수집해 새 특성을 만듬

&nbsp;

--------------------------------------------------------------------------------

&nbsp;

**나쁜 알고리즘 사례**

- 훈련 데이터 과대적합 : 모델이 훈련 데이터에 너무 잘 맞지만 일반성이 떨어지는 것을 의미
  - 파라미터 수가 적은 모델선택, 훈련 데이터 더 많이 모음, 훈련 데이터의 잡음을 줄임

- 훈련 데이터 과소적합 : 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때 발생, 삶의 만족도에 대한 선형 모델은 과소적합되기 쉬움
  - 모델 파라미터가 더 많은 강력한 모델 사용, 학습 알고리즘에 더 좋은 특성 제공, 모델의 제약을 줄임

&nbsp;



## 연습문제

&nbsp;

**머신러닝을 어떻게 정의할 수 있나요?**

> 머신러닝은 데이터로부터 학습할 수 있는 시스템을 만드는 것입니다. 학습이란 어떤 작업에서 주어진 성능 지표가 더 나아지는 것을 의미합니다.

 

**머신러닝이 도움을 줄 수 있는 문제 유형 네 가지를 말해보세요.**


> 명확한 해결책이 없는 복잡한 문제, 수작업으로 만든 긴 규칙 리스트를 대체하는 경우, 변화하는 환경에 적응하는 시스템을 만들 경우, 사람에게 통찰을 제공해야 하는 경우(예를 들면 데이터 미이닝)에 머신러닝이 도움을 줄 수 있습니다. 

 

**레이블된 훈련 세트란 무엇인가요?**


> 레이블도니 훈련 세트는 각 샘플에 대해 원하는 정답(레이블)을 담고 있는 훈련 세트입니다. 

 

**가장 널리 사용되는 지도 학습 작업 두 가지는 무엇인가요?**


> 가장 일반적인 두 가지 지도 학습 문제는 회귀와 분류 입니다. 

 

**보편적인 비지도 학습 작업 네 자지는 무엇인가요?**


> 보편적인 비지도 학습 문제는 군집, 시각화, 차원 축소, 연관 규칙 학습입니다. 

 

**사전 정보가 없는 여러 지형에서 로봇을 걸어가게 하려면 어떤 종류의 머신러닝 알고리즘을 사용할 수 있나요?**


>  알려지지 않은 지형을 탐험하는 로봇을 학습시키는 가장 좋은 방법은 강화학습 입니다. 이는 전형적으로 강화 학습이 다루는 유형의 문제입니다. 이 문제를 지도 학습이나 비지도 학습으로 표현하는 것도 가능하지만 일반적이지 않습니다. 

 

**고객을 여러 그룹으로 분할하려면 어떤 알고리즘을 사용해야 하나요?**


> 만약 그룹을 어떻게 정의할지 모른다면 비슷한 고객끼리 군집으로 나누기 위해 군집 알고리즘(비지도 학습)을 사용할 수 있습니다. 그러나 어떤 그룹이 있어야 할 지 안다면 분류 알고리즘(지도 학습)에 각 그룹에 대한 샘플을 주입합니다. 그러면 알고리즘이 전체 고객을 이런 그룹으로 분류하게 될 것입니다. 

 

**스팸 감지의 문제는 지도 학습과 비지도 학습 중 어떤 문제로 볼 수 있나요?**


> 스팸 감지는 전형적인 지도 학습 문제입니다. 알고리즘에 많은 이메일과 이에 상응하는 레이블(스팸 혹은 스팸 아님)이 제공됩니다. 

 

**온라인 학습 시스템이 무엇인가요?**


> 온라인 학습 시스템은 배치 학습 시스템과 달리 점진적으로 학습할 수 있습니다. 이 방식은 변화하는 데이터와 자율 시스템에 빠르게 적응하고 매우 많은 양의 데이터를 훈련시킬 수 있습니다.  

 

**외부 메모리 학습이 무엇인가요?**


> 외부 메모리 알고리즘은 컴퓨터의 주메모리에 들어갈 수 없는 대용량의 데이터를 다룰 수 있습니다. 외부 메모리 학습 알고리즘은 데이터를 미니배치로 나누고 온라인 학습 기법을 사용해 학습합니다. 

 

**예측을 하기 위해 유사도 측정에 의존하는 학습 알고리즘은 무엇인가요?**


> 사례 기반 학습 시스템은 훈련 데이터를 기억하는 학습입니다. 새로운 샘플이 주어지면 유사도 측정을 사용해 학습된 샘플 중에서 가장 비슷한 것을 찾아 예측으로 사용합니다. 

 

**모델 파라미터와 학습 알고리즘의 하이퍼파라미터 사이에는 어떤 차이가 있나요?**

 > 모델은 하나 이상의 파라미터(예를 들면 선형 모델의 기울기)를 사용해 새로운 샘플이 주어지면 무엇을 예측할 지 결정합니다. 학습 알고리즘은 모델이 새로운 샘플에 잘 일반화되도록 이런 파라미터들의 최적값을 찾습니다. 하이퍼파라미터는 모델이 아니라 이런 학습 알고리즘 자체의 파라미터 입니다(예를 들면 적용할 규제의 정도).

 

**모델 기반 알고리즘이 찾는 것은 무엇인가요? 성공을 위해 이 알고리즘이 사용하는 가장 일반적인 전략은 무엇인가요? 예측은 어떻게 만드나요?**

> 모델 기반 학습 알고리즘은 새로운 샘플에 잘 일반화되기 위한 모델 파라미터의 최적값을 찾습니다. 일반적으로 훈련 데이터에서 시스템의 예측이 얼마나 나쁜지 측정하고 모델에 규제가 있다면 모델 복잡도에 대한 패널티를 더한 비용 함수를 최소화함으로써 시스템을 훈련시킵니다. 예측을 만들려면 학습 알고리즘이 찾은 파라미터를 사용하는 모델의 예측 함수에 새로운 샘플의 특성을 주입합니다. 

 

**머신러닝의 주요 도전 과제는 무엇인가요?**

> 머신러닝의 주요 도전 과제는 부족한 데이터, 낮은 데이터 품질, 대표성 없는 데이터, 무의미한 특성, 훈련 데이터에 과소적합된 과도하게 간단한 모델, 훈련 데이터에 과대적합된 과도하게 복잡한 모델 등입니다. 

 

**모델이 훈련 데이터에서의 성능은 좋지만 새로운 샘플에서의 일반화 성능이 나쁘다면 어떤 문제가 있는 건가요? 가능한 해결책 세 가지는 무엇인가요?**

> 모델이 훈련 데이터에서는 잘 작동하지만 새로운 샘플에서는 형편없다면 이 모델은 훈련 데이터에 과대적합되었을 가능성이 높습니다.(또는 매우 운이 좋은 경우만 훈련 데이터에 있는 것입니다.) 과대 적합에 대한 해결책은 더 많은 데이터를 모으거나, 모델을 단순화하거나(간단한 알고리즘을 선택하거나, 특성이나 파라미터의 수를 줄이거나, 모델에 규제를 추가합니다.), 훈련 데이터에 있는 잡음을 감소시키는 것입니다. 

 

**테스트 세트가 무엇이고 왜 사용해야 하나요?**


> 테스트 세트는 실전에 배치되기 전에 모델이 새로운 샘플에 대해 만들 일반화 오차를 추정하기 위해 사용합니다.

 

**검증 세트의 목적은 무엇인가요?**


> 검증 세트는 모델을 비교하는 데 사용됩니다. 이를 사용해 가장 좋은 모델을 고르고 하이퍼파라미터를 튜닝합니다. 

 

**훈련-개발 세트가 무엇인가요? 언제 필요하고 어떻게 사용해야 하나요?**

> 훈련-개발 세트는(모델을 실전에 투입했을 때 사용될 데이터와 가능한 최대로 가까워야하는) 검증, 테스트 세트에 사용되는 데이터와 훈련 세트 사이에 데이터 불일치 위험이 있을 때 사용합니다. 훈련 세트의 일부에서 모델을 훈련하고 훈련-개발 세트와 검증 세트에서 평가합니다. 모델이 훈련 세트에서 잘 동작하지만 훈련-개발 세트에서 나쁜 성능을  낸다면 아마도 훈련 세트에 과대적합되었을 가능성이 높습니다. 훈련 세트와 훈련-개발 세트 양쪽에서 모두 잘 동작하지만 검증 세트에서 성능이 나쁘다면 훈련 데이터와 검증+테스트 데이터 사이에 데이터 불일치가 있을 가능성이 높습니다. 검증+테스트 데이터에 더 가깝게 되도록 훈련 데이터를 개선해야 합니다. 

 

**테스트 세트를 사용해 하이퍼파라미터를 튜닝하면 어떤 문제가 생기나요?**

> 테스트 세트를 사용해 하이퍼파라미터를 튜닝하면 테스트 세트에 과대적합될 위험이 있고 일반화 오차를 낙관적으로 측정하게 됩니다(모델을 출시하면 기대한 것보다 나쁜 성능을 낼 것입니다.)
