---
title : '[DL/CV] ì†Œê·œëª¨ ë°ì´í„°ë¡œ ì»¨ë¸Œë„· í›ˆë ¨í•˜ê¸° : ë°ì´í„° ì¦ì‹ í™œìš©í•˜ê¸° ğŸ¤¹'
layout : single
---

# 2. ì†Œê·œëª¨ ë°ì´í„°ë¡œ ì»¨ë¸Œë„· í›ˆë ¨í•˜ê¸°

### 2.0 ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë”¥ëŸ¬ë‹ í›ˆë ¨í•˜ê¸°
ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸°ì— 'ì¶©ë¶„í•œ ìƒ˜í”Œ'ì´ë¼ëŠ” ì •ì˜ëŠ” ìƒëŒ€ì ì´ë‹¤. ìš°ì„  í›ˆë ¨í•˜ë ¤ëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ ê¹Šì´ì— ëŒ€í•´ ìƒëŒ€ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë³µì¡í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ì»¨ë¸Œë„·ì„ ìˆ˜ì‹­ê°œì˜ ë°ì´í„°ì…‹ìœ¼ë¡œë§Œ í›ˆë ¨í•œë‹¤ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ , ëª¨ë¸ì´ ì‘ê³  ê·œì œê°€ ì˜ë˜ì–´ ìˆëŠ” ëª¨ë¸ë¡œ ê°„ë‹¨í•œ ë¬¸ì œë¥¼ í‘¼ë‹¤ê³  í•˜ë©´ ìˆ˜ë°±ê°œì˜ ìƒ˜í”Œë¡œë„ ì¶©ë¶„í•  ìˆ˜ ìˆë‹¤. 

ì»¨ë¸Œë„·ì€ ì§€ì—­ì ì´ê³  í‰í–‰ì´ë™ìœ¼ë¡œ ë³€í•˜ì§€ ì•ŠëŠ” íŠ¹ì„±ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì§€ê°ì— ê´€í•œ ë¬¸ì œì—ì„œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„±ê³µí•™ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì²˜ìŒë¶€í„° ì»¨ë¸Œë„·ì„ í›ˆë ¨í•´ë„ ì–´ëŠì •ë„ì˜ ê²°ê³¼ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ê±°ê¸°ì— ë”í•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ íƒœìƒì ìœ¼ë¡œ ë‹¤ëª©ì ì´ê¸° ë•Œë¬¸ì— ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì¸ ImageNet ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ë‚´ë ¤ë°›ì•„ì„œ ë§¤ìš° ì ì€ ë°ì´í„°ë¡œ ê°•ë ¥í•œ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. (*ì´ê²ƒì´ ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ í° ì¥ì  ì¤‘ í•˜ë‚˜ì¸ íŠ¹ì„± ì¬ì‚¬ìš©ì´ë‹¤.*)

### 2.0.1 ë°ì´í„° ë‚´ë ¤ë°›ê¸°
ì´ë²ˆ ê¸€ì—ì„œ ì‚¬ìš©í•  ê°•ì•„ì§€ vs ê³ ì–‘ì´ ë°ì´í„°ì…‹ì€ ìºê¸€ì—ì„œ ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒë¥¼ ê°œìµœí•  ë‹¹ì‹œ ì œê³µí–ˆë˜ ë°ì´í„°ì…‹ì´ë‹¤. ì›ë³¸ ë°ì´í„°ì…‹ì„ ìºê¸€ì—ì„œ ë‹¤ìš´ ë°›ì„ìˆ˜ ìˆì§€ë§Œ ë‹¤ìŒ ì½”ë“œë¡œ ê°„ë‹¨íˆ ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ê² ë‹¤.


```python
import gdown
gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')
```

    Downloading...
    From: https://drive.google.com/uc?id=18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd
    To: /content/dogs-vs-cats.zip
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 852M/852M [00:12<00:00, 66.9MB/s]





    'dogs-vs-cats.zip'




```python
!unzip -qq dogs-vs-cats.zip
!unzip -qq train.zip
```

ë‚´ë ¤ë°›ì€ ë°ì´í„°ì…‹ì€ 25,000ê°œì˜ ê°•ì•„ì§€ì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ (ê°ê° 12,500ê°œì”©)ë¥¼ ë‹´ê³  ìˆê³ , 3ê°œì˜ ì„œë¸Œì…‹ì´ ë“¤ì–´ìˆëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê²ƒì´ë‹¤. í´ë˜ìŠ¤ë§ˆë‹¤ 1,000ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ í›ˆë ¨ì„¸íŠ¸, í´ë˜ìŠ¤ë§ˆë‹¤ 500ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ ê²€ì¦ì„¸íŠ¸, í´ë˜ìŠ¤ë§ˆë‹¤ 1,000ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ í…ŒìŠ¤íŠ¸ì„¸íŠ¸ì´ë‹¤. 

(*ì†Œê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¨ë¸Œë„·ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì„ ì—°ìŠµí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•œë‹¤.*)

shutil íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•´ì„œ 3ê°œì˜ ì„œë¸Œì…‹ì´ ë“¤ì–´ìˆëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤.


```python
import os, shutil, pathlib

original_dir = pathlib.Path("train")  # ì›ë³¸ ë°ì´í„°ì…‹ì´ ì••ì¶• í•´ì œë˜ì–´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ
new_base_dir = pathlib.Path("cats_vs_dogs_small")  # ì„œë¸Œì…‹ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)  # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ì²˜ìŒ 1,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ í›ˆë ¨ ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
make_subset("validation", start_index=1000, end_index=1500) # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ê·¸ ë‹¤ìŒ 500ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê²€ì • ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
make_subset("test", start_index=1500, end_index=2500)  # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ê·¸ ë‹¤ìŒ 1,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸ ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
```

ì´ì œ í›ˆë ¨ ì´ë¯¸ì§€ 2,000ê°œ, ê²€ì¦ ì´ë¯¸ì§€ 1,000ê°œ, í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ 2,000ê°œê°€ ì¤€ë¹„ë˜ì—ˆë‹¤. 

### 2.1 ëª¨ë¸ êµ¬ì¶•í•˜ê¸°

[**í•©ì„±ê³± ì‹ ê²½ë§ : ì»´í“¨í„° ë¹„ì „ì˜ ê¸°ë³¸**](https://hamin-chang.github.io/conv/) ì´ ê¸€ì—ì„œ ì‚¬ìš©í–ˆë˜ Conv2Dì¸µê³¼ MaxPooling2Dì¸µì„ ë²ˆê°ˆì•„ ìŒ“ì€ ì»¨ë¸Œë„·ì„ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê² ë‹¤. ì„ì˜ë¡œ ì„ íƒí•œ ì…ë ¥ í¬ê¸° 180x180ì˜ ì…ë ¥ìœ¼ë¡œ ì‹œì‘í•´ì„œ Flatten ì¸µ ì´ì „ì— 7x7 í¬ê¸°ì˜ íŠ¹ì„±ë§µìœ¼ë¡œ ì¤„ì–´ë“¤ê²ƒì´ë‹¤. Dogs vs Cats ë¬¸ì œëŠ” ì´ì§„ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ í•©ì„±ê³± ì¸µ ë‹¤ìŒ Dense ì¸µì€ í¬ê¸°ê°€ 1ì´ê³  sigmoid í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì´ ë³´ê³  ìˆëŠ” ìƒ˜í”Œì´ í•œ í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¸ì½”ë”©í•  ê²ƒì´ë‹¤. ë˜í•œ ëª¨ë¸ì˜ ì²« ì¸µì„ Rescaling ì¸µìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ë°, ì´ ì¸µì€ [0,255] ë²”ìœ„ì¸ ì´ë¯¸ì§€ ì…ë ¥ì„ [0,1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ì¸µì´ë‹¤.


```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(180, 180, 3))  # ì„ì˜ë¡œ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 180x180ìœ¼ë¡œ ì„¤ì •
x = layers.Rescaling(1./255)(inputs)  # ì…ë ¥ì„ 255ë¡œ ë‚˜ëˆ ì„œ [0,1]ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                     
     rescaling (Rescaling)       (None, 180, 180, 3)       0         
                                                                     
     conv2d (Conv2D)             (None, 178, 178, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 89, 89, 32)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 43, 43, 64)       0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 41, 41, 128)       73856     
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 20, 20, 128)      0         
     2D)                                                             
                                                                     
     conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 9, 9, 256)        0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    
                                                                     
     flatten (Flatten)           (None, 12544)             0         
                                                                     
     dense (Dense)               (None, 1)                 12545     
                                                                     
    =================================================================
    Total params: 991,041
    Trainable params: 991,041
    Non-trainable params: 0
    _________________________________________________________________


ì»´íŒŒì¼ ë‹¨ê³„ì—ì„œëŠ” RMS ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜ë¡œ ì´ì§„ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼(binary crossentropy) (*ëª¨ë¸ì˜ ë§ˆì§€ë§‰ì´ í•˜ë‚˜ì˜ ì‹œê·¸ëª¨ì´ë“œ ìœ ë‹›ì´ê¸° ë•Œë¬¸ì—*)ë¥¼ ì‚¬ìš©í•œë‹¤.


```python
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])
```

### 2.2 ë°ì´í„° ì „ì²˜ë¦¬
í˜„ì¬ ë°ì´í„°ê°€ JPEG íŒŒì¼ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì£¼ì…í•˜ë ¤ë©´ ë‹¤ìŒì˜ ê³¼ì •ì„ ê±°ì³ì•¼í•œë‹¤.


1.   ì‚¬ì§„ íŒŒì¼ì„ ì½ëŠ”ë‹¤.
2.   JPEG ì½˜í…ì¸ ë¥¼ RGB í”½ì…€ê°’ìœ¼ë¡œ ë””ì½”ë”©í•œë‹¤.
3.   RGB í”½ì…€ê°’ì„ ë¶€ë™ ì†Œìˆ˜ì  íƒ€ì…ì˜ í…ì„œë¡œ ë³€í™˜í•œë‹¤.
4.   ë™ì¼í•œ í¬ê¸° (ì—¬ê¸°ì„œ ì„ì˜ë¡œ ì •í•œ 180x180)ë¡œ ë³€í™˜í•œë‹¤.
5.   ë°°ì¹˜ë¡œ ë¬¶ëŠ”ë‹¤.(í•˜ë‚˜ì˜ ë°°ì¹˜ë‹¹ 32ê°œì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±)

ìœ„ì˜ ê³¼ì •ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì¼€ë¼ìŠ¤ì˜ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•œë‹¤. ì¼€ë¼ìŠ¤ê°€ ì œê³µí•˜ëŠ” image_dataset_from_directory( ) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë””ìŠ¤í¬ì— ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ í…ì„œì˜ ë°°ì¹˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. 

ë‹¤ìŒ ì½”ë“œì™€ í•¨ê»˜ image_dataset_from_directory( ) í•¨ìˆ˜ë¥¼ ì•Œì•„ë³´ì



```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / 'train', # ë¨¼ì € directoryì˜ ì„œë¸Œ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ê³  , ì´ë¯¸ì§€ íŒŒì¼ì„ ì¸ë±ì‹±
    image_size = (180,180), # ê° ì„œë¸Œ ë””ë ‰í„°ë¦¬ì— ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ë™ì¼í•œ í¬ê¸°ë¡œ ë³€í™˜
    batch_size = 32) # í•œ ë°°ì¹˜ë‹¹ 32ê°œì˜ ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆê²Œ ë°°ì¹˜ë¡œ ë¬¶ìŒ

validation_dataset = image_dataset_from_directory( # ê²€ì¦ ë°ì´í„°ë„ ë˜‘ê°™ì´ ì „ì²˜ë¦¬
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)

test_dataset = image_dataset_from_directory(   # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë˜‘ê°™ì´ ì „ì²˜ë¦¬
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)

''' í…ì„œí”Œë¡œ Dataset ê°ì²´ì˜ ìœ ìš©í•œ ë©”ì„œë“œ
.shuffle(buffer_size) : ë²„í¼ ì•ˆì˜ ì›ì†Œë¥¼ ì„ëŠ”ë‹¤.
.prefetch(buffer_size) : ì¥ì¹˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ GPU ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•œë‹¤.
.map(callable) : ì„ì˜ì˜ ë³€í™˜ì„ ë°ì´í„°ì…‹ì˜ ì›ì†Œì— ì ìš©í•œë‹¤. (ex.ì›ì†Œ í¬ê¸°ë¥¼ (16,)->(4,)ë¡œ ë³€í™˜)
(callable í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì´ ë°˜í™˜í•˜ëŠ” 1ê°œì˜ ì›ì†Œë¥¼ ì…ë ¥ìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.)'''
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    Found 2000 files belonging to 2 classes.





    ' í…ì„œí”Œë¡œ Dataset ê°ì²´ì˜ ìœ ìš©í•œ ë©”ì„œë“œ\n.shuffle(buffer_size) : ë²„í¼ ì•ˆì˜ ì›ì†Œë¥¼ ì„ëŠ”ë‹¤.\n.prefetch(buffer_size) : ì¥ì¹˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ GPU ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•œë‹¤.\n.map(callable) : ì„ì˜ì˜ ë³€í™˜ì„ ë°ì´í„°ì…‹ì˜ ì›ì†Œì— ì ìš©í•œë‹¤. (ex.ì›ì†Œ í¬ê¸°ë¥¼ (16,)->(4,)ë¡œ ë³€í™˜)\n(callable í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì´ ë°˜í™˜í•˜ëŠ” 1ê°œì˜ ì›ì†Œë¥¼ ì…ë ¥ìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.)'



ìœ„ì˜ ì½”ë“œì—ì„œ ì¤€ë¹„í•œ Dataset ê°ì²´ì˜ ì¶œë ¥ í•˜ë‚˜ë¥¼ ì‚´í´ë³´ì.


```python

for data_batch, labels_batch in train_dataset:
    print("ë°ì´í„° ë°°ì¹˜ í¬ê¸°:", data_batch.shape)
    print("ë ˆì´ë¸” ë°°ì¹˜ í¬ê¸°:", labels_batch.shape)
    break
```

    ë°ì´í„° ë°°ì¹˜ í¬ê¸°: (32, 180, 180, 3)
    ë ˆì´ë¸” ë°°ì¹˜ í¬ê¸°: (32,)


ìœ„ì˜ ì¶œë ¥ì€ 180x180 RGB ì´ë¯¸ì§€ì˜ ë°°ì¹˜ ((32,180,180,3) í¬ê¸°)ì™€ ì •ìˆ˜ ë ˆì´ë¸” ë°°ì¹˜((32,) í¬ê¸°)ì´ë‹¤. ê° ë°°ì¹˜ì—ëŠ” 32ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤.

ì´ì œ ì¤€ë¹„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•´ë³´ì.


```python
callbacks = [ 
    keras.callbacks.ModelCheckpoint(           # ì½œë°± ì‚¬ìš©
        filepath="convnet_from_scratch.keras", # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        save_best_only=True,                   # val_loss ê°’ì´ ì´ì „ë³´ë‹¤ ë‚®ì„ ë•Œë§Œ ì €ì¥
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,        # ê²€ì¦ ë°ì´í„°
    callbacks=callbacks)
```

    Epoch 1/30
    63/63 [==============================] - 17s 97ms/step - loss: 0.7200 - accuracy: 0.5315 - val_loss: 0.6768 - val_accuracy: 0.5750
    Epoch 2/30
    63/63 [==============================] - 6s 90ms/step - loss: 0.7213 - accuracy: 0.5785 - val_loss: 0.9444 - val_accuracy: 0.5100
    Epoch 3/30
    63/63 [==============================] - 5s 80ms/step - loss: 0.6674 - accuracy: 0.6230 - val_loss: 0.6560 - val_accuracy: 0.5880
    Epoch 4/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.6304 - accuracy: 0.6725 - val_loss: 0.6412 - val_accuracy: 0.6320
    Epoch 5/30
    63/63 [==============================] - 6s 98ms/step - loss: 0.5930 - accuracy: 0.6930 - val_loss: 0.5871 - val_accuracy: 0.6910
    Epoch 6/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.5564 - accuracy: 0.7180 - val_loss: 0.6310 - val_accuracy: 0.6670
    Epoch 7/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.4986 - accuracy: 0.7665 - val_loss: 0.5895 - val_accuracy: 0.7030
    Epoch 8/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.4653 - accuracy: 0.7885 - val_loss: 0.6056 - val_accuracy: 0.7140
    Epoch 9/30
    63/63 [==============================] - 5s 75ms/step - loss: 0.3962 - accuracy: 0.8130 - val_loss: 0.7551 - val_accuracy: 0.6860
    Epoch 10/30
    63/63 [==============================] - 6s 91ms/step - loss: 0.3414 - accuracy: 0.8520 - val_loss: 0.6346 - val_accuracy: 0.7260
    Epoch 11/30
    63/63 [==============================] - 6s 85ms/step - loss: 0.2960 - accuracy: 0.8765 - val_loss: 0.7377 - val_accuracy: 0.7080
    Epoch 12/30
    63/63 [==============================] - 6s 84ms/step - loss: 0.2524 - accuracy: 0.8910 - val_loss: 0.8512 - val_accuracy: 0.6840
    Epoch 13/30
    63/63 [==============================] - 7s 104ms/step - loss: 0.1845 - accuracy: 0.9235 - val_loss: 1.1699 - val_accuracy: 0.7030
    Epoch 14/30
    63/63 [==============================] - 6s 85ms/step - loss: 0.1530 - accuracy: 0.9415 - val_loss: 1.7814 - val_accuracy: 0.6510
    Epoch 15/30
    63/63 [==============================] - 7s 107ms/step - loss: 0.1198 - accuracy: 0.9575 - val_loss: 1.0960 - val_accuracy: 0.7090
    Epoch 16/30
    63/63 [==============================] - 7s 105ms/step - loss: 0.1165 - accuracy: 0.9610 - val_loss: 1.2967 - val_accuracy: 0.7060
    Epoch 17/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.0862 - accuracy: 0.9660 - val_loss: 1.4664 - val_accuracy: 0.7130
    Epoch 18/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0621 - accuracy: 0.9815 - val_loss: 1.7707 - val_accuracy: 0.7280
    Epoch 19/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0672 - accuracy: 0.9740 - val_loss: 1.5680 - val_accuracy: 0.7260
    Epoch 20/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0789 - accuracy: 0.9750 - val_loss: 2.3996 - val_accuracy: 0.6900
    Epoch 21/30
    63/63 [==============================] - 5s 71ms/step - loss: 0.0744 - accuracy: 0.9770 - val_loss: 1.6955 - val_accuracy: 0.7160
    Epoch 22/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0644 - accuracy: 0.9755 - val_loss: 1.7621 - val_accuracy: 0.7070
    Epoch 23/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0466 - accuracy: 0.9850 - val_loss: 3.0233 - val_accuracy: 0.6730
    Epoch 24/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0583 - accuracy: 0.9835 - val_loss: 1.7579 - val_accuracy: 0.7320
    Epoch 25/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0476 - accuracy: 0.9850 - val_loss: 2.2815 - val_accuracy: 0.7080
    Epoch 26/30
    63/63 [==============================] - 5s 69ms/step - loss: 0.0287 - accuracy: 0.9925 - val_loss: 2.3078 - val_accuracy: 0.6940
    Epoch 27/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0668 - accuracy: 0.9840 - val_loss: 2.2340 - val_accuracy: 0.7220
    Epoch 28/30
    63/63 [==============================] - 5s 76ms/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 2.6606 - val_accuracy: 0.7110
    Epoch 29/30
    63/63 [==============================] - 5s 73ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 2.9502 - val_accuracy: 0.7080
    Epoch 30/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 2.5123 - val_accuracy: 0.7250


í›ˆë ¨í•œ ëª¨ë¸ì˜ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë³´ì.


```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```


    
![png](20221018_files/20221018_18_0.png)
    



    
![png](20221018_files/20221018_18_1.png)
    


ìœ„ì˜ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ëª¨ë¸ì´ ê³¼ëŒ€ì í•©ëœë‹¤. í›ˆë ¨ ì •í™•ë„ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ 100%ì— ê°€ê¹Œì´ ë„ë‹¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê²€ì¦ ì •í™•ë„ëŠ” 13ë²ˆì§¸ ì—í¬í¬ë§Œì— ê±°ì˜ ìµœê³ ì ì— ë‹¤ë‹¤ë¥´ê³  ë”ì´ìƒ ì§„ì „ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¼ í…ŒìŠ¤íŠ¸ ì •í™•ë„ëŠ” ì–´ë–¨ê¹Œ? ê³¼ëŒ€ì í•© ë˜ê¸° ì „ì˜ ìƒíƒœë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì½œë°±ìœ¼ë¡œ ì €ì¥í•œ ëª¨ë¸ì„ ë¡œë“œí•˜ì.


```python

test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 36ms/step - loss: 0.5975 - accuracy: 0.6900
    í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.690


í…ŒìŠ¤íŠ¸ ì •í™•ë„ëŠ” 69%ë¥¼ ì–»ì—ˆë‹¤.

ì´ë²ˆì¥ì—ì„œ ë‹¤ë£¨ëŠ” ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì— ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤. ì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í•œ ë“œë¡­ì•„ì›ƒì´ë‚˜ L2 ê·œì œ ë“±ì˜ ë°©ë²•ë“¤ë„ ìˆì§€ë§Œ , ì»´í“¨í„° ë¹„ì „ì— íŠ¹í™”ëœ ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ë°©ë²•ì¸ **ë°ì´í„° ì¦ì‹**ì„ ì‹œë„í•˜ê² ë‹¤.

### 2.3 ë°ì´í„° ì¦ì‹
ë°ì´í„° ì¦ì‹ì€ ê¸°ì¡´ í›ˆë ¨ ìƒ˜í”Œì„ ì´ìš©í•´ì„œ ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì—¬ëŸ¬ê°€ì§€ ëœë¤í•œ ë³€í™˜ì„ ì ìš©í•´ì„œ ìƒ˜í”Œì„ ëŠ˜ë¦¬ëŠ” ë°©ë²•ì´ ìˆë‹¤. ë°ì´í„° ì¦ì‹ì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” ëª¨ë¸ì´ í›ˆë ¨í•  ë•Œ ì •í™•íˆ ê°™ì€ ë°ì´í„°ë¥¼ ë‘ë²ˆ ë§Œë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì¸¡ë©´ì„ í•™ìŠµí•˜ë¯€ë¡œ ë” ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.

ì¼€ë¼ìŠ¤ì—ì„œ ëª¨ë¸ì˜ ì‹œì‘ë¶€ë¶„ì— **ë°ì´í„° ì¦ì‹ì¸µ**ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì½”ë“œëŠ” ë°ì´í„° ì¦ì‹ ì¸µì˜ ì˜ˆì‹œ ì½”ë“œë‹¤.


```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),  # ëœë¤í•˜ê²Œ 50% ì´ë¯¸ì§€ë¥¼ ìˆ˜í‰ì„ ë’¤ì§‘ëŠ”ë‹¤.
        layers.RandomRotation(0.1),       # [-10%,+10%] ë²”ìœ„ ì•ˆì—ì„œ ëœë¤í•œ ê°’ë§Œí¼ ì´ë¯¸ì§€ íšŒì „
        layers.RandomZoom(0.2),  ])         # [-20%,+20%] ë²”ìœ„ ì•ˆì—ì„œ ëœë¤í•œ ë¹„ìœ¨ë§Œí¼ ì´ë¯¸ì§€ í™•ëŒ€ or ì¶•ì†Œ
```

ì¦ì‹ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•´ë³´ì.


```python
plt.figure(figsize=(10, 10))
for images, _ in train_dataset.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
```


    
![png](20221018_files/20221018_24_0.png)
    


ì¶œë ¥ëœ ì´ë¯¸ì§€ë“¤ì„ ë³´ë©´ ì•Œê² ì§€ë§Œ ì—¬ì „íˆ ì…ë ¥ ë°ì´í„°ë“¤ ì‚¬ì´ì— ìƒí˜¸ ì—°ê´€ì„±ì´ í¬ë‹¤. ì¦‰, ê¸°ì¡´ ì •ë³´ì˜ ì¬ì¡°í•©ë§Œ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì™„ì „íˆ ê³¼ëŒ€ì í•©ì„ ì œê±°í•  ìˆ˜ëŠ” ì—†ë‹¤. ê³¼ëŒ€ì í•©ì„ ë”ìš± í™•ì‹¤íˆ ì–µì œí•˜ê¸° ìœ„í•´ Dense ì¸µ ì§ì „ì— Dropoutì¸µì„ ì¶”ê°€í•œë‹¤.

ë˜í•œ ì´ë¯¸ì§€ ì¦ì‹ì¸µì€ Dropout ì¸µì²˜ëŸ¼ predictë‚˜ evaluateê°™ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ê³„ì—ì„œëŠ” ë™ì‘í•˜ëŠ” ì•ŠëŠ”ë‹¤. ì¦‰, ëª¨ë¸ì„ í‰ê°€í•  ë•ŒëŠ” ë°ì´í„° ì¦ì‹ê³¼ ë“œë¡­ì•„ì›ƒì´ ì—†ëŠ” ëª¨ë¸ì²˜ëŸ¼ ë™ì‘í•œë‹¤.


```python
inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)           # ì´ë¯¸ì§€ ì¦ì‹ì¸µ ì¶”ê°€
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)              # Dropout ì¸µ ì¶”ê°€
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

```


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/100
    63/63 [==============================] - 8s 101ms/step - loss: 0.7366 - accuracy: 0.5020 - val_loss: 0.6907 - val_accuracy: 0.5000
    Epoch 2/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.6942 - accuracy: 0.5330 - val_loss: 0.6770 - val_accuracy: 0.5590
    Epoch 3/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6927 - accuracy: 0.5510 - val_loss: 0.8064 - val_accuracy: 0.5150
    Epoch 4/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.6721 - accuracy: 0.5970 - val_loss: 0.6623 - val_accuracy: 0.5900
    Epoch 5/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.6472 - accuracy: 0.6290 - val_loss: 0.7294 - val_accuracy: 0.5530
    Epoch 6/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6408 - accuracy: 0.6365 - val_loss: 0.6342 - val_accuracy: 0.6630
    Epoch 7/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.6093 - accuracy: 0.6890 - val_loss: 0.5593 - val_accuracy: 0.6920
    Epoch 8/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.6034 - accuracy: 0.6910 - val_loss: 0.5635 - val_accuracy: 0.6970
    Epoch 9/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5731 - accuracy: 0.7045 - val_loss: 0.5722 - val_accuracy: 0.6940
    Epoch 10/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.5628 - accuracy: 0.7080 - val_loss: 0.5701 - val_accuracy: 0.7080
    Epoch 11/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5564 - accuracy: 0.7220 - val_loss: 0.7103 - val_accuracy: 0.6690
    Epoch 12/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5399 - accuracy: 0.7330 - val_loss: 0.5260 - val_accuracy: 0.7270
    Epoch 13/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5245 - accuracy: 0.7415 - val_loss: 0.5854 - val_accuracy: 0.7070
    Epoch 14/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.5260 - accuracy: 0.7375 - val_loss: 0.4788 - val_accuracy: 0.7720
    Epoch 15/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5038 - accuracy: 0.7630 - val_loss: 0.5134 - val_accuracy: 0.7420
    Epoch 16/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.5038 - accuracy: 0.7570 - val_loss: 0.4751 - val_accuracy: 0.7680
    Epoch 17/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4806 - accuracy: 0.7685 - val_loss: 0.4781 - val_accuracy: 0.7740
    Epoch 18/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.4832 - accuracy: 0.7705 - val_loss: 0.4908 - val_accuracy: 0.7490
    Epoch 19/100
    63/63 [==============================] - 6s 92ms/step - loss: 0.4620 - accuracy: 0.7850 - val_loss: 0.7823 - val_accuracy: 0.6990
    Epoch 20/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4638 - accuracy: 0.7890 - val_loss: 0.4866 - val_accuracy: 0.7720
    Epoch 21/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4385 - accuracy: 0.7945 - val_loss: 0.4414 - val_accuracy: 0.8110
    Epoch 22/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.4320 - accuracy: 0.8055 - val_loss: 0.4776 - val_accuracy: 0.7870
    Epoch 23/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.4391 - accuracy: 0.7990 - val_loss: 0.4527 - val_accuracy: 0.7970
    Epoch 24/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.4046 - accuracy: 0.8210 - val_loss: 0.4723 - val_accuracy: 0.7880
    Epoch 25/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.4148 - accuracy: 0.8060 - val_loss: 0.4603 - val_accuracy: 0.7960
    Epoch 26/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.4099 - accuracy: 0.8240 - val_loss: 0.4889 - val_accuracy: 0.7940
    Epoch 27/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3970 - accuracy: 0.8110 - val_loss: 0.7713 - val_accuracy: 0.7430
    Epoch 28/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.3954 - accuracy: 0.8310 - val_loss: 0.4524 - val_accuracy: 0.8110
    Epoch 29/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.3820 - accuracy: 0.8245 - val_loss: 0.5146 - val_accuracy: 0.8030
    Epoch 30/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.3792 - accuracy: 0.8380 - val_loss: 0.4337 - val_accuracy: 0.8120
    Epoch 31/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.3564 - accuracy: 0.8540 - val_loss: 0.5081 - val_accuracy: 0.7770
    Epoch 32/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.3727 - accuracy: 0.8335 - val_loss: 0.4726 - val_accuracy: 0.8090
    Epoch 33/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3614 - accuracy: 0.8415 - val_loss: 0.5023 - val_accuracy: 0.8080
    Epoch 34/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3529 - accuracy: 0.8485 - val_loss: 0.4630 - val_accuracy: 0.8110
    Epoch 35/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3374 - accuracy: 0.8620 - val_loss: 0.8080 - val_accuracy: 0.7570
    Epoch 36/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.3543 - accuracy: 0.8580 - val_loss: 0.4616 - val_accuracy: 0.8230
    Epoch 37/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.3373 - accuracy: 0.8525 - val_loss: 0.4217 - val_accuracy: 0.8340
    Epoch 38/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.3332 - accuracy: 0.8550 - val_loss: 0.4735 - val_accuracy: 0.8190
    Epoch 39/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.3032 - accuracy: 0.8675 - val_loss: 0.4315 - val_accuracy: 0.8300
    Epoch 40/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2977 - accuracy: 0.8730 - val_loss: 0.4437 - val_accuracy: 0.8470
    Epoch 41/100
    63/63 [==============================] - 6s 99ms/step - loss: 0.3005 - accuracy: 0.8645 - val_loss: 0.3975 - val_accuracy: 0.8230
    Epoch 42/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2899 - accuracy: 0.8790 - val_loss: 0.5765 - val_accuracy: 0.8180
    Epoch 43/100
    63/63 [==============================] - 6s 91ms/step - loss: 0.3003 - accuracy: 0.8725 - val_loss: 0.5932 - val_accuracy: 0.7840
    Epoch 44/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.3084 - accuracy: 0.8860 - val_loss: 0.4764 - val_accuracy: 0.7960
    Epoch 45/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2841 - accuracy: 0.8820 - val_loss: 0.4053 - val_accuracy: 0.8260
    Epoch 46/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2676 - accuracy: 0.8890 - val_loss: 0.4277 - val_accuracy: 0.8430
    Epoch 47/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2758 - accuracy: 0.8860 - val_loss: 0.4477 - val_accuracy: 0.8250
    Epoch 48/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2686 - accuracy: 0.8930 - val_loss: 0.5004 - val_accuracy: 0.8220
    Epoch 49/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2779 - accuracy: 0.8915 - val_loss: 0.5253 - val_accuracy: 0.8040
    Epoch 50/100
    63/63 [==============================] - 6s 99ms/step - loss: 0.2503 - accuracy: 0.8955 - val_loss: 0.9680 - val_accuracy: 0.7690
    Epoch 51/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2727 - accuracy: 0.8940 - val_loss: 0.4147 - val_accuracy: 0.8210
    Epoch 52/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2417 - accuracy: 0.8985 - val_loss: 0.6074 - val_accuracy: 0.8240
    Epoch 53/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2429 - accuracy: 0.8970 - val_loss: 0.4815 - val_accuracy: 0.8230
    Epoch 54/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.2299 - accuracy: 0.9120 - val_loss: 0.5419 - val_accuracy: 0.8100
    Epoch 55/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2559 - accuracy: 0.8905 - val_loss: 0.6649 - val_accuracy: 0.8030
    Epoch 56/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2423 - accuracy: 0.9125 - val_loss: 0.5062 - val_accuracy: 0.8290
    Epoch 57/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2625 - accuracy: 0.9000 - val_loss: 0.4762 - val_accuracy: 0.8160
    Epoch 58/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2184 - accuracy: 0.9180 - val_loss: 0.5613 - val_accuracy: 0.8210
    Epoch 59/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2191 - accuracy: 0.9145 - val_loss: 0.4793 - val_accuracy: 0.8480
    Epoch 60/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2253 - accuracy: 0.9115 - val_loss: 0.4898 - val_accuracy: 0.8340
    Epoch 61/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2497 - accuracy: 0.9035 - val_loss: 0.5486 - val_accuracy: 0.8380
    Epoch 62/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2251 - accuracy: 0.9120 - val_loss: 0.6480 - val_accuracy: 0.8170
    Epoch 63/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2057 - accuracy: 0.9180 - val_loss: 0.6012 - val_accuracy: 0.8090
    Epoch 64/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2392 - accuracy: 0.9070 - val_loss: 0.7982 - val_accuracy: 0.7870
    Epoch 65/100
    63/63 [==============================] - 7s 106ms/step - loss: 0.2344 - accuracy: 0.9095 - val_loss: 0.5322 - val_accuracy: 0.8200
    Epoch 66/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2084 - accuracy: 0.9190 - val_loss: 1.4610 - val_accuracy: 0.7520
    Epoch 67/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1963 - accuracy: 0.9225 - val_loss: 0.8049 - val_accuracy: 0.8030
    Epoch 68/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2132 - accuracy: 0.9170 - val_loss: 0.6219 - val_accuracy: 0.8380
    Epoch 69/100
    63/63 [==============================] - 7s 105ms/step - loss: 0.1866 - accuracy: 0.9315 - val_loss: 0.5037 - val_accuracy: 0.8490
    Epoch 70/100
    63/63 [==============================] - 7s 109ms/step - loss: 0.2079 - accuracy: 0.9195 - val_loss: 0.6616 - val_accuracy: 0.8230
    Epoch 71/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2044 - accuracy: 0.9230 - val_loss: 0.6571 - val_accuracy: 0.8300
    Epoch 72/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2269 - accuracy: 0.9175 - val_loss: 0.7447 - val_accuracy: 0.8040
    Epoch 73/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1957 - accuracy: 0.9210 - val_loss: 0.5015 - val_accuracy: 0.8460
    Epoch 74/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.2087 - accuracy: 0.9250 - val_loss: 0.5579 - val_accuracy: 0.8340
    Epoch 75/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2140 - accuracy: 0.9200 - val_loss: 0.5589 - val_accuracy: 0.8360
    Epoch 76/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.2199 - accuracy: 0.9290 - val_loss: 0.6654 - val_accuracy: 0.8480
    Epoch 77/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.1672 - accuracy: 0.9405 - val_loss: 0.6647 - val_accuracy: 0.8360
    Epoch 78/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1770 - accuracy: 0.9350 - val_loss: 0.6143 - val_accuracy: 0.8170
    Epoch 79/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1752 - accuracy: 0.9295 - val_loss: 0.6483 - val_accuracy: 0.8400
    Epoch 80/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.2064 - accuracy: 0.9220 - val_loss: 0.9454 - val_accuracy: 0.7900
    Epoch 81/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2104 - accuracy: 0.9195 - val_loss: 0.5437 - val_accuracy: 0.8540
    Epoch 82/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1769 - accuracy: 0.9340 - val_loss: 0.5951 - val_accuracy: 0.8390
    Epoch 83/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1903 - accuracy: 0.9280 - val_loss: 0.7472 - val_accuracy: 0.8410
    Epoch 84/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1801 - accuracy: 0.9300 - val_loss: 0.5435 - val_accuracy: 0.8600
    Epoch 85/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.1798 - accuracy: 0.9360 - val_loss: 0.7137 - val_accuracy: 0.8530
    Epoch 86/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1973 - accuracy: 0.9305 - val_loss: 0.6568 - val_accuracy: 0.8450
    Epoch 87/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1689 - accuracy: 0.9345 - val_loss: 0.6006 - val_accuracy: 0.8640
    Epoch 88/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1599 - accuracy: 0.9440 - val_loss: 1.3120 - val_accuracy: 0.8050
    Epoch 89/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1991 - accuracy: 0.9270 - val_loss: 0.6726 - val_accuracy: 0.8410
    Epoch 90/100
    63/63 [==============================] - 6s 94ms/step - loss: 0.1820 - accuracy: 0.9330 - val_loss: 0.7732 - val_accuracy: 0.8550
    Epoch 91/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1948 - accuracy: 0.9270 - val_loss: 0.6773 - val_accuracy: 0.8330
    Epoch 92/100
    63/63 [==============================] - 7s 103ms/step - loss: 0.1979 - accuracy: 0.9275 - val_loss: 0.5139 - val_accuracy: 0.8420
    Epoch 93/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1698 - accuracy: 0.9445 - val_loss: 0.8142 - val_accuracy: 0.8570
    Epoch 94/100
    63/63 [==============================] - 6s 95ms/step - loss: 0.1746 - accuracy: 0.9385 - val_loss: 0.6159 - val_accuracy: 0.8470
    Epoch 95/100
    63/63 [==============================] - 6s 98ms/step - loss: 0.1748 - accuracy: 0.9400 - val_loss: 0.8730 - val_accuracy: 0.7350
    Epoch 96/100
    63/63 [==============================] - 7s 97ms/step - loss: 0.2152 - accuracy: 0.9265 - val_loss: 0.7178 - val_accuracy: 0.8530
    Epoch 97/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1700 - accuracy: 0.9385 - val_loss: 0.5947 - val_accuracy: 0.8420
    Epoch 98/100
    63/63 [==============================] - 6s 97ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.6206 - val_accuracy: 0.8550
    Epoch 99/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1689 - accuracy: 0.9455 - val_loss: 0.9251 - val_accuracy: 0.8260
    Epoch 100/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2022 - accuracy: 0.9360 - val_loss: 0.7826 - val_accuracy: 0.8310


ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë³´ë©´ , ë°ì´í„° ì¦ì‹ê³¼ ë“œë¡­ì•„ì›ƒ ë•ë¶„ì—ê³¼ëŒ€ì í•©ì´ ì´ì „ì˜ ëª¨ë¸ ë³´ë‹¤ í›¨ì”¬ ëŠ¦ì€ 60,70ë²ˆì§¸ ì—í¬í¬ ê·¼ì²˜ì—ì„œ ì‹œì‘ëœë‹¤. ì¦‰, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì›”ë“±íˆ ì¢‹ì•„ì¡Œë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•´ë³´ì.


```python
test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 37ms/step - loss: 0.4403 - accuracy: 0.8290
    í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.829


í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ 82.9% ì •ë„ ì–»ì—ˆë‹¤. ì´ì „ì— ì–»ì—ˆë˜ 69% ë³´ë‹¤ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ íŠœë‹í•˜ë©´ ë” ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì— í•œê³„ê°€ ìˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ë” ì¢‹ì€ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ **ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ í™œìš©**í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ ê¸€ì—ì„œ ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.

[<ì¼€ë¼ìŠ¤ ì°½ì‹œìì—ê²Œ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ê°œì • 2íŒ>(ê¸¸ë²—, 2022)ì„ í•™ìŠµí•˜ê³  ê°œì¸ í•™ìŠµìš©ìœ¼ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.]

ì¶œì²˜: í”„ë‘ì†Œì™€ ìˆ„ë ˆ ì§€ìŒ, âŒœì¼€ë¼ìŠ¤ ì°½ì‹œìì—ê²Œ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ê°œì •2íŒâŒŸ, ë°•í•´ì„  ì˜®ê¹€, ê¸¸ë²—, 2022 , 318-336ìª½

ë„ì„œë³´ê¸°: https://www.gilbut.co.kr/book/view?bookcode=BN003496
