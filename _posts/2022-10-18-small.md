---
title : '[DL/CV] ì†Œê·œëª¨ ë°ì´í„°ë¡œ ì»¨ë¸Œë„· í›ˆë ¨í•˜ê¸° : ë°ì´í„° ì¦ì‹ í™œìš©í•˜ê¸° ğŸ¤¹'
layout : single
---

# 2. ì†Œê·œëª¨ ë°ì´í„°ë¡œ ì»¨ë¸Œë„· í›ˆë ¨í•˜ê¸°

### 2.0 ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë”¥ëŸ¬ë‹ í›ˆë ¨í•˜ê¸°
ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸°ì— 'ì¶©ë¶„í•œ ìƒ˜í”Œ'ì´ë¼ëŠ” ì •ì˜ëŠ” ìƒëŒ€ì ì´ë‹¤. ìš°ì„  í›ˆë ¨í•˜ë ¤ëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ ê¹Šì´ì— ëŒ€í•´ ìƒëŒ€ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë³µì¡í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ì»¨ë¸Œë„·ì„ ìˆ˜ì‹­ê°œì˜ ë°ì´í„°ì…‹ìœ¼ë¡œë§Œ í›ˆë ¨í•œë‹¤ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ , ëª¨ë¸ì´ ì‘ê³  ê·œì œê°€ ì˜ë˜ì–´ ìˆëŠ” ëª¨ë¸ë¡œ ê°„ë‹¨í•œ ë¬¸ì œë¥¼ í‘¼ë‹¤ê³  í•˜ë©´ ìˆ˜ë°±ê°œì˜ ìƒ˜í”Œë¡œë„ ì¶©ë¶„í•  ìˆ˜ ìˆë‹¤. 

ì»¨ë¸Œë„·ì€ ì§€ì—­ì ì´ê³  í‰í–‰ì´ë™ìœ¼ë¡œ ë³€í•˜ì§€ ì•ŠëŠ” íŠ¹ì„±ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì§€ê°ì— ê´€í•œ ë¬¸ì œì—ì„œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ ì‘ì€ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì„±ê³µí•™ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì²˜ìŒë¶€í„° ì»¨ë¸Œë„·ì„ í›ˆë ¨í•´ë„ ì–´ëŠì •ë„ì˜ ê²°ê³¼ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ê±°ê¸°ì— ë”í•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ íƒœìƒì ìœ¼ë¡œ ë‹¤ëª©ì ì´ê¸° ë•Œë¬¸ì— ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì¸ ImageNet ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ë‚´ë ¤ë°›ì•„ì„œ ë§¤ìš° ì ì€ ë°ì´í„°ë¡œ ê°•ë ¥í•œ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. (*ì´ê²ƒì´ ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ í° ì¥ì  ì¤‘ í•˜ë‚˜ì¸ íŠ¹ì„± ì¬ì‚¬ìš©ì´ë‹¤.*)

### 2.0.1 ë°ì´í„° ë‚´ë ¤ë°›ê¸°
ì´ë²ˆ ê¸€ì—ì„œ ì‚¬ìš©í•  ê°•ì•„ì§€ vs ê³ ì–‘ì´ ë°ì´í„°ì…‹ì€ ìºê¸€ì—ì„œ ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒë¥¼ ê°œìµœí•  ë‹¹ì‹œ ì œê³µí–ˆë˜ ë°ì´í„°ì…‹ì´ë‹¤. ì›ë³¸ ë°ì´í„°ì…‹ì„ ìºê¸€ì—ì„œ ë‹¤ìš´ ë°›ì„ìˆ˜ ìˆì§€ë§Œ ë‹¤ìŒ ì½”ë“œë¡œ ê°„ë‹¨íˆ ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ê² ë‹¤.


```python
import gdown
gdown.download(id='18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd', output='dogs-vs-cats.zip')
```

    Downloading...
    From: https://drive.google.com/uc?id=18uC7WTuEXKJDDxbj-Jq6EjzpFrgE7IAd
    To: /content/dogs-vs-cats.zip
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 852M/852M [00:12<00:00, 66.9MB/s]





    'dogs-vs-cats.zip'




```python
!unzip -qq dogs-vs-cats.zip
!unzip -qq train.zip
```

ë‚´ë ¤ë°›ì€ ë°ì´í„°ì…‹ì€ 25,000ê°œì˜ ê°•ì•„ì§€ì™€ ê³ ì–‘ì´ ì´ë¯¸ì§€ (ê°ê° 12,500ê°œì”©)ë¥¼ ë‹´ê³  ìˆê³ , 3ê°œì˜ ì„œë¸Œì…‹ì´ ë“¤ì–´ìˆëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê²ƒì´ë‹¤. í´ë˜ìŠ¤ë§ˆë‹¤ 1,000ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ í›ˆë ¨ì„¸íŠ¸, í´ë˜ìŠ¤ë§ˆë‹¤ 500ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ ê²€ì¦ì„¸íŠ¸, í´ë˜ìŠ¤ë§ˆë‹¤ 1,000ê°œì˜ ìƒ˜í”Œë¡œ ì´ë£¨ì–´ì§„ í…ŒìŠ¤íŠ¸ì„¸íŠ¸ì´ë‹¤. 

(*ì†Œê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¨ë¸Œë„·ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì„ ì—°ìŠµí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•œë‹¤.*)

shutil íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•´ì„œ 3ê°œì˜ ì„œë¸Œì…‹ì´ ë“¤ì–´ìˆëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤.


```python
import os, shutil, pathlib

original_dir = pathlib.Path("train")  # ì›ë³¸ ë°ì´í„°ì…‹ì´ ì••ì¶• í•´ì œë˜ì–´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ
new_base_dir = pathlib.Path("cats_vs_dogs_small")  # ì„œë¸Œì…‹ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname,
                            dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)  # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ì²˜ìŒ 1,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ í›ˆë ¨ ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
make_subset("validation", start_index=1000, end_index=1500) # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ê·¸ ë‹¤ìŒ 500ê°œì˜ ì´ë¯¸ì§€ë¥¼ ê²€ì • ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
make_subset("test", start_index=1500, end_index=2500)  # ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ê·¸ ë‹¤ìŒ 1,000ê°œì˜ ì´ë¯¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸ ì„œë¸Œì…‹ìœ¼ë¡œ ì €ì¥
```

ì´ì œ í›ˆë ¨ ì´ë¯¸ì§€ 2,000ê°œ, ê²€ì¦ ì´ë¯¸ì§€ 1,000ê°œ, í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ 2,000ê°œê°€ ì¤€ë¹„ë˜ì—ˆë‹¤. 

### 2.1 ëª¨ë¸ êµ¬ì¶•í•˜ê¸°

[**í•©ì„±ê³± ì‹ ê²½ë§ : ì»´í“¨í„° ë¹„ì „ì˜ ê¸°ë³¸**](https://hamin-chang.github.io/conv/) ì´ ê¸€ì—ì„œ ì‚¬ìš©í–ˆë˜ Conv2Dì¸µê³¼ MaxPooling2Dì¸µì„ ë²ˆê°ˆì•„ ìŒ“ì€ ì»¨ë¸Œë„·ì„ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê² ë‹¤. ì„ì˜ë¡œ ì„ íƒí•œ ì…ë ¥ í¬ê¸° 180x180ì˜ ì…ë ¥ìœ¼ë¡œ ì‹œì‘í•´ì„œ Flatten ì¸µ ì´ì „ì— 7x7 í¬ê¸°ì˜ íŠ¹ì„±ë§µìœ¼ë¡œ ì¤„ì–´ë“¤ê²ƒì´ë‹¤. Dogs vs Cats ë¬¸ì œëŠ” ì´ì§„ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ í•©ì„±ê³± ì¸µ ë‹¤ìŒ Dense ì¸µì€ í¬ê¸°ê°€ 1ì´ê³  sigmoid í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì´ ë³´ê³  ìˆëŠ” ìƒ˜í”Œì´ í•œ í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¸ì½”ë”©í•  ê²ƒì´ë‹¤. ë˜í•œ ëª¨ë¸ì˜ ì²« ì¸µì„ Rescaling ì¸µìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ë°, ì´ ì¸µì€ [0,255] ë²”ìœ„ì¸ ì´ë¯¸ì§€ ì…ë ¥ì„ [0,1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ì¸µì´ë‹¤.


```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(180, 180, 3))  # ì„ì˜ë¡œ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 180x180ìœ¼ë¡œ ì„¤ì •
x = layers.Rescaling(1./255)(inputs)  # ì…ë ¥ì„ 255ë¡œ ë‚˜ëˆ ì„œ [0,1]ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 180, 180, 3)]     0         
                                                                     
     rescaling (Rescaling)       (None, 180, 180, 3)       0         
                                                                     
     conv2d (Conv2D)             (None, 178, 178, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 89, 89, 32)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 43, 43, 64)       0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 41, 41, 128)       73856     
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 20, 20, 128)      0         
     2D)                                                             
                                                                     
     conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 9, 9, 256)        0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    
                                                                     
     flatten (Flatten)           (None, 12544)             0         
                                                                     
     dense (Dense)               (None, 1)                 12545     
                                                                     
    =================================================================
    Total params: 991,041
    Trainable params: 991,041
    Non-trainable params: 0
    _________________________________________________________________


ì»´íŒŒì¼ ë‹¨ê³„ì—ì„œëŠ” RMS ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜ë¡œ ì´ì§„ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼(binary crossentropy) (*ëª¨ë¸ì˜ ë§ˆì§€ë§‰ì´ í•˜ë‚˜ì˜ ì‹œê·¸ëª¨ì´ë“œ ìœ ë‹›ì´ê¸° ë•Œë¬¸ì—*)ë¥¼ ì‚¬ìš©í•œë‹¤.


```python
model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])
```

### 2.2 ë°ì´í„° ì „ì²˜ë¦¬
í˜„ì¬ ë°ì´í„°ê°€ JPEG íŒŒì¼ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì£¼ì…í•˜ë ¤ë©´ ë‹¤ìŒì˜ ê³¼ì •ì„ ê±°ì³ì•¼í•œë‹¤.


1.   ì‚¬ì§„ íŒŒì¼ì„ ì½ëŠ”ë‹¤.
2.   JPEG ì½˜í…ì¸ ë¥¼ RGB í”½ì…€ê°’ìœ¼ë¡œ ë””ì½”ë”©í•œë‹¤.
3.   RGB í”½ì…€ê°’ì„ ë¶€ë™ ì†Œìˆ˜ì  íƒ€ì…ì˜ í…ì„œë¡œ ë³€í™˜í•œë‹¤.
4.   ë™ì¼í•œ í¬ê¸° (ì—¬ê¸°ì„œ ì„ì˜ë¡œ ì •í•œ 180x180)ë¡œ ë³€í™˜í•œë‹¤.
5.   ë°°ì¹˜ë¡œ ë¬¶ëŠ”ë‹¤.(í•˜ë‚˜ì˜ ë°°ì¹˜ë‹¹ 32ê°œì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±)

ìœ„ì˜ ê³¼ì •ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì¼€ë¼ìŠ¤ì˜ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•œë‹¤. ì¼€ë¼ìŠ¤ê°€ ì œê³µí•˜ëŠ” image_dataset_from_directory( ) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë””ìŠ¤í¬ì— ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ í…ì„œì˜ ë°°ì¹˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. 

ë‹¤ìŒ ì½”ë“œì™€ í•¨ê»˜ image_dataset_from_directory( ) í•¨ìˆ˜ë¥¼ ì•Œì•„ë³´ì



```python
from tensorflow.keras.utils import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
    new_base_dir / 'train', # ë¨¼ì € directoryì˜ ì„œë¸Œ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ê³  , ì´ë¯¸ì§€ íŒŒì¼ì„ ì¸ë±ì‹±
    image_size = (180,180), # ê° ì„œë¸Œ ë””ë ‰í„°ë¦¬ì— ìˆëŠ” ì´ë¯¸ì§€ íŒŒì¼ì„ ë™ì¼í•œ í¬ê¸°ë¡œ ë³€í™˜
    batch_size = 32) # í•œ ë°°ì¹˜ë‹¹ 32ê°œì˜ ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆê²Œ ë°°ì¹˜ë¡œ ë¬¶ìŒ

validation_dataset = image_dataset_from_directory( # ê²€ì¦ ë°ì´í„°ë„ ë˜‘ê°™ì´ ì „ì²˜ë¦¬
    new_base_dir / "validation",
    image_size=(180, 180),
    batch_size=32)

test_dataset = image_dataset_from_directory(   # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë˜‘ê°™ì´ ì „ì²˜ë¦¬
    new_base_dir / "test",
    image_size=(180, 180),
    batch_size=32)

''' í…ì„œí”Œë¡œ Dataset ê°ì²´ì˜ ìœ ìš©í•œ ë©”ì„œë“œ
.shuffle(buffer_size) : ë²„í¼ ì•ˆì˜ ì›ì†Œë¥¼ ì„ëŠ”ë‹¤.
.prefetch(buffer_size) : ì¥ì¹˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ GPU ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•œë‹¤.
.map(callable) : ì„ì˜ì˜ ë³€í™˜ì„ ë°ì´í„°ì…‹ì˜ ì›ì†Œì— ì ìš©í•œë‹¤. (ex.ì›ì†Œ í¬ê¸°ë¥¼ (16,)->(4,)ë¡œ ë³€í™˜)
(callable í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì´ ë°˜í™˜í•˜ëŠ” 1ê°œì˜ ì›ì†Œë¥¼ ì…ë ¥ìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.)'''
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    Found 2000 files belonging to 2 classes.




```
'í…ì„œí”Œë¡œ Dataset ê°ì²´ì˜ ìœ ìš©í•œ ë©”ì„œë“œ
.shuffle(buffer_size) : ë²„í¼ ì•ˆì˜ ì›ì†Œë¥¼ ì„ëŠ”ë‹¤.
.prefetch(buffer_size) : ì¥ì¹˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ GPU ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•œë‹¤.
.map(callable) : ì„ì˜ì˜ ë³€í™˜ì„ ë°ì´í„°ì…‹ì˜ ì›ì†Œì— ì ìš©í•œë‹¤. (ex.ì›ì†Œ í¬ê¸°ë¥¼ (16,)->(4,)ë¡œ ë³€í™˜)\n(callable í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì´ ë°˜í™˜í•˜ëŠ” 1ê°œì˜ ì›ì†Œë¥¼ ì…ë ¥ìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤.)'
```


ìœ„ì˜ ì½”ë“œì—ì„œ ì¤€ë¹„í•œ Dataset ê°ì²´ì˜ ì¶œë ¥ í•˜ë‚˜ë¥¼ ì‚´í´ë³´ì.


```python

for data_batch, labels_batch in train_dataset:
    print("ë°ì´í„° ë°°ì¹˜ í¬ê¸°:", data_batch.shape)
    print("ë ˆì´ë¸” ë°°ì¹˜ í¬ê¸°:", labels_batch.shape)
    break
```

    ë°ì´í„° ë°°ì¹˜ í¬ê¸°: (32, 180, 180, 3)
    ë ˆì´ë¸” ë°°ì¹˜ í¬ê¸°: (32,)


ìœ„ì˜ ì¶œë ¥ì€ 180x180 RGB ì´ë¯¸ì§€ì˜ ë°°ì¹˜ ((32,180,180,3) í¬ê¸°)ì™€ ì •ìˆ˜ ë ˆì´ë¸” ë°°ì¹˜((32,) í¬ê¸°)ì´ë‹¤. ê° ë°°ì¹˜ì—ëŠ” 32ê°œì˜ ìƒ˜í”Œì´ ìˆë‹¤.

ì´ì œ ì¤€ë¹„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•´ë³´ì.


```python
callbacks = [ 
    keras.callbacks.ModelCheckpoint(           # ì½œë°± ì‚¬ìš©
        filepath="convnet_from_scratch.keras", # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        save_best_only=True,                   # val_loss ê°’ì´ ì´ì „ë³´ë‹¤ ë‚®ì„ ë•Œë§Œ ì €ì¥
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,        # ê²€ì¦ ë°ì´í„°
    callbacks=callbacks)
```

    Epoch 1/30
    63/63 [==============================] - 17s 97ms/step - loss: 0.7200 - accuracy: 0.5315 - val_loss: 0.6768 - val_accuracy: 0.5750
    Epoch 2/30
    63/63 [==============================] - 6s 90ms/step - loss: 0.7213 - accuracy: 0.5785 - val_loss: 0.9444 - val_accuracy: 0.5100
    .
    .
    .
    Epoch 29/30
    63/63 [==============================] - 5s 73ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 2.9502 - val_accuracy: 0.7080
    Epoch 30/30
    63/63 [==============================] - 5s 70ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 2.5123 - val_accuracy: 0.7250


í›ˆë ¨í•œ ëª¨ë¸ì˜ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë³´ì.


```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```


    
![small1](https://user-images.githubusercontent.com/77332628/196624197-e1024bcb-d139-4c28-897b-dcfb9cd10fde.png)
    



    
![small2](https://user-images.githubusercontent.com/77332628/196624206-1dbb5458-08ea-46a9-aaf5-a1e235eb56c3.png)
    


ìœ„ì˜ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ëª¨ë¸ì´ ê³¼ëŒ€ì í•©ëœë‹¤. í›ˆë ¨ ì •í™•ë„ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ 100%ì— ê°€ê¹Œì´ ë„ë‹¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê²€ì¦ ì •í™•ë„ëŠ” 13ë²ˆì§¸ ì—í¬í¬ë§Œì— ê±°ì˜ ìµœê³ ì ì— ë‹¤ë‹¤ë¥´ê³  ë”ì´ìƒ ì§„ì „ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¼ í…ŒìŠ¤íŠ¸ ì •í™•ë„ëŠ” ì–´ë–¨ê¹Œ? ê³¼ëŒ€ì í•© ë˜ê¸° ì „ì˜ ìƒíƒœë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì½œë°±ìœ¼ë¡œ ì €ì¥í•œ ëª¨ë¸ì„ ë¡œë“œí•˜ì.


```python

test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 36ms/step - loss: 0.5975 - accuracy: 0.6900
    í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.690


í…ŒìŠ¤íŠ¸ ì •í™•ë„ëŠ” 69%ë¥¼ ì–»ì—ˆë‹¤.

ì´ë²ˆì¥ì—ì„œ ë‹¤ë£¨ëŠ” ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì— ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤. ì´ì „ ê¸€ì—ì„œ ì–¸ê¸‰í•œ ë“œë¡­ì•„ì›ƒì´ë‚˜ L2 ê·œì œ ë“±ì˜ ë°©ë²•ë“¤ë„ ìˆì§€ë§Œ , ì»´í“¨í„° ë¹„ì „ì— íŠ¹í™”ëœ ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ë°©ë²•ì¸ **ë°ì´í„° ì¦ì‹**ì„ ì‹œë„í•˜ê² ë‹¤.

### 2.3 ë°ì´í„° ì¦ì‹
ë°ì´í„° ì¦ì‹ì€ ê¸°ì¡´ í›ˆë ¨ ìƒ˜í”Œì„ ì´ìš©í•´ì„œ ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì—¬ëŸ¬ê°€ì§€ ëœë¤í•œ ë³€í™˜ì„ ì ìš©í•´ì„œ ìƒ˜í”Œì„ ëŠ˜ë¦¬ëŠ” ë°©ë²•ì´ ìˆë‹¤. ë°ì´í„° ì¦ì‹ì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” ëª¨ë¸ì´ í›ˆë ¨í•  ë•Œ ì •í™•íˆ ê°™ì€ ë°ì´í„°ë¥¼ ë‘ë²ˆ ë§Œë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë©´ ëª¨ë¸ì´ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì¸¡ë©´ì„ í•™ìŠµí•˜ë¯€ë¡œ ë” ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.

ì¼€ë¼ìŠ¤ì—ì„œ ëª¨ë¸ì˜ ì‹œì‘ë¶€ë¶„ì— **ë°ì´í„° ì¦ì‹ì¸µ**ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì½”ë“œëŠ” ë°ì´í„° ì¦ì‹ ì¸µì˜ ì˜ˆì‹œ ì½”ë“œë‹¤.


```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),  # ëœë¤í•˜ê²Œ 50% ì´ë¯¸ì§€ë¥¼ ìˆ˜í‰ì„ ë’¤ì§‘ëŠ”ë‹¤.
        layers.RandomRotation(0.1),       # [-10%,+10%] ë²”ìœ„ ì•ˆì—ì„œ ëœë¤í•œ ê°’ë§Œí¼ ì´ë¯¸ì§€ íšŒì „
        layers.RandomZoom(0.2),  ])         # [-20%,+20%] ë²”ìœ„ ì•ˆì—ì„œ ëœë¤í•œ ë¹„ìœ¨ë§Œí¼ ì´ë¯¸ì§€ í™•ëŒ€ or ì¶•ì†Œ
```

ì¦ì‹ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•´ë³´ì.


```python
plt.figure(figsize=(10, 10))
for images, _ in train_dataset.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")
```


    
![small3](https://user-images.githubusercontent.com/77332628/196624212-775977a9-9d27-425b-9162-4d5e6deb392f.png)
    


ì¶œë ¥ëœ ì´ë¯¸ì§€ë“¤ì„ ë³´ë©´ ì•Œê² ì§€ë§Œ ì—¬ì „íˆ ì…ë ¥ ë°ì´í„°ë“¤ ì‚¬ì´ì— ìƒí˜¸ ì—°ê´€ì„±ì´ í¬ë‹¤. ì¦‰, ê¸°ì¡´ ì •ë³´ì˜ ì¬ì¡°í•©ë§Œ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì™„ì „íˆ ê³¼ëŒ€ì í•©ì„ ì œê±°í•  ìˆ˜ëŠ” ì—†ë‹¤. ê³¼ëŒ€ì í•©ì„ ë”ìš± í™•ì‹¤íˆ ì–µì œí•˜ê¸° ìœ„í•´ Dense ì¸µ ì§ì „ì— Dropoutì¸µì„ ì¶”ê°€í•œë‹¤.

ë˜í•œ ì´ë¯¸ì§€ ì¦ì‹ì¸µì€ Dropout ì¸µì²˜ëŸ¼ predictë‚˜ evaluateê°™ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ê³„ì—ì„œëŠ” ë™ì‘í•˜ëŠ” ì•ŠëŠ”ë‹¤. ì¦‰, ëª¨ë¸ì„ í‰ê°€í•  ë•ŒëŠ” ë°ì´í„° ì¦ì‹ê³¼ ë“œë¡­ì•„ì›ƒì´ ì—†ëŠ” ëª¨ë¸ì²˜ëŸ¼ ë™ì‘í•œë‹¤.


```python
inputs = keras.Input(shape=(180, 180, 3))
x = data_augmentation(inputs)           # ì´ë¯¸ì§€ ì¦ì‹ì¸µ ì¶”ê°€
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)              # Dropout ì¸µ ì¶”ê°€
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(loss="binary_crossentropy",
              optimizer="rmsprop",
              metrics=["accuracy"])

```


```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch_with_augmentation.keras",
        save_best_only=True,
        monitor="val_loss")
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)
```

    Epoch 1/100
    63/63 [==============================] - 8s 101ms/step - loss: 0.7366 - accuracy: 0.5020 - val_loss: 0.6907 - val_accuracy: 0.5000
    Epoch 2/100
    63/63 [==============================] - 6s 93ms/step - loss: 0.6942 - accuracy: 0.5330 - val_loss: 0.6770 - val_accuracy: 0.5590
    .
    .
    .
    Epoch 99/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.1689 - accuracy: 0.9455 - val_loss: 0.9251 - val_accuracy: 0.8260
    Epoch 100/100
    63/63 [==============================] - 6s 96ms/step - loss: 0.2022 - accuracy: 0.9360 - val_loss: 0.7826 - val_accuracy: 0.8310



```python
import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()
```

![small4](https://user-images.githubusercontent.com/77332628/196624214-4cfb8c97-bda3-4284-943d-8c4e73a47cc6.png)

![small5](https://user-images.githubusercontent.com/77332628/196624216-391cd22e-fef3-4200-9740-acb05e7851ac.png)

ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë³´ë©´ , ë°ì´í„° ì¦ì‹ê³¼ ë“œë¡­ì•„ì›ƒ ë•ë¶„ì—ê³¼ëŒ€ì í•©ì´ ì´ì „ì˜ ëª¨ë¸ ë³´ë‹¤ í›¨ì”¬ ëŠ¦ì€ 60,70ë²ˆì§¸ ì—í¬í¬ ê·¼ì²˜ì—ì„œ ì‹œì‘ëœë‹¤. ì¦‰, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì›”ë“±íˆ ì¢‹ì•„ì¡Œë‹¤.


ë§ˆì§€ë§‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•´ë³´ì.


```python
test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.3f}")
```

    63/63 [==============================] - 3s 37ms/step - loss: 0.4403 - accuracy: 0.8290
    í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.829


í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ 82.9% ì •ë„ ì–»ì—ˆë‹¤. ì´ì „ì— ì–»ì—ˆë˜ 69% ë³´ë‹¤ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ íŠœë‹í•˜ë©´ ë” ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì— í•œê³„ê°€ ìˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ë” ì¢‹ì€ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ **ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ í™œìš©**í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ ê¸€ì—ì„œ ë‹¤ë¤„ë³´ë„ë¡ í•˜ê² ë‹¤.

[<ì¼€ë¼ìŠ¤ ì°½ì‹œìì—ê²Œ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ê°œì • 2íŒ>(ê¸¸ë²—, 2022)ì„ í•™ìŠµí•˜ê³  ê°œì¸ í•™ìŠµìš©ìœ¼ë¡œ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.]

ì¶œì²˜: í”„ë‘ì†Œì™€ ìˆ„ë ˆ ì§€ìŒ, âŒœì¼€ë¼ìŠ¤ ì°½ì‹œìì—ê²Œ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ê°œì •2íŒâŒŸ, ë°•í•´ì„  ì˜®ê¹€, ê¸¸ë²—, 2022 , 318-336ìª½

ë„ì„œë³´ê¸°: https://www.gilbut.co.kr/book/view?bookcode=BN003496
