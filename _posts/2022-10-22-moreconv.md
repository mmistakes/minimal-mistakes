---
title : '[DL/CV] 이미지 분할 : 컴퓨터 비전 심화 ✂️'
layout : single
---

## 4. 컴퓨터 비전의 여러 능력


### 4.0 세 가지 주요 컴퓨터 비전 작업
[앞의 글](https://hamin-chang.github.io/pretrain/)에서 다룬 컴퓨터 비전을 이용하 이미지 분류는 컴퓨터 비전에 적용할 수 있는 하나의 딥러닝 애플리케이션이다. 일반적으로 3가지의 주요 컴퓨터 비전 작업을 알아 둘 필요가 있다.



*   **이미지 분류** : 이미지에 하나 이상의 레이블을 할당하는 것.
*   **이미지 분할** : 이미지를 다른 영역으로 나누거나 분할하는 기술. 각 영역은 하나의 범주를 나타낸다.
*   **객체 탐지** : 이미지에 목표 객체 주변에 **바운딩 박스**라고 불리는 사각형을 그리는 것이 목표. 각 사각형은 하나의 클래스에 연결된다.

아래 그림은 왼쪽부터 이미지 분류, 이미지 분류+위치 파악 , 객체 탐지, 이미지 분할을 나타내는 이미지다.

그림1

또한, 컴퓨터 비전을 위한 딥러닝은 위에서 언급한 3가지 기술 외에도 이미지 유사도 평가, 키포인트 감지, 포즈 추정 등이 있다. 이번 글에서는 **이미지 분할**을 위한 딥러닝을 다뤄보도록 하겠다.

### 4.1 이미지 분할 예제
딥러닝을 이요한 이미지 분할은 딥러닝 모델을 사용해서 이미지 안의 각 픽셀에 클래스를 할당하는 것이다. 즉, 이미지를 여러 다른 영역으로 분할한다. 이미지 분할에는 두가지 종류가 있다.


*   **시맨틱 분할** : 각 픽셀이 독립적으로 하나의 의미를 가진 범주로 분류된다. 아래 이미지 처럼 여러명의 사람이 있다면 사람에 해당되는 모든 픽셀은 동일한 'person' 범주로 매핑된다.
*   **인스턴스 분할** : 이미지 픽셀을 범주로 분류하는 것뿐만 아니라 개별 객체 인스턴스를 구분한다. 여러명의 사람들이 있으면 각 사람마다 'person 1','person 2'등으로 개별 클래스로 다룬다.

그림2

이번 예제에서느 시맨틱 분할에 초점을 맞추겠다. 이번 예제의 데이터로는 Oxford-IIIT Pets 데이터셋을 사용한다. 이 데이터셋은 다양한 품종의 고양이와 강아지 사진 7,000여개와 각 사진의 전경-배경 **분할 마스크**(이미지 분할에서의 레이블)를 포함한다. 입력 이미지와 동일한 크기의 이미지고 컬러 채널은 하나다. 분할 마스크의 픽셀은 3개의 정수 값 중 하나를 가진다.


*   1: 전경
*   2: 배경
*   3: 윤곽

먼저 아래의 코드로 데이터셋을 내려받고 압축을 푼다.





```python
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz
```

    --2022-10-22 05:41:13--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
    Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2
    Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz [following]
    --2022-10-22 05:41:13--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
    Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz [following]
    --2022-10-22 05:41:13--  https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz
    Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98
    Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 791918971 (755M) [application/octet-stream]
    Saving to: ‘images.tar.gz’
    
    images.tar.gz       100%[===================>] 755.23M  29.0MB/s    in 27s     
    
    2022-10-22 05:41:41 (28.0 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]
    
    --2022-10-22 05:41:41--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
    Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2
    Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz [following]
    --2022-10-22 05:41:41--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
    Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.
    HTTP request sent, awaiting response... 301 Moved Permanently
    Location: https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz [following]
    --2022-10-22 05:41:42--  https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz
    Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98
    Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 19173078 (18M) [application/octet-stream]
    Saving to: ‘annotations.tar.gz’
    
    annotations.tar.gz  100%[===================>]  18.28M  14.3MB/s    in 1.3s    
    
    2022-10-22 05:41:43 (14.3 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]
    


입력 파일 경로와 분할 마스크 파일 경로를 각각 리스트로 구성한다.


```python
import os

input_dir = 'images/'
target_dir = 'annotations/trimaps'

input_img_paths = sorted(           # 입력 사진은 images/ 폴더에 JPG 파일로 저장되어 있다.
    [os.path.join(input_dir,fname)
    for fname in os.listdir(input_dir) if fname.endswith('jpg')])

target_paths = sorted(
    [os.path.join(target_dir,fname)
    for fname in os.listdir(target_dir) 
    if fname.endswith('.png') and not fname.startswith('.')])

```

입력 이미지와 분할 마스크를 살펴보자. 샘플 이미지는 다음과 같다.


```python
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array

plt.axis("off")
plt.imshow(load_img(input_img_paths[9]))
```




    <matplotlib.image.AxesImage at 0x7f3e64f15450>




    
![png](20221022_files/20221022_6_1.png)
    


이에 해당하는 분할 마스크는 다음과 같다.


```python
def display_target(target_array):
  # 원래 레이블이 1,2,3여기에 1을 빼고 127을 곱해서 0(검정),127(회색),254(거의 흰색)으로 만든다.
  normalized_array = (target_array.astype('uint8')-1) * 127
  plt.axis('off')
  plt.imshow(normalized_array[:,:,0])
img = img_to_array(load_img(target_paths[9],color_mode='grayscale'))
display_target(img)
```


    
![png](20221022_files/20221022_8_0.png)
    


그 다음 입력과 타깃을 2개의 넘파이 배열로 로드하고 이 배열을 훈련과 검증 ㅅ트로 나눈다. (데이터셋이 작기 때문에 모두 메모리로 로드 가능)


```python
import numpy as np
import random

img_size = (200,200) # 입력 , 타깃을 모두 200x200 크기로 변경
num_imgs = len(input_img_paths) # 데이터에 있는 전체 샘플 개수

random.Random(1337).shuffle(input_img_paths) # 파일경로를 뒤섞는다.(같은 순서 유지하도록 같은 시드 사용)
random.Random(1337).shuffle(target_paths) #

def path_to_input_image(path):
  return img_to_array(load_img(path,target_size=img_size))

def path_to_target(path):
  img = img_to_array(
      load_img(path,target_size=img_size,color_mode='grayscale'))
  img = img.astype('uint8') - 1 # 레이블이 0,1,2가 되도록 1 빼기
  return img

input_imgs = np.zeros((num_imgs,)+img_size+(3,),dtype='float32') # 전체이미지를 input_imgs에 float32배열로 로드
targets = np.zeros((num_imgs,)+img_size+(1,),dtype='uint8') # 타킷 마스크는 targets에 uint8로 로드(같은 순서로)
for i in range(num_imgs):
  input_imgs[i] = path_to_input_image(input_img_paths[i])
  targets[i] = path_to_target(target_paths[i]) 

num_val_samples = 1000  # 검증 데이터 샘플 개수
train_input_imgs = input_imgs[:-num_val_samples] # 데이터를 훈련세트와 검증세트로 나눔
train_targets = targets[:-num_val_samples]
val_input_imgs = input_imgs[-num_val_samples:]
val_targets = targets[-num_val_samples:]
  # stride : 2으로 다운샘플링, padding : 'same' 으로 패딩이 특성맵 크기에 영향 미치지 않도록
```

이제 이미지 분할을 위한 딥러닝 모델을 구축해보자.


```python
from tensorflow import keras
from tensorflow.keras import layers

def get_model(img_size, num_classes):
    inputs = keras.Input(shape=img_size + (3,))
    x = layers.Rescaling(1./255)(inputs)

    x = layers.Conv2D(64, 3, strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(128, 3, strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2D(128, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(256, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.Conv2D(256, 3, activation="relu", padding="same")(x)

    x = layers.Conv2DTranspose(256, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(256, 3, activation="relu", padding="same", strides=2)(x)
    x = layers.Conv2DTranspose(128, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(128, 3, activation="relu", padding="same", strides=2)(x)
    x = layers.Conv2DTranspose(64, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(64, 3, activation="relu", padding="same", strides=2)(x)

    outputs = layers.Conv2D(num_classes, 3, activation="softmax", padding="same")(x)

    model = keras.Model(inputs, outputs)
    return model

model = get_model(img_size=img_size, num_classes=3)
model.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_1 (InputLayer)        [(None, 200, 200, 3)]     0         
                                                                     
     rescaling (Rescaling)       (None, 200, 200, 3)       0         
                                                                     
     conv2d (Conv2D)             (None, 100, 100, 64)      1792      
                                                                     
     conv2d_1 (Conv2D)           (None, 100, 100, 64)      36928     
                                                                     
     conv2d_2 (Conv2D)           (None, 50, 50, 128)       73856     
                                                                     
     conv2d_3 (Conv2D)           (None, 50, 50, 128)       147584    
                                                                     
     conv2d_4 (Conv2D)           (None, 25, 25, 256)       295168    
                                                                     
     conv2d_5 (Conv2D)           (None, 25, 25, 256)       590080    
                                                                     
     conv2d_transpose (Conv2DTra  (None, 25, 25, 256)      590080    
     nspose)                                                         
                                                                     
     conv2d_transpose_1 (Conv2DT  (None, 50, 50, 256)      590080    
     ranspose)                                                       
                                                                     
     conv2d_transpose_2 (Conv2DT  (None, 50, 50, 128)      295040    
     ranspose)                                                       
                                                                     
     conv2d_transpose_3 (Conv2DT  (None, 100, 100, 128)    147584    
     ranspose)                                                       
                                                                     
     conv2d_transpose_4 (Conv2DT  (None, 100, 100, 64)     73792     
     ranspose)                                                       
                                                                     
     conv2d_transpose_5 (Conv2DT  (None, 200, 200, 64)     36928     
     ranspose)                                                       
                                                                     
     conv2d_6 (Conv2D)           (None, 200, 200, 3)       1731      
                                                                     
    =================================================================
    Total params: 2,880,643
    Trainable params: 2,880,643
    Non-trainable params: 0
    _________________________________________________________________


구축한 모델의 처음 절반은 이미지 분류에서 사용하는 컨브넷과 비슷하다. Conv2D 층을 쌓고 점진적으로 필터 개수를 늘리는 방식으로 이미지를 절반으로 세번 다운샘플링하고, 마지막 합성곱 층의 활성화 출력은 (25,25,256)으로 끝난다. 처음 절반의 목적은 이미지를 작은 특성 맵으로 인코딩하는 것이다. 공간상의 각 픽셀은 원본 이미지에 있는 더 큰 영역에 대한 정보를 담고 있다.(일종의 압축) 
또 이미지 분류와 다른 점은 다운샘플링 방식에 있다. 위의 코드에서는 MaxPooling2D 층이 아닌 stride를 2로 설정해서 다운샘플링했다. 이미지 분할에서는 각 픽셀의 출력값을 분할마스크에 할당하기 때문에 공간상의 위치 정보를 완전히 삭제하는 Maxpooling2D 층 대신 위치 정보를 유지하는 스트라이드 합성곱을 이용한 다운샘플링을 사용한다.

위의 코드의 나머지 절반은 Conv2DTranspose 층을 쌓은 것이다. 이 모델의 최종 출력은 타깃 마스크(분할 마스크)의 크기인 (200,200,3)과 동일해야한다. 하지만 처음 절반은 (25,25,256) 크기의 특성맵을 출력한다. 그렇기 때문에 나머지 절반에서는 다운샘플링이 아니라 특성맵을 **업샘플링(UpSampling)**해야한다. 업샘플링을 하기 위해 Conv2DTranspose 층을 사용해서 Conv2D 층을 사용해서 압축한 C(25,25,256)크기의 특성맵을 Conv2DTranspose 층을 쌓아서 (200,200,3)크기의 이미지를 다시 얻는 것이다.

다음은 업샘플링을 포함한 이미지 분할을 시각화한 이미지다.

그림3

그 다음 모델을 컴파일하고 훈련을 진행한다.


```python
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")

callbacks = [
    keras.callbacks.ModelCheckpoint("oxford_segmentation.keras",
                                    save_best_only=True)
]

history = model.fit(train_input_imgs, train_targets,
                    epochs=50,
                    callbacks=callbacks,
                    batch_size=64,
                    validation_data=(val_input_imgs, val_targets))
```

    Epoch 1/50
    100/100 [==============================] - 88s 711ms/step - loss: 4.8130 - val_loss: 0.8630
    Epoch 2/50
    100/100 [==============================] - 64s 636ms/step - loss: 0.8802 - val_loss: 0.8161
    Epoch 3/50
    100/100 [==============================] - 63s 632ms/step - loss: 0.8278 - val_loss: 0.7679
    Epoch 4/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.7782 - val_loss: 0.7276
    Epoch 5/50
    100/100 [==============================] - 63s 630ms/step - loss: 0.7516 - val_loss: 0.6835
    Epoch 6/50
    100/100 [==============================] - 63s 632ms/step - loss: 0.7030 - val_loss: 0.6467
    Epoch 7/50
    100/100 [==============================] - 63s 632ms/step - loss: 0.6571 - val_loss: 0.6501
    Epoch 8/50
    100/100 [==============================] - 63s 631ms/step - loss: 0.6280 - val_loss: 0.5910
    Epoch 9/50
    100/100 [==============================] - 63s 631ms/step - loss: 0.5939 - val_loss: 0.6645
    Epoch 10/50
    100/100 [==============================] - 63s 630ms/step - loss: 0.5850 - val_loss: 0.5545
    Epoch 11/50
    100/100 [==============================] - 63s 631ms/step - loss: 0.5504 - val_loss: 0.5091
    Epoch 12/50
    100/100 [==============================] - 63s 630ms/step - loss: 0.5387 - val_loss: 0.4948
    Epoch 13/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.5121 - val_loss: 0.6203
    Epoch 14/50
    100/100 [==============================] - 63s 630ms/step - loss: 0.4981 - val_loss: 0.4760
    Epoch 15/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.4841 - val_loss: 0.4524
    Epoch 16/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4715 - val_loss: 0.4434
    Epoch 17/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4634 - val_loss: 0.4878
    Epoch 18/50
    100/100 [==============================] - 63s 626ms/step - loss: 0.4500 - val_loss: 0.4513
    Epoch 19/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4372 - val_loss: 0.4466
    Epoch 20/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4252 - val_loss: 0.4146
    Epoch 21/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.4161 - val_loss: 0.3910
    Epoch 22/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4066 - val_loss: 0.4356
    Epoch 23/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.4001 - val_loss: 0.3965
    Epoch 24/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.3950 - val_loss: 0.4185
    Epoch 25/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.3843 - val_loss: 0.4800
    Epoch 26/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.3686 - val_loss: 0.3672
    Epoch 27/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3694 - val_loss: 0.4166
    Epoch 28/50
    100/100 [==============================] - 63s 630ms/step - loss: 0.3559 - val_loss: 0.3620
    Epoch 29/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3461 - val_loss: 0.4006
    Epoch 30/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3410 - val_loss: 0.3673
    Epoch 31/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3270 - val_loss: 0.4998
    Epoch 32/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3214 - val_loss: 0.3638
    Epoch 33/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3142 - val_loss: 0.4067
    Epoch 34/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.3050 - val_loss: 0.4709
    Epoch 35/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.2970 - val_loss: 0.3888
    Epoch 36/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2934 - val_loss: 0.4044
    Epoch 37/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2828 - val_loss: 0.3906
    Epoch 38/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.2720 - val_loss: 0.3889
    Epoch 39/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2693 - val_loss: 0.4127
    Epoch 40/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.2595 - val_loss: 0.3863
    Epoch 41/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2511 - val_loss: 0.4547
    Epoch 42/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.2462 - val_loss: 0.3854
    Epoch 43/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2380 - val_loss: 0.4295
    Epoch 44/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.2286 - val_loss: 0.4746
    Epoch 45/50
    100/100 [==============================] - 63s 629ms/step - loss: 0.2300 - val_loss: 0.4132
    Epoch 46/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.2189 - val_loss: 0.4338
    Epoch 47/50
    100/100 [==============================] - 63s 628ms/step - loss: 0.2105 - val_loss: 0.4912
    Epoch 48/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.2147 - val_loss: 0.4441
    Epoch 49/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.2062 - val_loss: 0.4395
    Epoch 50/50
    100/100 [==============================] - 63s 627ms/step - loss: 0.1985 - val_loss: 0.4796



```python
epochs = range(1, len(history.history["loss"])+1 )

loss = history.history["loss"]          # 첫번째 loss가 너무 커서 그래프에서는 삭제
val_loss = history.history['val_loss']

plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")

plt.title("Training and validation loss")
plt.legend()

```




    <matplotlib.legend.Legend at 0x7f3b4a02e390>




    
![png](20221022_files/20221022_15_1.png)
    


위의 손실 그래프를 보면 중간 즈음인 30 에포크 근처에서 과대적합이 시작된다. 콜백을 사용해서 저장한 검증 손실 기준 최상의 모델을 다시 로드해서 분할 마스크를 예측하는 방법을 알아보자.


```python
from tensorflow.keras.utils import array_to_img

model = keras.models.load_model("oxford_segmentation.keras")

i = 4
test_image = val_input_imgs[i]
plt.axis("off")
plt.imshow(array_to_img(test_image))

mask = model.predict(np.expand_dims(test_image, 0))[0]

def display_mask(pred):
    mask = np.argmax(pred, axis=-1)
    mask *= 127
    plt.axis("off")
    plt.imshow(mask)

display_mask(mask)
```

    1/1 [==============================] - 0s 173ms/step



    
![png](20221022_files/20221022_17_1.png)
    


예측된 마스크에 부수적으로 생각 작은 흔적들을 제외하면 모델은 잘 작동하는 것 같다. 저번 글과 이번 글을 통해서 딥러닝을 통해서 이미지 분류와 이미지 분할을 수행하는 방법을 배웠다. 하지만 실전 문제를 풀기 위해서는 아직 부족하다. 최고 수준의 모델을 구축하려면 전문가 수준의 빠르고 정확한 결정을 내릴수 있는 능력을 가져야한다. 전문가와 초보자 사이의 간격을 좁히기 위해 아키텍처 패턴에 대해 배워야 한다. 

다음 글에서는 최신 컨브넷 아키텍처 패턴과 컨브넷 모델이 학습한 것을 해석하는 것에 대해 다뤄보도록 하겠다.

[<케라스 창시자에게 배우는 딥러닝 개정 2판>(길벗, 2022)을 학습하고 개인 학습용으로 정리한 내용입니다.] 출처: 프랑소와 숄레 지음, ⌜케라스 창시자에게 배우는 딥러닝 개정2판⌟, 박해선 옮김, 길벗, 2022 도서보기: https://www.gilbut.co.kr/book/view?bookcode=BN003496
