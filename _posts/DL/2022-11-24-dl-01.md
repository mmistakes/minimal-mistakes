---
title:  "[DL] 활성화 함수(Activation Function)의 필요성 및 종류"
layout: single

categories: "DL"
tags: ["활성화함수", "Sigmoid", "Tanh", "ReLU", "Leakly ReLU"]

toc: true
toc_sticky: true
toc_label : "목차"
toc_icon: "bars"
---

# <span class="half_HL">✔️ 학습목표</span>
- 활성화 함수의 배경
- 활성화 함수의 필요성
- 활성화 함수의 종류 및 특징

***

> 선형함수인 h(x)=cx를 활성화함수로 사용한 3층 네트워크를 떠올려 이를 식으로 나타내면 y(x)=h(h(h(x)))가 된다. 이는 실은 y(x)=ax와 똑같은 식이다. a=c3이라고만 하면 끝이다. 즉, **은닉층(hidden layer)이 없는 네트워크**로 표현할 수 있다. <u>뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 **활성화함수**로는 반드시 비선형 함수를 사용</u>해야 한다. - 밑바닥부터 시작하는 딥러닝 -

<br>

# <span class="half_HL">1. 활성화 함수란?</span>
신경망에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 활성화 함수를 통과시킨 후 전달한다. **활성화 함수(activation function)는 입력 신호의 총합을 출력 신호로 변환하는 함수로, 입력 받은 신호를 얼마나 출력할지 결정하고 네트워크에 층을 쌓아 비선형을 표현할 수 있도록 해준다.**

신경망은 인간의 뇌를 기반으로 느슨하게 모델링되며, 여기서 수천 또는 수백만 개의 노드가 서로 촘촘하게 연결되어 있다. 뇌가 뉴런을 "발사"하는 것처럼, 인공신경망(ANN)에서 들어오는 노드로부터 어떤 가중치(weight)만큼 신호를 보내면 인공신경망이 발사된다.

<br>

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*Jx97VUn1i4GFBbD6G11eyg.png" width="500">
<br>
<small>출처 : himanshuxd.medium - 활성화 기능의 기본 사항</small>
</div>

<br>

위 그림의 D를 예로 들면, 가중치가 ```w1, w2, w3, ... wN``` 및 입력은 ```i1, i2, i3, ... iN``` 이라면 ```w1*i1 + w2*i2 + w3*i3 ... wN*iN``` 의 합계를 얻는다

신경망 및 연결의 여러 계층에 대해 ```wX``` 및 ```iX```의 다양한 값과 특정 뉴런이 활성화 되어있는지 여부에 따라 달라지는 합계 ```S```를 가질 수 있으므로 이를 **정규화하고 값의 범위가 크게 달라지는 것을 방지하기 위해 사용**한다. <u>전체 프로세스를 통계적으로 균형 있게 만들기 위해 이러한 값을 0, 1 또는 -1, 1 사이의 값으로 바꾸는 것을 활성화 함수라고 한다.</u> 이 프로세스는 코드의 온전함을 보존할 뿐만 아니라 비활성화된 입력에서 더 어려울 수 있는 복잡성과 필요한 컴퓨팅 성능을 줄이기 위한 것이다.

<br>

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1100/1*Ah2uXX7FVN3gHoy_6UobKA.gif" width="400">
<br>
<small>출처 : himanshuxd.medium - 활성화 기능의 기본 사항</small>
</div>

<br>

# <span class="half_HL">2. 활성화 함수의 배경</span>

🤔 **퍼셉트론(Perceptron) 이란?**
- 딥러닝 모델은 보통 여러 개의 층으로 이루어진다.
- 그 중에 하나의 층을 가져와 다시 쪼갠다면 보통 '노드'라고 불리는 것으로 쪼개지는데, 이것이 **퍼셉트론**이다.


🤔 **활성화 함수는 주로 비선형인데, 왜 선형이 아닌 비선형일까?**
- 선형 함수를 사용할 시 층을 깊게 하는 의미가 줄어들기 때문이다.
- **선형 분류기의 한계를 극복**하기 위해 **선형 분류기를 비선형 분류기**로 만드는 특징을 가진다.

🤔 **선형 분류기의 한계가 무엇일까?**
- 인공신경망에 대한 연구가 한계를 맞게된 첫 과제는 바로 **XOR 문제**였다.
- 아래 그림에서 확인할 수 있듯이 기존의 퍼셉트론은 AND와 OR 문제는 해결할 수 있었지만 선형 분류기라는 한계에 의해 XOR과 같은 non-linear한 문제는 해결할 수 없었다.

<div style="text-align : center;">
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkzozO%2FbtqA0OR0l7G%2FyuHw8Y762KYUfnaoP4Ymx1%2Fimg.png">
<br>
<small>출처 : https://ganghee-lee.tistory.com/30</small>
</div>

<br>

이러한 문제를 해결하기 위해서 나온 개념이 **은닉층(hidden layer)** 이다. 그러나 hidden layer도 무작정 쌓기만 한다고 해서 퍼셉트론을 선형분류기에서 비선형분류기로 바꿀 수 있는 것은 아니다.
왜냐하면 선형 시스템이 아무리 깊어지더라도 ```f(ax+by)=af(x)+bf(y)``` 성질 때문에 결국 하나의 layer로 깊은 layer를 구현할 수 있기 때문이다.

즉, linear한 연산을 갖는 layer를 수십개 쌓아도 결국 이는 하나의 linear 연산을 나타낼 수 있다. 이에 대한 해결책이 바로 **활성화 함수(activation function)** 이다. 활성화 함수를 사용하면 입력값에 대한 출력값이 linear 하게 나오지 않으므로 선형분류기를 비선형 시스템으로 만들 수 있다. 

∴ 따라서 MLP(Multiple layer perceptron)는 단지 linear layer 를 여러개 쌓는 개념이 아닌 활성화 함수를 이용한 non-linear 시스템을 여러 layer로 쌓는 개념이다.

<br>

# <span class="half_HL">3. 활성화 함수의 종류</span>

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*F9-nc6ez5GOJ1mdB3TLlow.png" width="500">
<br>
<small>출처 : himanshuxd.medium - 신경망 활성화 함수</small>
</div>

여러 종류의 활성화 함수가 있으나, 대표적으로 사용되는 

## (1) 시그모이드 함수(sigmoid function)

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*rPf0P4QBV4wD8dlfs45OHg.png" width="500">
<br>
<img src="https://miro.medium.com/max/1100/1*HaAT5CmVq2io8PdPKIgVHQ.png" width="250">
<br>
<small>출처 : himanshuxd.medium - sigmoid 함수 그래프</small>
</div>


시그모이드 함수는 Logistic 함수라고 불리기도 하며, x의 값에 따라 0~1의 값을 출력하는 S자형 함수이다. (데이터의 평균은 0.5를 갖게 된다.)

### 시그모이드의 특징
- 함수값이 (0, 1)로 제한된다.
- 중간 값은 12이다.
- 매우 큰 값을 가지면 함수값은 거의 1이며, 매우 작은 값을 가지면 거의 0이다.
- **Gradient Vanishing** 현상이 발생한다.
- **함숫값 중심이 0이 아니다.**

🤔 **vanishing gradient 이란?**
위 그림에서 시그모이드 함수의 기울기를 보면 알 수 있듯이 Input 값이 어느정도 크거나 작으면 기울기가 아주 작아진다. 이로 인해 **vanishing gradient** 현상이 발생한다. 미분함수에 대해 x=0에서 최대값 1/4 을 가지고, input값이 일정이상 올라가면 미분값이 거의 0에 수렴하게된다. 이는 |x|값이 커질 수록 Gradient Backpropagation시 미분값이 소실될 가능성이 크다.

∴ 이러한 단점들 때문에 초기에는 자주 사용하는 활성화 함수였지만, 최근에는 자주 사용하지 않게 되었다.


## (2) tanh 함수(Tanh function)

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1280/1*3OerqzdzmcDWbo6XC_zG_g.png" width="400">
<br>
<img src="https://miro.medium.com/max/1100/1*KoMk9IyQpjlD4G3Zuzn3kQ.png" width="100">
<br>
<small>출처 : himanshuxd.medium - tanh 함수 그래프</small>
</div>

### tanh의 특징
- tanh 함수는 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결했다.
- 하지만 미분함수에 대해 일정값 이상 커질시 미분값이 소실되는 gradient vanishing 문제는 여전히 남아있다.

∴ output데이터의 평균이 0으로써 시그모이드보다 대부분의 경우에서 학습이 더 잘 된다. 하지만 시그모이드와 마찬가지로 Vanishing gradient 현상이 일어난다.

## (3) ReLU 함수 (Rectified Linear Unit function)
> "Hidden layer에서 어떤 활성화 함수를 사용할지 모르겠으면 ReLU를 사용하면 된다" - Andrew ng

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1100/1*j_S4-AMQgduuX6ahg3o7eA.png" width="400">
<br>
<img src="https://miro.medium.com/max/1100/1*_M8-WXjWWRzR-_f-Ex72pQ.png" width="200">
<br>
<small>출처 : himanshuxd.medium - ReLU 함수 그래프</small>
</div>

### ReLU의 특징
- 대부분의 input값에 대해 기울기가 0이 아니기 때문에 학습이 빨리 된다.
  - 학습을 느리게 하는 원인 : gradient가 0이 되는 것
  - gradient가 0이 되는 것을 막아주기 때문에 시그모이드, Tanh 함수보다 학습이 빠름
- **hidden layer 에서 활성화 함수를 뭐 써야할 지 모르겠으면 일단 ReLU를 쓰면 된다.**

### ReLU의 문제 및 개선

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*29VH_NiSdoLJ1jUMLrURCA.png" width="600">
<br>
<small>출처 : himanshuxd.medium - sigmoid(왼쪽) 및 ReLU(오른쪽) 그래프</small>
</div>

ReLU에서는 뉴런이 죽는 ("Dying ReLU") 현상이 발견되는데 이를 해결하기 위해 Leakly ReLU가 등장했다. (기울기가 x=0의 왼쪽에서 누수 발생)

<div style="text-align : center;">
<img src="https://miro.medium.com/max/1400/1*siH_yCvYJ9rqWSUYeDBiRA.png" width="600">
<br>
<small>출처 : himanshuxd.medium - ReLU 및 Leakly ReLU 그래프</small>
</div>

Leaky ReLU에는 작은 음의 기울기가 있으므로 큰 변화도에 대해 전혀 실행하지 않는 대신 뉴런이 일부 값을 출력하므로 레이어도 훨씬 더 최적화된다.

<br>

# <span class="half_HL">✔️ Reference</span>
- [티스토리 by heeee__ya - [ML] 활성화 함수(Activation Function) 종류 정리](https://heeya-stupidbutstudying.tistory.com/entry/ML-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98Activation-Function)
- [네이버블로그 by HDLY - 딥러닝 - 활성화 함수(Activation) 종류 및 비교](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339)
- [티스토리 by Kanghee Lee - 활성화 함수(activation function)종류 및 정리](https://ganghee-lee.tistory.com/32)
- [티스토리 by Kanghee Lee - 활성화 함수(activation function)을 사용하는 이유](https://ganghee-lee.tistory.com/30)
- [reniew's blog - 딥러닝에서 사용하는 활성화 함수](https://reniew.github.io/12/)
- [himanshuxd medium - Activation Functions : Sigmoid, tanh, ReLU, Leaky ReLU, PReLU, ELU, Threshold ReLU and Softmax basics for Neural Networks and Deep Learning](https://himanshuxd.medium.com/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)
- [벨로그 by leejaejun - AIFFEL FD #20 활성화 함수 (Activation Function)](https://velog.io/@leejaejun/AIFFEL-FD-20-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98%EC%9D%98-%EC%9D%B4%ED%95%B4)
- [tt by 포자랩스 - Activation Function(활성함수)](https://www.theteams.kr/teams/2829/post/69499)
